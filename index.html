<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-12T00:00:00Z">2024-12-12</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">104</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context Canvas: Enhancing Text-to-Image Diffusion Models with Knowledge
  Graph-Based RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kavana Venkatesh, Yusuf Dalva, Ismini Lourentzou, Pinar Yanardag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel approach to enhance the capabilities of text-to-image
models by incorporating a graph-based RAG. Our system dynamically retrieves
detailed character information and relational data from the knowledge graph,
enabling the generation of visually accurate and contextually rich images. This
capability significantly improves upon the limitations of existing T2I models,
which often struggle with the accurate depiction of complex or culturally
specific subjects due to dataset constraints. Furthermore, we propose a novel
self-correcting mechanism for text-to-image models to ensure consistency and
fidelity in visual outputs, leveraging the rich context from the graph to guide
corrections. Our qualitative and quantitative experiments demonstrate that
Context Canvas significantly enhances the capabilities of popular models such
as Flux, Stable Diffusion, and DALL-E, and improves the functionality of
ControlNet for fine-grained image editing tasks. To our knowledge, Context
Canvas represents the first application of graph-based RAG in enhancing T2I
models, representing a significant advancement for producing high-fidelity,
context-aware multi-faceted images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://context-canvas.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Olympus: A Universal Task Router for Computer Vision Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanze Lin, Yunsheng Li, Dongdong Chen, Weijian Xu, Ronald Clark, Philip H. S. Torr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Olympus, a new approach that transforms Multimodal Large
Language Models (MLLMs) into a unified framework capable of handling a wide
array of computer vision tasks. Utilizing a controller MLLM, Olympus delegates
over 20 specialized tasks across images, videos, and 3D objects to dedicated
modules. This instruction-based routing enables complex workflows through
chained actions without the need for training heavy generative models. Olympus
easily integrates with existing MLLMs, expanding their capabilities with
comparable performance. Experimental results demonstrate that Olympus achieves
an average routing accuracy of 94.75% across 20 tasks and precision of 91.82%
in chained action scenarios, showcasing its effectiveness as a universal task
router that can solve a diverse range of computer vision tasks. Project page:
https://github.com/yuanze-lin/Olympus_page
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web
  Tutorials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiheng Xu, Dunjie Lu, Zhennan Shen, Junli Wang, Zekun Wang, Yuchen Mao, Caiming Xiong, Tao Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphical User Interface (GUI) agents hold great potential for automating
complex tasks across diverse digital environments, from web applications to
desktop software. However, the development of such agents is hindered by the
lack of high-quality, multi-step trajectory data required for effective
training. Existing approaches rely on expensive and labor-intensive human
annotation, making them unsustainable at scale. To address this challenge, we
propose AgentTrek, a scalable data synthesis pipeline that generates
high-quality GUI agent trajectories by leveraging web tutorials. Our method
automatically gathers tutorial-like texts from the internet, transforms them
into task goals with step-by-step instructions, and employs a visual-language
model agent to simulate their execution in a real digital environment. A
VLM-based evaluator ensures the correctness of the generated trajectories. We
demonstrate that training GUI agents with these synthesized trajectories
significantly improves their grounding and planning performance over the
current models. Moreover, our approach is more cost-efficient compared to
traditional human annotation methods. This work underscores the potential of
guided replay with web tutorials as a viable strategy for large-scale GUI agent
training, paving the way for more capable and autonomous digital agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://agenttrek.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TimeRefine: Temporal Grounding with Time Refining Video LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xizi Wang, Feng Cheng, Ziyang Wang, Huiyu Wang, Md Mohaiminul Islam, Lorenzo Torresani, Mohit Bansal, Gedas Bertasius, David Crandall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video temporal grounding aims to localize relevant temporal boundaries in a
video given a textual prompt. Recent work has focused on enabling Video LLMs to
perform video temporal grounding via next-token prediction of temporal
timestamps. However, accurately localizing timestamps in videos remains
challenging for Video LLMs when relying solely on temporal token prediction.
Our proposed TimeRefine addresses this challenge in two ways. First, instead of
directly predicting the start and end timestamps, we reformulate the temporal
grounding task as a temporal refining task: the model first makes rough
predictions and then refines them by predicting offsets to the target segment.
This refining process is repeated multiple times, through which the model
progressively self-improves its temporal localization accuracy. Second, to
enhance the model's temporal perception capabilities, we incorporate an
auxiliary prediction head that penalizes the model more if a predicted segment
deviates further from the ground truth, thus encouraging the model to make
closer and more accurate predictions. Our plug-and-play method can be
integrated into most LLM-based temporal grounding approaches. The experimental
results demonstrate that TimeRefine achieves 3.6% and 5.0% mIoU improvements on
the ActivityNet and Charades-STA datasets, respectively. Code and pretrained
models will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for
  Long-term Streaming Video and Audio Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pan Zhang, Xiaoyi Dong, Yuhang Cao, Yuhang Zang, Rui Qian, Xilin Wei, Lin Chen, Yifei Li, Junbo Niu, Shuangrui Ding, Qipeng Guo, Haodong Duan, Xin Chen, Han Lv, Zheng Nie, Min Zhang, Bin Wang, Wenwei Zhang, Xinyue Zhang, Jiaye Ge, Wei Li, Jingwen Li, Zhongying Tu, Conghui He, Xingcheng Zhang, Kai Chen, Yu Qiao, Dahua Lin, Jiaqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating AI systems that can interact with environments over long periods,
similar to human cognition, has been a longstanding research goal. Recent
advancements in multimodal large language models (MLLMs) have made significant
strides in open-world understanding. However, the challenge of continuous and
simultaneous streaming perception, memory, and reasoning remains largely
unexplored. Current MLLMs are constrained by their sequence-to-sequence
architecture, which limits their ability to process inputs and generate
responses simultaneously, akin to being unable to think while perceiving.
Furthermore, relying on long contexts to store historical data is impractical
for long-term interactions, as retaining all information becomes costly and
inefficient. Therefore, rather than relying on a single foundation model to
perform all functions, this project draws inspiration from the concept of the
Specialized Generalist AI and introduces disentangled streaming perception,
reasoning, and memory mechanisms, enabling real-time interaction with streaming
video and audio input. The proposed framework InternLM-XComposer2.5-OmniLive
(IXC2.5-OL) consists of three key modules: (1) Streaming Perception Module:
Processes multimodal information in real-time, storing key details in memory
and triggering reasoning in response to user queries. (2) Multi-modal Long
Memory Module: Integrates short-term and long-term memory, compressing
short-term memories into long-term ones for efficient retrieval and improved
accuracy. (3) Reasoning Module: Responds to queries and executes reasoning
tasks, coordinating with the perception and memory modules. This project
simulates human-like cognition, enabling multimodal large language models to
provide continuous and adaptive service over time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Github Repo:
  https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-OmniLive</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenNER 1.0: Standardized Open-Access Named Entity Recognition <span class="highlight-title">Dataset</span>s
  in 50+ Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chester Palen-Michel, Maxwell Pickering, Maya Kruse, Jonne Sälevä, Constantine Lignos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present OpenNER 1.0, a standardized collection of openly available named
entity recognition (NER) datasets. OpenNER contains 34 datasets spanning 51
languages, annotated in varying named entity ontologies. We correct annotation
format issues, standardize the original datasets into a uniform representation,
map entity type names to be more consistent across corpora, and provide the
collection in a structure that enables research in multilingual and
multi-ontology NER. We provide baseline models using three pretrained
multilingual language models to compare the performance of recent models and
facilitate future research in NER.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DISHONEST: Dissecting misInformation Spread using Homogeneous sOcial
  NEtworks and Semantic Topic classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caleb Stam, Emily Saldanha, Mahantesh Halappanavar, Anurag Acharya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of the COVID-19 pandemic resulted in a significant rise in the
spread of misinformation on online platforms such as Twitter. Oftentimes this
growth is blamed on the idea of the "echo chamber." However, the behavior said
to characterize these echo chambers exists in two dimensions. The first is in a
user's social interactions, where they are said to stick with the same clique
of like-minded users. The second is in the content of their posts, where they
are said to repeatedly espouse homogeneous ideas. In this study, we link the
two by using Twitter's network of retweets to study social interactions and
topic modeling to study tweet content. In order to measure the diversity of a
user's interactions over time, we develop a novel metric to track the speed at
which they travel through the social network. The application of these analysis
methods to misinformation-focused data from the pandemic demonstrates
correlation between social behavior and tweet content. We believe this
correlation supports the common intuition about how antisocial users behave,
and further suggests that it holds even in subcommunities already rife with
misinformation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiverseAgentEntropy: Quantifying Black-Box LLM Uncertainty through
  Diverse Perspectives and Multi-Agent Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Feng, Phu Mon Htut, Zheng Qi, Wei Xiao, Manuel Mager, Nikolaos Pappas, Kishaloy Halder, Yang Li, Yassine Benajiba, Dan Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantifying the uncertainty in the factual parametric knowledge of Large
Language Models (LLMs), especially in a black-box setting, poses a significant
challenge. Existing methods, which gauge a model's uncertainty through
evaluating self-consistency in responses to the original query, do not always
capture true uncertainty. Models might respond consistently to the origin query
with a wrong answer, yet respond correctly to varied questions from different
perspectives about the same query, and vice versa. In this paper, we propose a
novel method, DiverseAgentEntropy, for evaluating a model's uncertainty using
multi-agent interaction under the assumption that if a model is certain, it
should consistently recall the answer to the original query across a diverse
collection of questions about the same original query. We further implement an
abstention policy to withhold responses when uncertainty is high. Our method
offers a more accurate prediction of the model's reliability and further
detects hallucinations, outperforming other self-consistency-based methods.
Additionally, it demonstrates that existing models often fail to consistently
retrieve the correct answer to the same query under diverse varied questions
even when knowing the correct answer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JuStRank: Benchmarking LLM Judges for System Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ariel Gera, Odellia Boni, Yotam Perlitz, Roy Bar-Haim, Lilach Eden, Asaf Yehudai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the rapid progress of generative AI, there is a pressing need to
systematically compare and choose between the numerous models and
configurations available. The scale and versatility of such evaluations make
the use of LLM-based judges a compelling solution for this challenge.
Crucially, this approach requires first to validate the quality of the LLM
judge itself. Previous work has focused on instance-based assessment of LLM
judges, where a judge is evaluated over a set of responses, or response pairs,
while being agnostic to their source systems. We argue that this setting
overlooks critical factors affecting system-level ranking, such as a judge's
positive or negative bias towards certain systems. To address this gap, we
conduct the first large-scale study of LLM judges as system rankers. System
scores are generated by aggregating judgment scores over multiple system
outputs, and the judge's quality is assessed by comparing the resulting system
ranking to a human-based ranking. Beyond overall judge assessment, our analysis
provides a fine-grained characterization of judge behavior, including their
decisiveness and bias.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Does Representation Matter? Exploring Intermediate Layers in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oscar Skean, Md Rifat Arefin, <span class="highlight-author">Yann LeCun</span>, Ravid Shwartz-Ziv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding what defines a good representation in large language models
(LLMs) is fundamental to both theoretical understanding and practical
applications. In this paper, we investigate the quality of intermediate
representations in various LLM architectures, including Transformers and State
Space Models (SSMs). We find that intermediate layers often yield more
informative representations for downstream tasks than the final layers. To
measure the representation quality, we adapt and apply a suite of metrics -
such as prompt entropy, curvature, and augmentation-invariance - originally
proposed in other contexts. Our empirical study reveals significant
architectural differences, how representations evolve throughout training, and
how factors like input randomness and prompt length affect each layer. Notably,
we observe a bimodal pattern in the entropy of some intermediate layers and
consider potential explanations tied to training data. Overall, our results
illuminate the internal mechanics of LLMs and guide strategies for
architectural optimization and training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 2024 NeurIPs Workshop on Machine Learning and Compression</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Foundational Large Language Models for Materials Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vaibhav Mishra, Somaditya Singh, Dhruv Ahlawat, Mohd Zaki, Vaibhav Bihani, Hargun Singh Grover, Biswajit Mishra, Santiago Miret,  Mausam, N. M. Anoop Krishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Materials discovery and development are critical for addressing global
challenges. Yet, the exponential growth in materials science literature
comprising vast amounts of textual data has created significant bottlenecks in
knowledge extraction, synthesis, and scientific reasoning. Large Language
Models (LLMs) offer unprecedented opportunities to accelerate materials
research through automated analysis and prediction. Still, their effective
deployment requires domain-specific adaptation for understanding and solving
domain-relevant tasks. Here, we present LLaMat, a family of foundational models
for materials science developed through continued pretraining of LLaMA models
on an extensive corpus of materials literature and crystallographic data.
Through systematic evaluation, we demonstrate that LLaMat excels in
materials-specific NLP and structured information extraction while maintaining
general linguistic capabilities. The specialized LLaMat-CIF variant
demonstrates unprecedented capabilities in crystal structure generation,
predicting stable crystals with high coverage across the periodic table.
Intriguingly, despite LLaMA-3's superior performance in comparison to LLaMA-2,
we observe that LLaMat-2 demonstrates unexpectedly enhanced domain-specific
performance across diverse materials science tasks, including structured
information extraction from text and tables, more particularly in crystal
structure generation, a potential adaptation rigidity in overtrained LLMs.
Altogether, the present work demonstrates the effectiveness of domain
adaptation towards developing practically deployable LLM copilots for materials
research. Beyond materials science, our findings reveal important
considerations for domain adaptation of LLMs, such as model selection, training
methodology, and domain-specific performance, which may influence the
development of specialized scientific AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audios Don't Lie: Multi-Frequency Channel Attention Mechanism for Audio
  Deepfake Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09467v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09467v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangguang Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of artificial intelligence technology, the
application of deepfake technology in the audio field has gradually increased,
resulting in a wide range of security risks. Especially in the financial and
social security fields, the misuse of deepfake audios has raised serious
concerns. To address this challenge, this study proposes an audio deepfake
detection method based on multi-frequency channel attention mechanism (MFCA)
and 2D discrete cosine transform (DCT). By processing the audio signal into a
melspectrogram, using MobileNet V2 to extract deep features, and combining it
with the MFCA module to weight different frequency channels in the audio
signal, this method can effectively capture the fine-grained frequency domain
features in the audio signal and enhance the Classification capability of fake
audios. Experimental results show that compared with traditional methods, the
model proposed in this study shows significant advantages in accuracy,
precision,recall, F1 score and other indicators. Especially in complex audio
scenarios, this method shows stronger robustness and generalization
capabilities and provides a new idea for audio deepfake detection and has
important practical application value. In the future, more advanced audio
detection technologies and optimization strategies will be explored to further
improve the accuracy and generalization capabilities of audio deepfake
detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Impact of Copyrighted Material on Large Language Models: A Norwegian
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javier de la Rosa, Vladislav Mikhailov, Lemei Zhang, Freddy Wetjen, David Samuel, Peng Liu, Rolv-Arild Braaten, Petter Mæhlum, Magnus Breder Birkenes, Andrey Kutuzov, Tita Enstad, Svein Arne Brygfjeld, Jon Atle Gulla, Stephan Oepen, Erik Velldal, Wilfred Østgulen, Liljia Øvrelid, Aslak Sira Myhre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of copyrighted materials in training generative language models
raises critical legal and ethical questions. This paper presents a framework
for and the results of empirically assessing the impact of copyrighted
materials on the performance of large language models (LLMs) for Norwegian. We
found that both books and newspapers contribute positively when the models are
evaluated on a diverse set of Norwegian benchmarks, while fiction works
possibly lead to decreased performance. Our experiments could inform the
creation of a compensation scheme for authors whose works contribute to AI
development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>pre-print, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Intention To Implementation: Automating Biomedical Research via
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Luo, Linghang Shi, Yihao Li, Aobo Zhuang, Yeyun Gong, Ling Liu, Lin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional biomedical research is increasingly labor-intensive due to the
exponential growth of scientific literature and datasets. Artificial
intelligence (AI), particularly Large Language Models (LLMs), has the potential
to revolutionize this process by automating various steps. Still, significant
challenges remain, including the need for multidisciplinary expertise,
logicality of experimental design, and performance measurements. This paper
introduces BioResearcher, the first end-to-end automated system designed to
streamline the entire biomedical research process involving dry lab
experiments. BioResearcher employs a modular multi-agent architecture,
integrating specialized agents for search, literature processing, experimental
design, and programming. By decomposing complex tasks into logically related
sub-tasks and utilizing a hierarchical learning approach, BioResearcher
effectively addresses the challenges of multidisciplinary requirements and
logical complexity. Furthermore, BioResearcher incorporates an LLM-based
reviewer for in-process quality control and introduces novel evaluation metrics
to assess the quality and automation of experimental protocols. BioResearcher
successfully achieves an average execution success rate of 63.07% across eight
previously unmet research objectives. The generated protocols averagely
outperform typical agent systems by 22.0% on five quality metrics. The system
demonstrates significant potential to reduce researchers' workloads and
accelerate biomedical discoveries, paving the way for future innovations in
automated research systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical
  Ability Assessment of LLM-Powered AI Tutors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaushal Kumar Maurya, KV Aditya Srivatsa, Kseniia Petukhova, Ekaterina Kochmar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate whether current state-of-the-art large language
models (LLMs) are effective as AI tutors and whether they demonstrate
pedagogical abilities necessary for good AI tutoring in educational dialogues.
Previous efforts towards evaluation have been limited to subjective protocols
and benchmarks. To bridge this gap, we propose a unified evaluation taxonomy
with eight pedagogical dimensions based on key learning sciences principles,
which is designed to assess the pedagogical value of LLM-powered AI tutor
responses grounded in student mistakes or confusion in the mathematical domain.
We release MRBench -- a new evaluation benchmark containing 192 conversations
and 1,596 responses from seven state-of-the-art LLM-based and human tutors,
providing gold annotations for eight pedagogical dimensions. We assess
reliability of the popular Prometheus2 LLM as an evaluator and analyze each
tutor's pedagogical abilities, highlighting which LLMs are good tutors and
which ones are more suitable as question-answering systems. We believe that the
presented taxonomy, benchmark, and human-annotated labels will streamline the
evaluation process and help track the progress in AI tutors' development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text Generation Models for Luxembourgish with Limited Data: A Balanced
  Multilingual Strategy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alistair Plum, Tharindu Ranasinghe, Christoph Purschke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenges in developing language models for
less-represented languages, with a focus on Luxembourgish. Despite its active
development, Luxembourgish faces a digital data scarcity, exacerbated by
Luxembourg's multilingual context. We propose a novel text generation model
based on the T5 architecture, combining limited Luxembourgish data with equal
amounts, in terms of size and type, of German and French data. We hypothesise
that a model trained on Luxembourgish, German, and French will improve the
model's cross-lingual transfer learning capabilities and outperform monolingual
and large multilingual models. To verify this, the study at hand explores
whether multilingual or monolingual training is more beneficial for
Luxembourgish language generation. For the evaluation, we introduce LuxGen, a
text generation benchmark that is the first of its kind for Luxembourgish.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at VarDial 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Imitate, Explore, and Self-Improve: A Reproduction Report on
  Slow-thinking Reasoning Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, slow-thinking reasoning systems, such as o1, have demonstrated
remarkable capabilities in solving complex reasoning tasks. These systems
typically engage in an extended thinking process before responding to a query,
allowing them to generate more thorough, accurate, and well-reasoned solutions.
These systems are primarily developed and maintained by industry, with their
core techniques not publicly disclosed. In response, an increasing number of
studies from the research community aim to explore the technical foundations
underlying these powerful reasoning systems. Building on these prior efforts,
this paper presents a reproduction report on implementing o1-like reasoning
systems. We introduce an "imitate, explore, and self-improve" framework as our
primary technical approach to train the reasoning model. In the initial phase,
we use distilled long-form thought data to fine-tune the reasoning model,
enabling it to invoke a slow-thinking mode. The model is then encouraged to
explore challenging problems by generating multiple rollouts, which can result
in increasingly more high-quality trajectories that lead to correct answers.
Furthermore, the model undergoes self-improvement by iteratively refining its
training dataset. To verify the effectiveness of this approach, we conduct
extensive experiments on three challenging benchmarks. The experimental results
demonstrate that our approach achieves competitive performance compared to
industry-level reasoning systems on these benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report on Slow Thinking with LLMs: Part II</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Text Normalization for Luxembourgish using Real-Life Variation
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anne-Marie Lutgen, Alistair Plum, Christoph Purschke, Barbara Plank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Orthographic variation is very common in Luxembourgish texts due to the
absence of a fully-fledged standard variety. Additionally, developing NLP tools
for Luxembourgish is a difficult task given the lack of annotated and parallel
data, which is exacerbated by ongoing standardization. In this paper, we
propose the first sequence-to-sequence normalization models using the ByT5 and
mT5 architectures with training data obtained from word-level real-life
variation data. We perform a fine-grained, linguistically-motivated evaluation
to test byte-based, word-based and pipeline-based models for their strengths
and weaknesses in text normalization. We show that our sequence model using
real-life variation data is an effective approach for tailor-made normalization
in Luxembourgish.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at VarDial 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Bench to Bedside: A <span class="highlight-title">Review</span> of Clinical Trialsin Drug Discovery and
  Development 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyang Wang, Ming Liu, Benji Peng, Xinyuan Song, Charles Zhang, Xintian Sun, Qian Niu, Junyu Liu, Silin Chen, Keyu Chen, Ming Li, Pohsun Feng, Ziqian Bi, Yunze Wang, Yichao Zhang, Cheng Fei, Lawrence KQ Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical trials are an indispensable part of the drug development process,
bridging the gap between basic research and clinical application. During the
development of new drugs, clinical trials are used not only to evaluate the
safety and efficacy of the drug but also to explore its dosage, treatment
regimens, and potential side effects. This review discusses the various stages
of clinical trials, including Phase I (safety assessment), Phase II
(preliminary efficacy evaluation), Phase III (large-scale validation), and
Phase IV (post-marketing surveillance), highlighting the characteristics of
each phase and their interrelationships. Additionally, the paper addresses the
major challenges encountered in clinical trials, such as ethical issues,
subject recruitment difficulties, diversity and representativeness concerns,
and proposes strategies for overcoming these challenges. With the advancement
of technology, innovative technologies such as artificial intelligence, big
data, and digitalization are gradually transforming clinical trial design and
implementation, improving trial efficiency and data quality. The article also
looks forward to the future of clinical trials, particularly the impact of
emerging therapies such as gene therapy and immunotherapy on trial design, as
well as the importance of regulatory reforms and global collaboration. In
conclusion, the core role of clinical trials in drug development will continue
to drive the progress of innovative drug development and clinical treatment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Word Sense Linking: Disambiguating Outside the Sandbox 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrei Stefan Bejgu, Edoardo Barba, Luigi Procopio, Alberte Fernández-Castro, Roberto Navigli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Word Sense Disambiguation (WSD) is the task of associating a word in a given
context with its most suitable meaning among a set of possible candidates.
While the task has recently witnessed renewed interest, with systems achieving
performances above the estimated inter-annotator agreement, at the time of
writing it still struggles to find downstream applications. We argue that one
of the reasons behind this is the difficulty of applying WSD to plain text.
Indeed, in the standard formulation, models work under the assumptions that a)
all the spans to disambiguate have already been identified, and b) all the
possible candidate senses of each span are provided, both of which are
requirements that are far from trivial. In this work, we present a new task
called Word Sense Linking (WSL) where, given an input text and a reference
sense inventory, systems have to both identify which spans to disambiguate and
then link them to their most suitable meaning.We put forward a
transformer-based architecture for the task and thoroughly evaluate both its
performance and those of state-of-the-art WSD systems scaled to WSL,
iteratively relaxing the assumptions of WSD. We hope that our work will foster
easier integration of lexical semantics into downstream applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Falcon-UI: Understanding GUI Before Following User Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huawen Shen, Chang Liu, Gengluo Li, Xinlong Wang, Yu Zhou, Can Ma, Xiangyang Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pursuing human-like interaction for Graphical User Interface (GUI) agents
requires understanding the GUI context and following user instructions.
However, existing works typically couple these two aspects and focus more on
instruct-following abilities, while ignoring the importance of understanding
the GUI context. In this paper, we introduce an instruction-free GUI navigation
dataset, termed Insight-UI Dataset, to enhance model comprehension of GUI
environments. Insight-UI Dataset is automatically generated from the Common
Crawl corpus, simulating various platforms -- including iOS, Android, Windows,
and Linux -- across multiple resolutions on 312K domains. Although GUI
interactions vary by context, diverse interfaces share common internal
patterns, such as clicking an item to view its details. It implies the
feasibility of independent GUI operation learning, followed by joint
optimization with instruction tuning. Thereby, we develop the GUI agent model
Falcon-UI, which is initially pretrained on Insight-UI Dataset and subsequently
fine-tuned on Android and Web GUI datasets, including AITW, AITZ, Android
Control, and Mind2Web. With 7 billion parameters, Falcon-UI achieves accuracy
comparable to the 72 billion-parameter Qwen2VL on AITZ, validating the
alignment between GUI context comprehension and agent performance. Our code and
dataset will be open-sourced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Graphical Models for Vision-Language Compositional Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fiorenzo Parascandolo, Nicholas Moratelli, Enver Sangineto, Lorenzo Baraldi, Rita Cucchiara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has empirically shown that Vision-Language Models (VLMs) struggle
to fully understand the compositional properties of the human language, usually
modeling an image caption as a "bag of words". As a result, they perform poorly
on compositional tasks, which require a deeper understanding of the different
entities of a sentence (subject, verb, etc.) jointly with their mutual
relationships in order to be solved. In this paper, we model the dependency
relations among textual and visual tokens using a Causal Graphical Model (CGM),
built using a dependency parser, and we train a decoder conditioned by the VLM
visual encoder. Differently from standard autoregressive or parallel
predictions, our decoder's generative process is partially-ordered following
the CGM structure. This structure encourages the decoder to learn only the main
causal dependencies in a sentence discarding spurious correlations. Using
extensive experiments on five compositional benchmarks, we show that our method
significantly outperforms all the state-of-the-art compositional approaches by
a large margin, and it also improves over methods trained using much larger
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training LayoutLM from Scratch for Efficient Named-Entity Recognition in
  the Insurance Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benno Uthayasooriyar, Antoine Ly, Franck Vermet, Caio Corro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generic pre-trained neural networks may struggle to produce good results in
specialized domains like finance and insurance. This is due to a domain
mismatch between training data and downstream tasks, as in-domain data are
often scarce due to privacy constraints. In this work, we compare different
pre-training strategies for LayoutLM. We show that using domain-relevant
documents improves results on a named-entity recognition (NER) problem using a
novel dataset of anonymized insurance-related financial documents called
Payslips. Moreover, we show that we can achieve competitive results using a
smaller and faster model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Coling 2025 workshop (FinNLP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking LLMs for Mimicking Child-Caregiver Language in Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Liu, Abdellah Fourtassi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs can generate human-like dialogues, yet their ability to simulate early
child-adult interactions remains largely unexplored. In this paper, we examined
how effectively LLMs can capture the distinctive features of child-caregiver
language in interaction, using both static and interactive benchmarking
methods. We found that state-of-the-art LLMs like Llama 3 and GPT-4o can
approximate child-caregiver dialogues at the word and utterance level, but they
struggle to reproduce the child and caregiver's discursive patterns, exaggerate
alignment, and fail to reach the level of diversity shown by humans. The
broader goal of this work is to initiate the development of a comprehensive
benchmark for LLMs in child-oriented applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CRVQ: Channel-relaxed Vector Quantization for Extreme Compression of
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhuang Xu, Shiyu Ji, Qingfu Zhu, Wanxiang Che
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Powerful large language models (LLMs) are increasingly expected to be
deployed with lower computational costs, enabling their capabilities on
resource-constrained devices. Post-training quantization (PTQ) has emerged as a
star approach to achieve this ambition, with best methods compressing weights
to less than 2 bit on average. In this paper, we propose Channel-Relaxed Vector
Quantization (CRVQ), a novel technique that significantly improves the
performance of PTQ baselines at the cost of only minimal additional bits. This
state-of-the-art extreme compression method achieves its results through two
key innovations: (1) carefully selecting and reordering a very small subset of
critical weight channels, and (2) leveraging multiple codebooks to relax the
constraint of critical channels. With our method, we demonstrate a 38.9%
improvement over the current strongest sub-2-bit PTQ baseline, enabling nearer
lossless 1-bit compression. Furthermore, our approach offers flexible
customization of quantization bit-width and performance, providing a wider
range of deployment options for diverse hardware platforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Solve Domain-Specific Calculation Problems with
  Knowledge-Intensive Programs Generator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyuan Liu, Shihang Wang, Lizhi Qing, Jun Lin, Ji Zhang, Fei Wu, Kun Kuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain Large Language Models (LLMs) are developed for domain-specific tasks
based on general LLMs. But it still requires professional knowledge to
facilitate the expertise for some domain-specific tasks. In this paper, we
investigate into knowledge-intensive calculation problems. We find that the
math problems to be challenging for LLMs, when involving complex
domain-specific rules and knowledge documents, rather than simple formulations
of terminologies. Therefore, we propose a pipeline to solve the domain-specific
calculation problems with Knowledge-Intensive Programs Generator more
effectively, named as KIPG. It generates knowledge-intensive programs according
to the domain-specific documents. For each query, key variables are extracted,
then outcomes which are dependent on domain knowledge are calculated with the
programs. By iterative preference alignment, the code generator learns to
improve the logic consistency with the domain knowledge. Taking legal domain as
an example, we have conducted experiments to prove the effectiveness of our
pipeline, and extensive analysis on the modules. We also find that the code
generator is also adaptable to other domains, without training on the new
knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Understanding the Robustness of LLM-based Evaluations under
  Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manav Chaudhary, Harshit Gupta, Savita Bhat, Vasudeva Varma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional evaluation metrics like BLEU and ROUGE fall short when capturing
the nuanced qualities of generated text, particularly when there is no single
ground truth. In this paper, we explore the potential of Large Language Models
(LLMs), specifically Google Gemini 1, to serve as automatic evaluators for
non-standardized metrics in summarization and dialog-based tasks. We conduct
experiments across multiple prompting strategies to examine how LLMs fare as
quality evaluators when compared with human judgments on the SummEval and USR
datasets, asking the model to generate both a score as well as a justification
for the score. Furthermore, we explore the robustness of the LLM evaluator by
using perturbed inputs. Our findings suggest that while LLMs show promise,
their alignment with human evaluators is limited, they are not robust against
perturbations and significant improvements are required for their standalone
use as reliable evaluators for subjective metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICON 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ First Train to Generate, then Generate to Train: UnitedSynT5 for
  Few-Shot NLI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sourav Banerjee, Anush Mahajan, Ayushi Agarwal, Eishkaran Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Inference (NLI) tasks require identifying the relationship
between sentence pairs, typically classified as entailment, contradiction, or
neutrality. While the current state-of-the-art (SOTA) model, Entailment
Few-Shot Learning (EFL), achieves a 93.1% accuracy on the Stanford Natural
Language Inference (SNLI) dataset, further advancements are constrained by the
dataset's limitations. To address this, we propose a novel approach leveraging
synthetic data augmentation to enhance dataset diversity and complexity. We
present UnitedSynT5, an advanced extension of EFL that leverages a T5-based
generator to synthesize additional premise-hypothesis pairs, which are
rigorously cleaned and integrated into the training data. These augmented
examples are processed within the EFL framework, embedding labels directly into
hypotheses for consistency. We train a GTR-T5-XL model on this expanded
dataset, achieving a new benchmark of 94.7% accuracy on the SNLI dataset,
94.01% accuracy on the E-SNLI dataset, and 92.57% accuracy on the MultiNLI
dataset, surpassing the previous SOTA models. This research demonstrates the
potential of synthetic data augmentation in improving NLI models, offering a
path forward for further advancements in natural language understanding tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Make Satire Boring Again: Reducing Stylistic Bias of Satirical Corpus by
  Utilizing Generative LLMs <span class="chip">COLING2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asli Umay Ozturk, Recep Firat Cekinel, Asli Umay Ozturk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Satire detection is essential for accurately extracting opinions from textual
data and combating misinformation online. However, the lack of diverse corpora
for satire leads to the problem of stylistic bias which impacts the models'
detection performances. This study proposes a debiasing approach for satire
detection, focusing on reducing biases in training data by utilizing generative
large language models. The approach is evaluated in both cross-domain (irony
detection) and cross-lingual (English) settings. Results show that the
debiasing method enhances the robustness and generalizability of the models for
satire and irony detection tasks in Turkish and English. However, its impact on
causal language models, such as Llama-3.1, is limited. Additionally, this work
curates and presents the Turkish Satirical News Dataset with detailed human
annotations, with case studies on classification, debiasing, and
explainability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to BUCC2025 Workshop @COLING2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CleanComedy: Creating Friendly Humor through Generative Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dmitry Vikhorev, Daria Galimzianova, Svetlana Gorovaia, Elizaveta Zhemchuzhina, Ivan P. Yamshchikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humor generation is a challenging task in natural language processing due to
limited resources and the quality of existing datasets. Available humor
language resources often suffer from toxicity and duplication, limiting their
effectiveness for training robust models. This paper proposes CleanComedy, a
specialized, partially annotated toxicity-filtered corpus of English and
Russian jokes collected from various sources. We study the effectiveness of our
data filtering approach through a survey on humor and toxicity levels in
various joke groups. In addition, we study advances in computer humor
generation by comparing jokes written by humans with various groups of
generative jokes, including our baseline models trained on the CleanComedy
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReFF: Reinforcing Format Faithfulness in Language Models across Varied
  Tasks <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiashu Yao, Heyan Huang, Zeming Liu, Haoyu Wen, Wei Su, Boao Qian, Yuhang Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Following formatting instructions to generate well-structured content is a
fundamental yet often unmet capability for large language models (LLMs). To
study this capability, which we refer to as format faithfulness, we present
FormatBench, a comprehensive format-related benchmark. Compared to previous
format-related benchmarks, FormatBench involves a greater variety of tasks in
terms of application scenes (traditional NLP tasks, creative works, autonomous
agency tasks), human-LLM interaction styles (single-turn instruction,
multi-turn chat), and format types (inclusion, wrapping, length, coding).
Moreover, each task in FormatBench is attached with a format checker program.
Extensive experiments on the benchmark reveal that state-of-the-art open- and
closed-source LLMs still suffer from severe deficiency in format faithfulness.
By virtue of the decidable nature of formats, we propose to Reinforce Format
Faithfulness (ReFF) to help LLMs generate formatted output as instructed
without compromising general quality. Without any annotated data, ReFF can
substantially improve the format faithfulness rate (e.g., from 21.6% in
original LLaMA3 to 95.0% on caption segmentation task), while keep the general
quality comparable (e.g., from 47.3 to 46.4 in F1 scores). Combined with
labeled training data, ReFF can simultaneously improve both format faithfulness
(e.g., from 21.6% in original LLaMA3 to 75.5%) and general quality (e.g., from
47.3 to 61.6 in F1 scores). We further offer an interpretability analysis to
explain how ReFF improves both format faithfulness and general quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Text Embedding Meets Large Language Model: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijie Nie, Zhangchi Feng, Mingxin Li, Cunwang Zhang, Yanzhao Zhang, Dingkun Long, Richong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text embedding has become a foundational technology in natural language
processing (NLP) during the deep learning era, driving advancements across a
wide array of downstream tasks. While many natural language understanding
challenges can now be modeled using generative paradigms and leverage the
robust generative and comprehension capabilities of large language models
(LLMs), numerous practical applications, such as semantic matching, clustering,
and information retrieval, continue to rely on text embeddings for their
efficiency and effectiveness. In this survey, we categorize the interplay
between LLMs and text embeddings into three overarching themes: (1)
LLM-augmented text embedding, enhancing traditional embedding methods with
LLMs; (2) LLMs as text embedders, utilizing their innate capabilities for
embedding generation; and (3) Text embedding understanding with LLMs,
leveraging LLMs to analyze and interpret embeddings. By organizing these
efforts based on interaction patterns rather than specific downstream
applications, we offer a novel and systematic overview of contributions from
various research and application domains in the era of LLMs. Furthermore, we
highlight the unresolved challenges that persisted in the pre-LLM era with
pre-trained language models (PLMs) and explore the emerging obstacles brought
forth by LLMs. Building on this analysis, we outline prospective directions for
the evolution of text embedding, addressing both theoretical and practical
opportunities in the rapidly advancing landscape of NLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PolyIPA -- Multilingual Phoneme-to-Grapheme Conversion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davor Lauc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents PolyIPA, a novel multilingual phoneme-to-grapheme
conversion model designed for multilingual name transliteration, onomastic
research, and information retrieval. The model leverages two helper models
developed for data augmentation: IPA2vec for finding soundalikes across
languages, and similarIPA for handling phonetic notation variations. Evaluated
on a test set that spans multiple languages and writing systems, the model
achieves a mean Character Error Rate of 0.055 and a character-level BLEU score
of 0.914, with particularly strong performance on languages with shallow
orthographies. The implementation of beam search further improves practical
utility, with top-3 candidates reducing the effective error rate by 52.7\% (to
CER: 0.026), demonstrating the model's effectiveness for cross-linguistic
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Filter-then-Generate: Large Language Models with Structure-Text Adapter
  for Knowledge Graph Completion <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Liu, Jihai Zhang, Fangquan Lin, Cheng Yang, Min Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) present massive inherent knowledge and superior
semantic comprehension capability, which have revolutionized various tasks in
natural language processing. Despite their success, a critical gap remains in
enabling LLMs to perform knowledge graph completion (KGC). Empirical evidence
suggests that LLMs consistently perform worse than conventional KGC approaches,
even through sophisticated prompt design or tailored instruction-tuning.
Fundamentally, applying LLMs on KGC introduces several critical challenges,
including a vast set of entity candidates, hallucination issue of LLMs, and
under-exploitation of the graph structure. To address these challenges, we
propose a novel instruction-tuning-based method, namely FtG. Specifically, we
present a \textit{filter-then-generate} paradigm and formulate the KGC task
into a multiple-choice question format. In this way, we can harness the
capability of LLMs while mitigating the issue casused by hallucinations.
Moreover, we devise a flexible ego-graph serialization prompt and employ a
structure-text adapter to couple structure and text information in a
contextualized manner. Experimental results demonstrate that FtG achieves
substantial performance gain compared to existing state-of-the-art methods. The
instruction dataset and code are available at
\url{https://github.com/LB0828/FtG}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2025 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Pixel Language Models on Non-Standardized Languages <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto Muñoz-Ortiz, Verena Blaschke, Barbara Plank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the potential of pixel-based models for transfer learning from
standard languages to dialects. These models convert text into images that are
divided into patches, enabling a continuous vocabulary representation that
proves especially useful for out-of-vocabulary words common in dialectal data.
Using German as a case study, we compare the performance of pixel-based models
to token-based models across various syntactic and semantic tasks. Our results
show that pixel-based models outperform token-based models in part-of-speech
tagging, dependency parsing and intent detection for zero-shot dialect
evaluation by up to 26 percentage points in some scenarios, though not in
Standard German. However, pixel-based models fall short in topic
classification. These findings emphasize the potential of pixel-based models
for handling dialectal data, though further research should be conducted to
assess their effectiveness in various linguistic contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenni Bi, Kai Han, Chuanjian Liu, Yehui Tang, Yunhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable abilities across various
language tasks, but solving complex reasoning problems remains a challenge.
While existing methods like Chain-of-Thought (CoT) and Tree-of-Thought (ToT)
enhance reasoning by decomposing problems or structuring prompts, they
typically perform a single pass of reasoning and may fail to revisit flawed
paths, compromising accuracy. To address this, we propose a novel reasoning
framework called Forest-of-Thought (FoT), which integrates multiple reasoning
trees to leverage collective decision-making for solving complex logical
problems. FoT utilizes sparse activation strategies to select the most relevant
reasoning paths, improving both efficiency and accuracy. Additionally, we
introduce a dynamic self-correction strategy that enables real-time error
correction and learning from past mistakes, as well as consensus-guided
decision making strategies to optimize correctness and computational resources.
Experimental results demonstrate that the FoT framework, combined with these
strategies, significantly enhances the reasoning capabilities of LLMs, enabling
them to solve complex tasks with greater precision and efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dial-In LLM: Human-Aligned Dialogue Intent Clustering with
  LLM-in-the-loop 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengze Hong, Yuanfeng Song, Di Jiang, Wailing Ng, Yanjie Sun, Chen Jason Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The discovery of customer intention from dialogue plays an important role in
automated support system. However, traditional text clustering methods are
poorly aligned with human perceptions due to the shift from embedding distance
to semantic distance, and existing quantitative metrics for text clustering may
not accurately reflect the true quality of intent clusters. In this paper, we
leverage the superior language understanding capabilities of Large Language
Models (LLMs) for designing better-calibrated intent clustering algorithms. We
first establish the foundation by verifying the robustness of fine-tuned LLM
utility in semantic coherence evaluation and cluster naming, resulting in an
accuracy of 97.50% and 94.40%, respectively, when compared to the human-labeled
ground truth. Then, we propose an iterative clustering algorithm that
facilitates cluster-level refinement and the continuous discovery of
high-quality intent clusters. Furthermore, we present several LLM-in-the-loop
semi-supervised clustering techniques tailored for intent discovery from
customer service dialogue. Experiments on a large-scale industrial dataset
comprising 1,507 intent clusters demonstrate the effectiveness of the proposed
techniques. The methods outperformed existing counterparts, achieving 6.25%
improvement in quantitative metrics and 12% enhancement in application-level
performance when constructing an intent classifier.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Task Learning with LLMs for Implicit Sentiment Analysis:
  Data-level and Task-level Automatic Weight Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenna Lai, Haoran Xie, Guandong Xu, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit sentiment analysis (ISA) presents significant challenges due to the
absence of salient cue words. Previous methods have struggled with insufficient
data and limited reasoning capabilities to infer underlying opinions.
Integrating multi-task learning (MTL) with large language models (LLMs) offers
the potential to enable models of varying sizes to reliably perceive and
recognize genuine opinions in ISA. However, existing MTL approaches are
constrained by two sources of uncertainty: data-level uncertainty, arising from
hallucination problems in LLM-generated contextual information, and task-level
uncertainty, stemming from the varying capacities of models to process
contextual information. To handle these uncertainties, we introduce MT-ISA, a
novel MTL framework that enhances ISA by leveraging the generation and
reasoning capabilities of LLMs through automatic MTL. Specifically, MT-ISA
constructs auxiliary tasks using generative LLMs to supplement sentiment
elements and incorporates automatic MTL to fully exploit auxiliary data. We
introduce data-level and task-level automatic weight learning (AWL), which
dynamically identifies relationships and prioritizes more reliable data and
critical tasks, enabling models of varying sizes to adaptively learn
fine-grained weights based on their reasoning capabilities. We investigate
three strategies for data-level AWL, while also introducing homoscedastic
uncertainty for task-level AWL. Extensive experiments reveal that models of
varying sizes achieve an optimal balance between primary prediction and
auxiliary tasks in MT-ISA. This underscores the effectiveness and adaptability
of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures, and 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mining Word Boundaries from Speech-Text Parallel Data for Cross-domain
  Chinese Word Segmentation <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09045v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09045v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuebin Wang, Lei Zhang, Zhenghua Li, Shilin Zhou, Chen Gong, Yang Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by early research on exploring naturally annotated data for Chinese
Word Segmentation (CWS), and also by recent research on integration of speech
and text processing, this work for the first time proposes to explicitly mine
word boundaries from speech-text parallel data. We employ the Montreal Forced
Aligner (MFA) toolkit to perform character-level alignment on speech-text data,
giving pauses as candidate word boundaries. Based on detailed analysis of
collected pauses, we propose an effective probability-based strategy for
filtering unreliable word boundaries. To more effectively utilize word
boundaries as extra training data, we also propose a robust complete-then-train
(CTT) strategy. We conduct cross-domain CWS experiments on two target domains,
i.e., ZX and AISHELL2. We have annotated about 1,000 sentences as the
evaluation data of AISHELL2. Experiments demonstrate the effectiveness of our
proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based
  on Layer Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09036v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09036v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meizhi Zhong, Xikai Liu, Chen Zhang, Yikun Lei, Yan Gao, Yao Hu, Kehai Chen, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language models (LLMs) have become a research hotspot. To accelerate
the inference of LLMs, storing computed caches in memory has become the
standard technique. However, as the inference length increases, growing KV
caches might lead to out-of-memory issues. Many existing methods address this
issue through KV cache compression, primarily by preserving key tokens
throughout all layers to reduce information loss. Most of them allocate a
uniform budget size for each layer to retain. However, we observe that the
minimum budget sizes needed to retain essential information vary across layers
and models based on the perspectives of attention and hidden state output.
Building on this observation, this paper proposes a simple yet effective KV
cache compression method that leverages layer uncertainty to allocate budget
size for each layer. Experimental results show that the proposed method can
reduce memory usage of the KV caches to only $\sim$20\% when compared to Full
KV inference while achieving nearly lossless performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dialogue Language Model with Large-Scale Persona Data Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengze Hong, Chen Zhang, Chaotao Chen, Rongzhong Lian, Di Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Maintaining persona consistency is paramount in the application of
open-domain dialogue systems, as exemplified by models like ChatGPT. Despite
significant advancements, the limited scale and diversity of current persona
dialogue datasets remain challenges to achieving robust persona-consistent
dialogue models. In this study, drawing inspiration from the success of
large-scale pre-training, we introduce PPDS, an open-domain persona dialogue
system that employs extensive generative pre-training on a persona dialogue
dataset to enhance persona consistency. Specifically, we present a persona
extraction model designed to autonomously and precisely generate vast persona
dialogue datasets. Additionally, we unveil a pioneering persona augmentation
technique to address the invalid persona bias inherent in the constructed
dataset. Both quantitative and human evaluations consistently highlight the
superior response quality and persona consistency of our proposed model,
underscoring its effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shiksha: A Technical Domain focused Translation <span class="highlight-title">Dataset</span> and Model for
  Indian Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Advait Joglekar, Srinivasan Umesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Machine Translation (NMT) models are typically trained on datasets
with limited exposure to Scientific, Technical and Educational domains.
Translation models thus, in general, struggle with tasks that involve
scientific understanding or technical jargon. Their performance is found to be
even worse for low-resource Indian languages. Finding a translation dataset
that tends to these domains in particular, poses a difficult challenge. In this
paper, we address this by creating a multilingual parallel corpus containing
more than 2.8 million rows of English-to-Indic and Indic-to-Indic high-quality
translation pairs across 8 Indian languages. We achieve this by bitext mining
human-translated transcriptions of NPTEL video lectures. We also finetune and
evaluate NMT models using this corpus and surpass all other publicly available
models at in-domain tasks. We also demonstrate the potential for generalizing
to out-of-domain translation tasks by improving the baseline by over 2 BLEU on
average for these Indian languages on the Flores+ benchmark. We are pleased to
release our model and dataset via this link: https://huggingface.co/SPRINGLab.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improvement in Sign Language Translation Using Text CTC Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sihan Tan, Taro Miyazaki, Nabeela Khan, Kazuhiro Nakadai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current sign language translation (SLT) approaches often rely on gloss-based
supervision with Connectionist Temporal Classification (CTC), limiting their
ability to handle non-monotonic alignments between sign language video and
spoken text. In this work, we propose a novel method combining joint
CTC/Attention and transfer learning. The joint CTC/Attention introduces
hierarchical encoding and integrates CTC with the attention mechanism during
decoding, effectively managing both monotonic and non-monotonic alignments.
Meanwhile, transfer learning helps bridge the modality gap between vision and
language in SLT. Experimental results on two widely adopted benchmarks,
RWTH-PHOENIX-Weather 2014 T and CSL-Daily, show that our method achieves
results comparable to state-of-the-art and outperforms the pure-attention
baseline. Additionally, this work opens a new door for future research into
gloss-free SLT using text-based CTC alignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Makes Cryptic Crosswords Challenging for LLMs? <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Sadallah, Daria Kotova, Ekaterina Kochmar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cryptic crosswords are puzzles that rely on general knowledge and the
solver's ability to manipulate language on different levels, dealing with
various types of wordplay. Previous research suggests that solving such puzzles
is challenging even for modern NLP models, including Large Language Models
(LLMs). However, there is little to no research on the reasons for their poor
performance on this task. In this paper, we establish the benchmark results for
three popular LLMs: Gemma2, LLaMA3 and ChatGPT, showing that their performance
on this task is still significantly below that of humans. We also investigate
why these models struggle to achieve superior performance. We release our code
and introduced datasets at
https://github.com/bodasadallah/decrypting-crosswords.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing the Robustness of Retrieval-Augmented Generation Systems in
  K-12 Educational Question Answering with Knowledge Discrepancies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianshi Zheng, Weihan Li, Jiaxin Bai, Weiqi Wang, Yangqiu Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) systems have demonstrated remarkable
potential as question answering systems in the K-12 Education domain, where
knowledge is typically queried within the restricted scope of authoritative
textbooks. However, the discrepancy between textbooks and the parametric
knowledge in Large Language Models (LLMs) could undermine the effectiveness of
RAG systems. To systematically investigate the robustness of RAG systems under
such knowledge discrepancies, we present EduKDQA, a question answering dataset
that simulates knowledge discrepancies in real applications by applying
hypothetical knowledge updates in answers and source documents. EduKDQA
includes 3,005 questions covering five subjects, under a comprehensive question
typology from the perspective of context utilization and knowledge integration.
We conducted extensive experiments on retrieval and question answering
performance. We find that most RAG systems suffer from a substantial
performance drop in question answering with knowledge discrepancies, while
questions that require integration of contextual knowledge and parametric
knowledge pose a challenge to LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World
  Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiwen Zhou, Wenyue Hua, Liangming Pan, Sitao Cheng, Xiaobao Wu, En Yu, William Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces RuleArena, a novel and challenging benchmark designed
to evaluate the ability of large language models (LLMs) to follow complex,
real-world rules in reasoning. Covering three practical domains -- airline
baggage fees, NBA transactions, and tax regulations -- RuleArena assesses LLMs'
proficiency in handling intricate natural language instructions that demand
long-context understanding, logical reasoning, and accurate mathematical
computation. Two key attributes distinguish RuleArena from traditional
rule-based reasoning benchmarks: (1) it extends beyond standard first-order
logic representations, and (2) it is grounded in authentic, practical
scenarios, providing insights into the suitability and reliability of LLMs for
real-world applications. Our findings reveal several notable limitations in
LLMs: (1) they struggle to identify and apply the appropriate rules, frequently
becoming confused by similar but distinct regulations, (2) they cannot
consistently perform accurate mathematical computations, even when they
correctly identify the relevant rules, and (3) in general, they perform poorly
in the benchmark. These results highlight significant challenges in advancing
LLMs' rule-guided reasoning capabilities in real-life applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Data and Codes are available at
  https://github.com/skyriver-2000/RuleArena</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reasoning-Aware Query-Focused Summarization over Multi-Table Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08970v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08970v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaochuan Lin, Xiangyong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query-focused summarization over multi-table data is a challenging yet
critical task for extracting precise and relevant information from structured
data. Existing methods often rely on complex preprocessing steps and struggle
to generalize across domains or handle the logical reasoning required for
multi-table queries. In this paper, we propose QueryTableSummarizer++, an
end-to-end generative framework leveraging large language models (LLMs)
enhanced with table-aware pre-training, query-aligned fine-tuning, and
reinforcement learning with feedback. Our method eliminates the need for
intermediate serialization steps and directly generates query-relevant
summaries. Experiments on a benchmark dataset demonstrate that
QueryTableSummarizer++ significantly outperforms state-of-the-art baselines in
terms of BLEU, ROUGE, and F1-score. Additional analyses highlight its
scalability, generalization across domains, and robust handling of complex
queries. Human evaluation further validates the superior quality and practical
applicability of the generated summaries, establishing QueryTableSummarizer++
as a highly effective solution for multi-table summarization tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Align, Generate, Learn: A Novel Closed-Loop Framework for Cross-Lingual
  In-Context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mateo Alejandro Rojas, Rafael Carranza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-lingual in-context learning (XICL) has emerged as a transformative
paradigm for leveraging large language models (LLMs) to tackle multilingual
tasks, especially for low-resource languages. However, existing approaches
often rely on external retrievers or task-specific fine-tuning, limiting their
scalability and generalizability. In this paper, we propose a novel
self-supervised framework that harnesses the generative capabilities of LLMs to
internally select and utilize task-relevant examples. Our method introduces two
key objectives: a retrieval-generation alignment loss to optimize the quality
of selected examples and a semantic coherence loss to ensure cross-lingual
consistency. Through extensive experiments on multilingual benchmarks, our
approach achieves state-of-the-art performance, significantly outperforming
existing baselines. Further analysis highlights its robustness across diverse
language families and its ability to generalize to unseen tasks. Human
evaluations confirm the superior fluency, relevance, and semantic correctness
of outputs generated by our method. This work provides a scalable, effective,
and generalizable solution for cross-lingual in-context learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mojito: Motion Trajectory and Intensity Control for Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuehai He, Shuohang Wang, Jianwei Yang, Xiaoxia Wu, Yiping Wang, Kuan Wang, Zheng Zhan, Olatunji Ruwase, Yelong Shen, Xin Eric Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in diffusion models have shown great promise in producing
high-quality video content. However, efficiently training diffusion models
capable of integrating directional guidance and controllable motion intensity
remains a challenging and under-explored area. This paper introduces Mojito, a
diffusion model that incorporates both \textbf{Mo}tion tra\textbf{j}ectory and
\textbf{i}ntensi\textbf{t}y contr\textbf{o}l for text to video generation.
Specifically, Mojito features a Directional Motion Control module that
leverages cross-attention to efficiently direct the generated object's motion
without additional training, alongside a Motion Intensity Modulator that uses
optical flow maps generated from videos to guide varying levels of motion
intensity. Extensive experiments demonstrate Mojito's effectiveness in
achieving precise trajectory and intensity control with high computational
efficiency, generating motion patterns that closely match specified directions
and intensities, providing realistic dynamics that align well with natural
motion in real-world scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoSLD: An Extremely Parameter-Efficient Mixture-of-Shared LoRAs for
  Multi-Task Learning <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lulu Zhao, Weihao Zeng, Xiaofeng Shi, Hua Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, LoRA has emerged as a crucial technique for fine-tuning large
pre-trained models, yet its performance in multi-task learning scenarios often
falls short. In contrast, the MoE architecture presents a natural solution to
this issue. However, it introduces challenges such as mutual interference of
data across multiple domains and knowledge forgetting of various tasks.
Additionally, MoE significantly increases the number of parameters, posing a
computational cost challenge. Therefore, in this paper, we propose MoSLD, a
mixture-of-shared-LoRAs model with a dropout strategy. MoSLD addresses these
challenges by sharing the upper projection matrix in LoRA among different
experts, encouraging the model to learn general knowledge across tasks, while
still allowing the lower projection matrix to focus on the unique features of
each task. The application of dropout alleviates the imbalanced update of
parameter matrix and mitigates parameter overfitting in LoRA. Extensive
experiments demonstrate that our model exhibits excellent performance in both
single-task and multi-task scenarios, with robust out-of-domain generalization
capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept by COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Scale Heterogeneous Text-Attributed Graph <span class="highlight-title">Dataset</span>s From Diverse
  Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhui Liu, Qizhuo Xie, Jinwei Shi, Jiaxu Shen, Tieke He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Heterogeneous Text-Attributed Graphs (HTAGs), where different types of
entities are not only associated with texts but also connected by diverse
relationships, have gained widespread popularity and application across various
domains. However, current research on text-attributed graph learning
predominantly focuses on homogeneous graphs, which feature a single node and
edge type, thus leaving a gap in understanding how methods perform on HTAGs.
One crucial reason is the lack of comprehensive HTAG datasets that offer
original textual content and span multiple domains of varying sizes. To this
end, we introduce a collection of challenging and diverse benchmark datasets
for realistic and reproducible evaluation of machine learning models on HTAGs.
Our HTAG datasets are multi-scale, span years in duration, and cover a wide
range of domains, including movie, community question answering, academic,
literature, and patent networks. We further conduct benchmark experiments on
these datasets with various graph neural networks. All source data, dataset
construction codes, processed HTAGs, data loaders, benchmark codes, and
evaluation setup are publicly available at GitHub and Hugging Face.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Text to Trajectory: Exploring Complex Constraint Representation and
  Decomposition in Safe Reinforcement Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pusen Dong, Tianchen Zhu, Yue Qiu, Haoyi Zhou, Jianxin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe reinforcement learning (RL) requires the agent to finish a given task
while obeying specific constraints. Giving constraints in natural language form
has great potential for practical scenarios due to its flexible transfer
capability and accessibility. Previous safe RL methods with natural language
constraints typically need to design cost functions manually for each
constraint, which requires domain expertise and lacks flexibility. In this
paper, we harness the dual role of text in this task, using it not only to
provide constraint but also as a training signal. We introduce the
Trajectory-level Textual Constraints Translator (TTCT) to replace the manually
designed cost function. Our empirical results demonstrate that TTCT effectively
comprehends textual constraint and trajectory, and the policies trained by TTCT
can achieve a lower violation rate than the standard cost function. Extra
studies are conducted to demonstrate that the TTCT has zero-shot transfer
capability to adapt to constraint-shift environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Phi-4 Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu, Cyril Zhang, Yi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present phi-4, a 14-billion parameter language model developed with a
training recipe that is centrally focused on data quality. Unlike most language
models, where pre-training is based primarily on organic data sources such as
web content or code, phi-4 strategically incorporates synthetic data throughout
the training process. While previous models in the Phi family largely distill
the capabilities of a teacher model (specifically GPT-4), phi-4 substantially
surpasses its teacher model on STEM-focused QA capabilities, giving evidence
that our data-generation and post-training techniques go beyond distillation.
Despite minimal changes to the phi-3 architecture, phi-4 achieves strong
performance relative to its size -- especially on reasoning-focused benchmarks
-- due to improved data, training curriculum, and innovations in the
post-training scheme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-assisted Knowledge Discovery in Biomedical Literature to Support
  Decision-making in Precision Oncology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting He, Kory Kreimeyer, Mimi Najjar, Jonathan Spiker, Maria Fatteh, Valsamo Anagnostou, Taxiarchis Botsis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The delivery of appropriate targeted therapies to cancer patients requires
the complete analysis of the molecular profiling of tumors and the patient's
clinical characteristics in the context of existing knowledge and recent
findings described in biomedical literature and several other sources. We
evaluated the potential contributions of specific natural language processing
solutions to support knowledge discovery from biomedical literature. Two models
from the Bidirectional Encoder Representations from Transformers (BERT) family,
two Large Language Models, and PubTator 3.0 were tested for their ability to
support the named entity recognition (NER) and the relation extraction (RE)
tasks. PubTator 3.0 and the BioBERT model performed best in the NER task (best
F1-score equal to 0.93 and 0.89, respectively), while BioBERT outperformed all
other solutions in the RE task (best F1-score 0.79) and a specific use case it
was applied to by recognizing nearly all entity mentions and most of the
relations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AMIA Annual Symposium 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Graph-Based Synthetic Data Pipeline for Scaling High-Quality Reasoning
  Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08864v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08864v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiankang Wang, Jianjun Xu, Xiaorui Wang, Yuxin Wang, Mengting Xing, Shancheng Fang, Zhineng Chen, Hongtao Xie, Yongdong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthesizing high-quality reasoning data for continual training has been
proven to be effective in enhancing the performance of Large Language Models
(LLMs). However, previous synthetic approaches struggle to easily scale up data
and incur high costs in the pursuit of high quality. In this paper, we propose
the Graph-based Synthetic Data Pipeline (GSDP), an economical and scalable
framework for high-quality reasoning data synthesis. Inspired by knowledge
graphs, we extracted knowledge points from seed data and constructed a
knowledge point relationships graph to explore their interconnections. By
exploring the implicit relationships among knowledge, our method achieves
$\times$255 data expansion. Furthermore, GSDP led by open-source models,
achieves synthesis quality comparable to GPT-4-0613 while maintaining
$\times$100 lower costs. To tackle the most challenging mathematical reasoning
task, we present the GSDP-MATH dataset comprising over 1.91 million pairs of
math problems and answers. After fine-tuning on GSDP-MATH, GSDP-7B based on
Mistral-7B achieves 37.7% accuracy on MATH and 78.4% on GSM8K, demonstrating
the effectiveness of our method. The dataset and models trained in this paper
will be available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Large Language Models on Cross-Cultural Values in Connection
  with Training Methodology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minsang Kim, Seungjun Baek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) closely interact with humans, and thus need an
intimate understanding of the cultural values of human society. In this paper,
we explore how open-source LLMs make judgments on diverse categories of
cultural values across countries, and its relation to training methodology such
as model sizes, training corpus, alignment, etc. Our analysis shows that LLMs
can judge socio-cultural norms similar to humans but less so on social systems
and progress. In addition, LLMs tend to judge cultural values biased toward
Western culture, which can be improved with training on the multilingual
corpus. We also find that increasing model size helps a better understanding of
social values, but smaller models can be enhanced by using synthetic data. Our
analysis reveals valuable insights into the design methodology of LLMs in
connection with their understanding of cultural values.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Impact of Coding Data Instruction Fine-Tuning on Large
  Language Models Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20535v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20535v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinlu Zhang, Zhiyu Zoey Chen, Xi Ye, Xianjun Yang, Lichang Chen, William Yang Wang, Linda Ruth Petzold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction Fine-Tuning (IFT) significantly enhances the zero-shot
capabilities of pretrained Large Language Models (LLMs). While coding data is
known to boost LLM reasoning abilities during pretraining, its role in
activating internal reasoning capacities during IFT remains understudied. This
paper investigates a key question: How does coding data impact LLMs' reasoning
capacities during IFT stage? To explore this, we thoroughly examine the impact
of coding data across different coding data proportions, model families, sizes,
and reasoning domains, from various perspectives. Specifically, we create three
IFT datasets with increasing coding data proportions, fine-tune six LLM
backbones across different families and scales on these datasets, evaluate the
tuned models' performance across twelve tasks in three reasoning domains, and
analyze the outcomes from three broad-to-granular perspectives: overall,
domain-level, and task-specific. Our holistic analysis provides valuable
insights into each perspective. First, coding data tuning enhances the overall
reasoning capabilities of LLMs across different model families and scales.
Moreover, while the impact of coding data varies by domain, it shows consistent
trends within each domain across different model families and scales.
Additionally, coding data generally provides comparable task-specific benefits
across model families, with optimal proportions in IFT datasets being
task-dependent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Altogether: Image Captioning via Re-aligning Alt-text <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hu Xu, Po-Yao Huang, Xiaoqing Ellen Tan, Ching-Feng Yeh, Jacob Kahn, Christine Jou, Gargi Ghosh, Omer Levy, Luke Zettlemoyer, Wen-tau Yih, Shang-Wen Li, Saining Xie, Christoph Feichtenhofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on creating synthetic data to improve the quality of image
captions. Existing works typically have two shortcomings. First, they caption
images from scratch, ignoring existing alt-text metadata, and second, lack
transparency if the captioners' training data (e.g. GPT) is unknown. In this
paper, we study a principled approach Altogether based on the key idea to edit
and re-align existing alt-texts associated with the images. To generate
training data, we perform human annotation where annotators start with the
existing alt-text and re-align it to the image content in multiple rounds,
consequently constructing captions with rich visual concepts. This differs from
prior work that carries out human annotation as a one-time description task
solely based on images and annotator knowledge. We train a captioner on this
data that generalizes the process of re-aligning alt-texts at scale. Our
results show our Altogether approach leads to richer image captions that also
improve text-to-image generation and zero-shot image classification tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by EMNLP 2024; Meta CLIP 1.2 Data Engine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LCFO: Long Context and Long Form Output <span class="highlight-title">Dataset</span> and Benchmarking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08268v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08268v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marta R. Costa-jussà, Pierre Andrews, Mariano Coria Meglioli, Joy Chen, Joe Chuang, David Dale, Christophe Ropers, Alexandre Mourachko, Eduardo Sánchez, Holger Schwenk, Tuan Tran, Arina Turkatenko, Carleigh Wood
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the Long Context and Form Output (LCFO) benchmark, a
novel evaluation framework for assessing gradual summarization and summary
expansion capabilities across diverse domains. LCFO consists of long input
documents (5k words average length), each of which comes with three summaries
of different lengths (20%, 10%, and 5% of the input text), as well as
approximately 15 questions and answers (QA) related to the input content.
Notably, LCFO also provides alignments between specific QA pairs and
corresponding summaries in 7 domains. The primary motivation behind providing
summaries of different lengths is to establish a controllable framework for
generating long texts from shorter inputs, i.e. summary expansion. To establish
an evaluation metric framework for summarization and summary expansion, we
provide human evaluation scores for human-generated outputs, as well as results
from various state-of-the-art large language models (LLMs). GPT-4o-mini
achieves best human scores among automatic systems in both summarization and
summary expansion tasks (~ +10% and +20%, respectively). It even surpasses
human output quality in the case of short summaries (~ +7%). Overall automatic
metrics achieve low correlations with human evaluation scores (~ 0.4) but
moderate correlation on specific evaluation aspects such as fluency and
attribution (~ 0.6). The LCFO benchmark offers a standardized platform for
evaluating summarization and summary expansion performance, as well as
corresponding automatic metrics, thereby providing an important evaluation
framework to advance generative AI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving the Validity of Automatically Generated Feedback via
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01304v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01304v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Scarlatos, Digory Smith, Simon Woodhead, Andrew Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatically generating feedback via large language models (LLMs) in
intelligent tutoring systems and online learning platforms has the potential to
improve the learning outcomes of many students. However, both feedback
generation and evaluation are challenging: feedback content has to be valid
especially in subjects like math, which requires models to understand the
problem, the solution, and where the student's error lies. Feedback also has to
be pedagogically valid to reflect effective tutoring strategies, such as
explaining possible misconceptions and encouraging the student, among other
desirable features. In this work, we address both problems of automatically
generating and evaluating feedback while considering both correctness and
alignment. First, we propose a rubric for evaluating math feedback and show
that GPT-4 is able to effectively use it to annotate human-written and
LLM-generated feedback. Second, we propose a framework for feedback generation
that optimizes both correctness and alignment using reinforcement learning
(RL). Specifically, we use GPT-4's annotations to create preferences over
feedback pairs in an augmented dataset for training via direct preference
optimization (DPO). We show that our methods significantly increase the
correctness and alignment of generated feedback with Llama 2, an open-source
LLM, qualitatively analyze our generation and evaluation systems using case
studies, and outline several areas for future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Best student paper award, Published in AIED 2024: The 25th
  International Conference on Artificial Intelligence in Education</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating <span class="highlight-title">GPT</span>-4 at Grading Handwritten Solutions in Math Exams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05231v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05231v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adriana Caraeni, Alexander Scarlatos, Andrew Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in generative artificial intelligence (AI) have shown promise
in accurately grading open-ended student responses. However, few prior works
have explored grading handwritten responses due to a lack of data and the
challenge of combining visual and textual information. In this work, we
leverage state-of-the-art multi-modal AI models, in particular GPT-4o, to
automatically grade handwritten responses to college-level math exams. Using
real student responses to questions in a probability theory exam, we evaluate
GPT-4o's alignment with ground-truth scores from human graders using various
prompting techniques. We find that while providing rubrics improves alignment,
the model's overall accuracy is still too low for real-world settings, showing
there is significant room for growth in this task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in LAK 2025: The 15th International Learning Analytics and
  Knowledge Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAGED: A Holistic Bias-Benchmarking Pipeline for Language Models with
  Customisable Fairness Calibration <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11149v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11149v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Guan, Nathaniel Demchak, Saloni Gupta, Ze Wang, Ediz Ertekin Jr., Adriano Koshiyama, Emre Kazim, Zekun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of unbiased large language models is widely recognized as
crucial, yet existing benchmarks fall short in detecting biases due to limited
scope, contamination, and lack of a fairness baseline. SAGED(bias) is the first
holistic benchmarking pipeline to address these problems. The pipeline
encompasses five core stages: scraping materials, assembling benchmarks,
generating responses, extracting numeric features, and diagnosing with
disparity metrics. SAGED includes metrics for max disparity, such as impact
ratio, and bias concentration, such as Max Z-scores. Noticing that metric tool
bias and contextual bias in prompts can distort evaluation, SAGED implements
counterfactual branching and baseline calibration for mitigation. For
demonstration, we use SAGED on G20 Countries with popular 8b-level models
including Gemma2, Llama3.1, Mistral, and Qwen2. With sentiment analysis, we
find that while Mistral and Qwen2 show lower max disparity and higher bias
concentration than Gemma2 and Llama3.1, all models are notably biased against
countries like Russia and (except for Qwen2) China. With further experiments to
have models role-playing U.S. presidents, we see bias amplifies and shifts in
heterogeneous directions. Moreover, we see Qwen2 and Mistral not engage in
role-playing, while Llama3.1 and Gemma2 role-play Trump notably more
intensively than Biden and Harris, indicating role-playing performance bias in
these models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2025 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Few-Shot Domain Adaptation for Named-Entity Recognition via Joint
  Constrained k-Means and Subspace Selection <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00426v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00426v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayoub Hammal, Benno Uthayasooriyar, Caio Corro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named-entity recognition (NER) is a task that typically requires large
annotated datasets, which limits its applicability across domains with varying
entity definitions. This paper addresses few-shot NER, aiming to transfer
knowledge to new domains with minimal supervision. Unlike previous approaches
that rely solely on limited annotated data, we propose a weakly supervised
algorithm that combines small labeled datasets with large amounts of unlabeled
data. Our method extends the k-means algorithm with label supervision, cluster
size constraints and domain-specific discriminative subspace selection. This
unified framework achieves state-of-the-art results in few-shot NER on several
English datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18564v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18564v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rong Wang, Kun Sun, Jonas Kuhn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable capabilities across
various tasks, yet they often struggle with spatial reasoning. This paper
presents a novel neural-symbolic framework that enhances LLMs' spatial
reasoning abilities through iterative feedback between LLMs and Answer Set
Programming (ASP). We evaluate our approach on two benchmark datasets: StepGame
and SparQA, implementing three distinct strategies: (1) direct prompting
baseline, (2) Facts+Rules prompting, and (3) DSPy-based LLM+ASP pipeline with
iterative refinement. Our experimental results demonstrate that the LLM+ASP
pipeline significantly outperforms baseline methods, achieving an average 82%
accuracy on StepGame and 69% on SparQA, marking improvements of 40-50% and
8-15% respectively over direct prompting. The success stems from three key
innovations: (1) effective separation of semantic parsing and logical reasoning
through a modular pipeline, (2) iterative feedback mechanism between LLMs and
ASP solvers that improves program rate, and (3) robust error handling that
addresses parsing, grounding, and solving failures. Additionally, we propose
Facts+Rules as a lightweight alternative that achieves comparable performance
on complex SparQA dataset, while reducing computational overhead.Our analysis
across different LLM architectures (Deepseek, Llama3-70B, GPT-4.0 mini)
demonstrates the framework's generalizability and provides insights into the
trade-offs between implementation complexity and reasoning capability,
contributing to the development of more interpretable and reliable AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EVQAScore: Efficient Video Question Answering Data Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06908v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06908v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Liang, Zirong Chen, Wentao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video question-answering (QA) is a core task in video understanding.
Evaluating the quality of video QA and video caption data quality for training
video large language models (VideoLLMs) is an essential challenge. Although
various methods have been proposed for assessing video caption quality, there
remains a lack of dedicated evaluation methods for Video QA. To address this
gap, we introduce EVQAScore, a reference-free method that leverages keyword
extraction to assess both video caption and video QA data quality.
Additionally, we incorporate frame sampling and rescaling techniques to enhance
the efficiency and robustness of our evaluation, this enables our score to
evaluate the quality of extremely long videos. Our approach achieves
state-of-the-art (SOTA) performance (32.8 for Kendall correlation and 42.3 for
Spearman correlation, 4.7 and 5.9 higher than the previous method PAC-S++) on
the VATEX-EVAL benchmark for video caption evaluation. Furthermore, by using
EVQAScore for data selection, we achieved SOTA results with only 12.5\% of the
original data volume, outperforming the previous SOTA method PAC-S and 100\% of
data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detection of Non-recorded Word Senses in English and Swedish 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02285v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02285v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Lautenschlager, Emma Sköldberg, Simon Hengchen, Dominik Schlechtweg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses the task of Unknown Sense Detection in English and
Swedish. The primary objective of this task is to determine whether the meaning
of a particular word usage is documented in a dictionary or not. For this
purpose, sense entries are compared with word usages from modern and historical
corpora using a pre-trained Word-in-Context embedder that allows us to model
this task in a few-shot scenario. Additionally, we use human annotations on the
target corpora to adapt hyperparameters and evaluate our models using 5-fold
cross-validation. Compared to a random sample from a corpus, our model is able
to considerably increase the detected number of word usages with non-recorded
senses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Importance Weighting Can Help Large Language Models Self-Improve 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09849v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09849v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunyang Jiang, Chi-min Chan, Wei Xue, Qifeng Liu, Yike Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable capability in numerous
tasks and applications. However, fine-tuning LLMs using high-quality datasets
under external supervision remains prohibitively expensive. In response, LLM
self-improvement approaches have been vibrantly developed recently. The typical
paradigm of LLM self-improvement involves training LLM on self-generated data,
part of which may be detrimental and should be filtered out due to the unstable
data quality. While current works primarily employs filtering strategies based
on answer correctness, in this paper, we demonstrate that filtering out correct
but with high distribution shift extent (DSE) samples could also benefit the
results of self-improvement. Given that the actual sample distribution is
usually inaccessible, we propose a new metric called DS weight to approximate
DSE, inspired by the Importance Weighting methods. Consequently, we integrate
DS weight with self-consistency to comprehensively filter the self-generated
samples and fine-tune the language model. Experiments show that with only a
tiny valid set (up to 5\% size of the training set) to compute DS weight, our
approach can notably promote the reasoning ability of current LLM
self-improvement methods. The resulting performance is on par with methods that
rely on external supervision from pre-trained reward models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Language Model Generalization in Low-Resource Extractive QA <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18446v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18446v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saptarshi Sengupta, Wenpeng Yin, Preslav Nakov, Shreya Ghosh, Suhang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate Extractive Question Answering (EQA) with Large
Language Models (LLMs) under domain drift, i.e., can LLMs generalize to domains
that require specific knowledge such as medicine and law in a zero-shot fashion
without additional in-domain training? To this end, we devise a series of
experiments to explain the performance gap empirically. Our findings suggest
that: (a) LLMs struggle with dataset demands of closed domains such as
retrieving long answer spans; (b) Certain LLMs, despite showing strong overall
performance, display weaknesses in meeting basic requirements as discriminating
between domain-specific senses of words which we link to pre-processing
decisions; (c) Scaling model parameters is not always effective for cross
domain generalization; and (d) Closed-domain datasets are quantitatively much
different than open-domain EQA datasets and current LLMs struggle to deal with
them. Our findings point out important directions for improving existing LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TOP-Training: Target-Oriented <span class="highlight-title">Pretrain</span>ing for Medical Extractive
  Question Answering <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16995v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16995v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saptarshi Sengupta, Connor Heaton, Shreya Ghosh, Wenpeng Yin, Preslav Nakov, Suhang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study extractive question-answering in the medical domain (Medical-EQA).
This problem has two main challenges: (i) domain specificity, as most AI models
lack necessary domain knowledge, and (ii) extraction-based answering style,
which restricts most autoregressive LLMs due to potential hallucinations. To
handle those challenges, we propose TOP-Training, a target-oriented
pre-training paradigm that stands out among all domain adaptation techniques
with two desirable features: (i) TOP-Training moves one step further than
popular domain-oriented fine-tuning since it not only moves closer to the
target domain, but also familiarizes itself with the target dataset, and (ii)
it does not assume the existence of a large set of unlabeled instances from the
target domain. Specifically, for a target Medical-EQA dataset, we extract its
entities and leverage large language models (LLMs) to generate synthetic texts
containing those entities; we then demonstrate that pretraining on this
synthetic text data yields better performance on the target Medical-EQA
benchmarks. Overall, our contributions are threefold: (i) TOP-Training, a new
pretraining technique to effectively adapt LLMs to better solve a target
problem, (ii) TOP-Training has a wide application scope because it does not
require the target problem to have a large set of unlabeled data, and (iii) our
experiments highlight the limitations of autoregressive LLMs, emphasizing
TOP-Training as a means to unlock the true potential of bidirectional LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Empathy Level Alignment via Reinforcement Learning for Empathetic
  Response Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02976v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02976v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Ma, Bo Zhang, Bo Xu, Jian Wang, Hongfei Lin, Xiao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Empathetic response generation, aiming to understand the user's situation and
feelings and respond empathically, is crucial in building human-like dialogue
systems. Traditional approaches typically employ maximum likelihood estimation
as the optimization objective during training, yet fail to align the empathy
levels between generated and target responses. To this end, we propose an
empathetic response generation framework using reinforcement learning (EmpRL).
The framework develops an effective empathy reward function and generates
empathetic responses by maximizing the expected reward through reinforcement
learning. EmpRL utilizes the pre-trained T5 model as the generator and further
fine-tunes it to initialize the policy. To align the empathy levels between
generated and target responses within a given context, an empathy reward
function containing three empathy communication mechanisms -- emotional
reaction, interpretation, and exploration -- is constructed using pre-designed
and pre-trained empathy identifiers. During reinforcement learning training,
the proximal policy optimization algorithm is used to fine-tune the policy,
enabling the generation of empathetic responses. Both automatic and human
evaluations demonstrate that the proposed EmpRL framework significantly
improves the quality of generated responses, enhances the similarity in empathy
levels between generated and target responses, and produces empathetic
responses covering both affective and cognitive aspects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Likely Do LLMs with CoT Mimic Human Reasoning? <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16048v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16048v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangsheng Bao, Hongbo Zhang, Cunxiang Wang, Linyi Yang, Yue Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-thought emerges as a promising technique for eliciting reasoning
capabilities from Large Language Models (LLMs). However, it does not always
improve task performance or accurately represent reasoning processes, leaving
unresolved questions about its usage. In this paper, we diagnose the underlying
mechanism by comparing the reasoning process of LLMs with humans, using causal
analysis to understand the relationships between the problem instruction,
reasoning, and the answer in LLMs. Our empirical study reveals that LLMs often
deviate from the ideal causal chain, resulting in spurious correlations and
potential consistency errors (inconsistent reasoning and answers). We also
examine various factors influencing the causal structure, finding that
in-context learning with examples strengthens it, while post-training
techniques like supervised fine-tuning and reinforcement learning on human
feedback weaken it. To our surprise, the causal structure cannot be
strengthened by enlarging the model size only, urging research on new
techniques. We hope that this preliminary study will shed light on
understanding and improving the reasoning process in LLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2025 Camera Version (8 pages, 3 figures, 18 tables)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NLPineers@ NLU of Devanagari Script Languages 2025: Hate Speech
  Detection using Ensembling of <span class="highlight-title">BERT</span>-based models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08163v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08163v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anmol Guragain, Nadika Poudel, Rajesh Piryani, Bishesh Khanal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores hate speech detection in Devanagari-scripted languages,
focusing on Hindi and Nepali, for Subtask B of the CHIPSAL@COLING 2025 Shared
Task. Using a range of transformer-based models such as XLM-RoBERTa, MURIL, and
IndicBERT, we examine their effectiveness in navigating the nuanced boundary
between hate speech and free expression. Our best performing model, implemented
as ensemble of multilingual BERT models achieve Recall of 0.7762 (Rank 3/31 in
terms of recall) and F1 score of 0.6914 (Rank 17/31). To address class
imbalance, we used backtranslation for data augmentation, and cosine similarity
to preserve label consistency after augmentation. This work emphasizes the need
for hate speech detection in Devanagari-scripted languages and presents a
foundation for further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity
  within Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13516v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13516v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Song, Xu Han, Zhengyan Zhang, Shengding Hu, Xiyu Shi, Kuai Li, Chen Chen, Zhiyuan Liu, Guangli Li, Tao Yang, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Activation sparsity refers to the existence of considerable
weakly-contributed elements among activation outputs. As a prevalent property
of the models using the ReLU activation function, activation sparsity has been
proven a promising paradigm to boost model inference efficiency. Nevertheless,
most large language models (LLMs) adopt activation functions without intrinsic
activation sparsity (e.g., GELU and Swish). Some recent efforts have explored
introducing ReLU or its variants as the substitutive activation function to
help LLMs achieve activation sparsity and inference acceleration, but few can
simultaneously obtain high sparsity and comparable model performance. This
paper introduces a simple and effective sparsification method named "ProSparse"
to push LLMs for higher activation sparsity while maintaining comparable
performance. Specifically, after substituting the activation function of LLMs
with ReLU, ProSparse adopts progressive sparsity regularization with a factor
smoothly increasing along the multi-stage sine curves. This can enhance
activation sparsity and mitigate performance degradation by avoiding radical
shifts in activation distributions. With ProSparse, we obtain high sparsity of
89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for end-size
MiniCPM-1B, respectively, achieving comparable performance to their original
Swish-activated versions. These present the most sparsely activated models
among open-source LLaMA versions and competitive end-size models, considerably
surpassing ReluLLaMA-7B (66.98%) and ReluLLaMA-13B (71.56%). Our inference
acceleration experiments further demonstrate the significant practical
acceleration potential of LLMs with higher activation sparsity, obtaining up to
4.52$\times$ inference speedup.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 4 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Missing Melodies: AI Music Generation and its "Nearly" Complete Omission
  of the Global South 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atharva Mehta, Shivam Chauhan, Monojit Choudhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in generative AI have sparked renewed interest and expanded
possibilities for music generation. However, the performance and versatility of
these systems across musical genres are heavily influenced by the availability
of training data. We conducted an extensive analysis of over one million hours
of audio datasets used in AI music generation research and manually reviewed
more than 200 papers from eleven prominent AI and music conferences and
organizations (AAAI, ACM, EUSIPCO, EURASIP, ICASSP, ICML, IJCAI, ISMIR,
NeurIPS, NIME, SMC) to identify a critical gap in the fair representation and
inclusion of the musical genres of the Global South in AI research. Our
findings reveal a stark imbalance: approximately 86% of the total dataset hours
and over 93% of researchers focus primarily on music from the Global North.
However, around 40% of these datasets include some form of non-Western music,
genres from the Global South account for only 14.6% of the data. Furthermore,
approximately 51% of the papers surveyed concentrate on symbolic music
generation, a method that often fails to capture the cultural nuances inherent
in music from regions such as South Asia, the Middle East, and Africa. As AI
increasingly shapes the creation and dissemination of music, the significant
underrepresentation of music genres in datasets and research presents a serious
threat to global musical diversity. We also propose some important steps to
mitigate these risks and foster a more inclusive future for AI-driven music
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to CACM, 12 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large language models as oracles for instantiating ontologies with
  domain-specific knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.04108v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.04108v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni Ciatto, Andrea Agiollo, Matteo Magnini, Andrea Omicini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background. Endowing intelligent systems with semantic data commonly requires
designing and instantiating ontologies with domain-specific knowledge.
Especially in the early phases, those activities are typically performed
manually by human experts possibly leveraging on their own experience. The
resulting process is therefore time-consuming, error-prone, and often biased by
the personal background of the ontology designer. Objective. To mitigate that
issue, we propose a novel domain-independent approach to automatically
instantiate ontologies with domain-specific knowledge, by leveraging on large
language models (LLMs) as oracles. Method. Starting from (i) an initial schema
composed by inter-related classes and properties and (ii) a set of query
templates, our method queries the LLM multiple times, and generates instances
for both classes and properties from its replies. Thus, the ontology is
automatically filled with domain-specific knowledge, compliant to the initial
schema. As a result, the ontology is quickly and automatically enriched with
manifold instances, which experts may consider to keep, adjust, discard, or
complement according to their own needs and expertise. Contribution. We
formalise our method in general way and instantiate it over various LLMs, as
well as on a concrete case study. We report experiments rooted in the
nutritional domain where an ontology of food meals and their ingredients is
automatically instantiated from scratch, starting from a categorisation of
meals and their relationships. There, we analyse the quality of the generated
ontologies and compare ontologies attained by exploiting different LLMs.
Experimentally, our approach achieves a quality metric that is up to five times
higher than the state-of-the-art, while reducing erroneous entities and
relations by up to ten times. Finally, we provide a SWOT analysis of the
proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniBias: Unveiling and Mitigating LLM Bias through Internal Attention
  and FFN Manipulation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20612v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20612v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanzhang Zhou, Zijian Feng, Zixiao Zhu, Junlang Qian, Kezhi Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated impressive capabilities in
various tasks using the in-context learning (ICL) paradigm. However, their
effectiveness is often compromised by inherent bias, leading to prompt
brittleness, i.e., sensitivity to design settings such as example selection,
order, and prompt formatting. Previous studies have addressed LLM bias through
external adjustment of model outputs, but the internal mechanisms that lead to
such bias remain unexplored. Our work delves into these mechanisms,
particularly investigating how feedforward neural networks (FFNs) and attention
heads result in the bias of LLMs. By Interpreting the contribution of
individual FFN vectors and attention heads, we identify the biased LLM
components that skew LLMs' prediction toward specific labels. To mitigate these
biases, we introduce UniBias, an inference-only method that effectively
identifies and eliminates biased FFN vectors and attention heads. Extensive
experiments across 12 NLP datasets demonstrate that UniBias significantly
enhances ICL performance and alleviates prompt brittleness of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TCM-FTP: Fine-Tuning Large Language Models for Herbal Prescription
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10510v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10510v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingzhi Zhou, Xin Dong, Chunhao Li, Yuning Bai, Yulong Xu, Ka Chun Cheung, Simon See, Xinpeng Song, Runshun Zhang, Xuezhong Zhou, Nevin L. Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional Chinese medicine (TCM) has relied on specific combinations of
herbs in prescriptions to treat various symptoms and signs for thousands of
years. Predicting TCM prescriptions poses a fascinating technical challenge
with significant practical implications. However, this task faces limitations
due to the scarcity of high-quality clinical datasets and the complex
relationship between symptoms and herbs. To address these issues, we introduce
\textit{DigestDS}, a novel dataset comprising practical medical records from
experienced experts in digestive system diseases. We also propose a method,
TCM-FTP (TCM Fine-Tuning Pre-trained), to leverage pre-trained large language
models (LLMs) via supervised fine-tuning on \textit{DigestDS}. Additionally, we
enhance computational efficiency using a low-rank adaptation technique.
Moreover, TCM-FTP incorporates data augmentation by permuting herbs within
prescriptions, exploiting their order-agnostic nature. Impressively, TCM-FTP
achieves an F1-score of 0.8031, significantly outperforming previous methods.
Furthermore, it demonstrates remarkable accuracy in dosage prediction,
achieving a normalized mean square error of 0.0604. In contrast, LLMs without
fine-tuning exhibit poor performance. Although LLMs have demonstrated
wide-ranging capabilities, our work underscores the necessity of fine-tuning
for TCM prescription prediction and presents an effective way to accomplish
this.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version to be published in BIBM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08237v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08237v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingchen Song, Mengtao Xing, Changwei Ma, Shengqiang Li, Di Wu, Binbin Zhang, Fuping Pan, Dinghao Zhou, Yuekai Zhang, Shun Lei, Zhendong Peng, Zhiyong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is well known that LLM-based systems are data-hungry. Recent LLM-based TTS
works typically employ complex data processing pipelines to obtain high-quality
training data. These sophisticated pipelines require excellent models at each
stage (e.g., speech denoising, speech enhancement, speaker diarization, and
punctuation models), which themselves demand high-quality training data and are
rarely open-sourced. Even with state-of-the-art models, issues persist, such as
incomplete background noise removal and misalignment between punctuation and
actual speech pauses. Moreover, the stringent filtering strategies often retain
only 10-30\% of the original data, significantly impeding data scaling efforts.
In this work, we leverage a noise-robust audio tokenizer (S3Tokenizer) to
design a simplified yet effective TTS data processing pipeline that maintains
data quality while substantially reducing data acquisition costs, achieving a
data retention rate of over 50\%. Beyond data scaling challenges, LLM-based TTS
systems also incur higher deployment costs compared to conventional approaches.
Current systems typically use LLMs solely for text-to-token generation, while
requiring separate models (e.g., flow matching models) for token-to-waveform
generation, which cannot be directly executed by LLM inference engines, further
complicating deployment. To address these challenges, we eliminate redundant
modules in both LLM and flow components, replacing the flow model backbone with
an LLM architecture. Building upon this simplified flow backbone, we propose a
unified architecture for both streaming and non-streaming inference,
significantly reducing deployment costs. Finally, we explore the feasibility of
unifying TTS and ASR tasks using the same data for training, thanks to the
simplified pipeline and the S3Tokenizer that reduces the quality requirements
for TTS training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProSwitch: Knowledge-Guided Instruction Tuning to Switch Between
  Professional and Non-Professional Responses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09131v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09131v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Zong, Yuyan Chen, Weiming Lu, Jian Shao, Yongfeng Huang, Heng Chang, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated efficacy in various linguistic
applications, including question answering and controlled text generation.
However, studies into their ability to switch between opposite styles of
responses in professional domains remain underexplored. This study introduces a
novel approach, named ProSwitch, which enables a language model to switch
between professional and non-professional answers, by tuning and evaluating
through the guidance of domain and style knowledge. ProSwitch unfolds in three
phases: LLM-augmented preparation to collect domain knowledge and QA pairs,
instruction tuning to optimize LLMs with multiple levels of knowledge, and
comprehensive evaluation to assess both style discrimination and
reference-based quality of the generated text. Comparative analysis of
ProSwitch against general and specialized LLMs reveals that our approach
outperforms baselines in switching between professional and non-professional
responses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages main body, 16 pages total</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evolving Alignment via Asymmetric Self-Play 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00062v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00062v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Ye, Rishabh Agarwal, Tianqi Liu, Rishabh Joshi, Sarmishta Velury, Quoc V. Le, Qijun Tan, Yuan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current RLHF frameworks for aligning large language models (LLMs) typically
assume a fixed prompt distribution, which is sub-optimal and limits the
scalability of alignment and generalizability of models. To address this, we
introduce a general open-ended RLHF framework that casts alignment as an
asymmetric game between two players: (i) a creator that generates increasingly
informative prompt distributions using reward signals, and (ii) a solver that
learns to produce more preferred responses on prompts produced by the creator.
This framework of Evolving Alignment via Asymmetric Self-Play (eva), results in
a simple and efficient approach that can utilize any existing RLHF algorithm
for scalable alignment. eva outperforms state-of-the-art methods on widely-used
benchmarks, without the need of any additional human crafted prompts.
Specifically, eva improves the win rate of Gemma-2-9B-it on Arena-Hard from
51.6% to 60.1% with DPO, from 55.7% to 58.9% with SPPO, from 52.3% to 60.7%
with SimPO, and from 54.8% to 60.3% with ORPO, surpassing its 27B version and
matching claude-3-opus. This improvement is persistent even when new human
crafted prompts are introduced. Finally, we show eva is effective and robust
under various ablation settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, spotlight @ neurips language gamification workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Match, Compare, or Select? An Investigation of Large Language Models for
  Entity Matching <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16884v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16884v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianshu Wang, Xiaoyang Chen, Hongyu Lin, Xuanang Chen, Xianpei Han, Hao Wang, Zhenyu Zeng, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity matching (EM) is a critical step in entity resolution (ER). Recently,
entity matching based on large language models (LLMs) has shown great promise.
However, current LLM-based entity matching approaches typically follow a binary
matching paradigm that ignores the global consistency among record
relationships. In this paper, we investigate various methodologies for
LLM-based entity matching that incorporate record interactions from different
perspectives. Specifically, we comprehensively compare three representative
strategies: matching, comparing, and selecting, and analyze their respective
advantages and challenges in diverse scenarios. Based on our findings, we
further design a compound entity matching framework (ComEM) that leverages the
composition of multiple strategies and LLMs. ComEM benefits from the advantages
of different sides and achieves improvements in both effectiveness and
efficiency. Experimental results on 8 ER datasets and 10 LLMs verify the
superiority of incorporating record interactions through the selecting
strategy, as well as the further cost-effectiveness brought by ComEM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at COLING 2025. Our code is available at
  https://github.com/tshu-w/ComEM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding the RoPE Extensions of Long-Context LLMs: An Attention
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13282v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13282v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meizhi Zhong, Chen Zhang, Yikun Lei, Xikai Liu, Yan Gao, Yao Hu, Kehai Chen, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enabling LLMs to handle lengthy context is currently a research hotspot. Most
LLMs are built upon rotary position embedding (RoPE), a popular position
encoding method. Therefore, a prominent path is to extrapolate the RoPE trained
on comparably short texts to far longer texts. A heavy bunch of efforts have
been dedicated to boosting the extrapolation via extending the formulations of
the RoPE, however, few of them have attempted to showcase their inner workings
comprehensively. In this paper, we are driven to offer a straightforward yet
in-depth understanding of RoPE extensions from an attention perspective and on
two benchmarking tasks. A broad array of experiments reveals several valuable
findings: 1) Maintaining attention patterns to those at the pretrained length
improves extrapolation; 2) Large attention uncertainty leads to retrieval
errors; 3) Using longer continual pretraining lengths for RoPE extensions could
reduce attention uncertainty and significantly enhance extrapolation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reasoning Graph Enhanced Exemplars Retrieval for In-Context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11147v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11147v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukang Lin, Bingchen Zhong, Shuoran Jiang, Joanna Siebert, Qingcai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have exhibited remarkable few-shot learning
capabilities and unified the paradigm of NLP tasks through the in-context
learning (ICL) technique. Despite the success of ICL, the quality of the
exemplar demonstrations can significantly influence the LLM's performance.
Existing exemplar selection methods mainly focus on the semantic similarity
between queries and candidate exemplars. On the other hand, the logical
connections between reasoning steps can be beneficial to depict the
problem-solving process as well. In this paper, we proposes a novel method
named Reasoning Graph-enhanced Exemplar Retrieval (RGER). RGER first quires LLM
to generate an initial response, then expresses intermediate problem-solving
steps to a graph structure. After that, it employs graph kernel to select
exemplars with semantic and structural similarity. Extensive experiments
demonstrate the structural relationship is helpful to the alignment of queries
and candidate exemplars. The efficacy of RGER on math and logit reasoning tasks
showcases its superiority over state-of-the-art retrieval-based approaches. Our
code is released at https://github.com/Yukang-Lin/RGER.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training on the Test Task Confounds Evaluation and Emergence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07890v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07890v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ricardo Dominguez-Olmedo, Florian E. Dorner, Moritz Hardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a fundamental problem in the evaluation of large language models
that we call training on the test task. Unlike wrongful practices like training
on the test data, leakage, or data contamination, training on the test task is
not a malpractice. Rather, the term describes a growing set of practices that
utilize knowledge about evaluation tasks at training time. We demonstrate that
training on the test task confounds both relative model evaluations and claims
about emergent capabilities. We argue that the seeming superiority of one model
family over another may be explained by a different degree of training on the
test task. To this end, we propose an effective method to adjust for the effect
of training on the test task on benchmark evaluations. Put simply, to fine-tune
each model under comparison on the same task-relevant data before evaluation.
We then show that instances of emergent behavior disappear gradually as models
train on the test task. Our work promotes a new perspective on the evaluation
of large language models with broad implications for benchmarking and the study
of emergent capabilities
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning and Machine Learning, Advancing Big Data Analytics and
  Management: Unveiling AI's Potential Through Tools, Techniques, and
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01268v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01268v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pohsun Feng, Ziqian Bi, Yizhu Wen, Xuanhe Pan, Benji Peng, Ming Liu, Jiawei Xu, Keyu Chen, Junyu Liu, Caitlyn Heqi Yin, Sen Zhang, Jinlang Wang, Qian Niu, Ming Li, Tianyang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI), machine learning, and deep learning have become
transformative forces in big data analytics and management, enabling
groundbreaking advancements across diverse industries. This article delves into
the foundational concepts and cutting-edge developments in these fields, with a
particular focus on large language models (LLMs) and their role in natural
language processing, multimodal reasoning, and autonomous decision-making.
Highlighting tools such as ChatGPT, Claude, and Gemini, the discussion explores
their applications in data analysis, model design, and optimization.
  The integration of advanced algorithms like neural networks, reinforcement
learning, and generative models has enhanced the capabilities of AI systems to
process, visualize, and interpret complex datasets. Additionally, the emergence
of technologies like edge computing and automated machine learning (AutoML)
democratizes access to AI, empowering users across skill levels to engage with
intelligent systems. This work also underscores the importance of ethical
considerations, transparency, and fairness in the deployment of AI
technologies, paving the way for responsible innovation.
  Through practical insights into hardware configurations, software
environments, and real-world applications, this article serves as a
comprehensive resource for researchers and practitioners. By bridging
theoretical underpinnings with actionable strategies, it showcases the
potential of AI and LLMs to revolutionize big data management and drive
meaningful advancements across domains such as healthcare, finance, and
autonomous systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This book contains 155 pages and 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Controlled Evaluation of Syntactic Knowledge in Multilingual Language
  Models <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07474v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07474v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daria Kryvosheieva, Roger Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) are capable of acquiring elements of human-like
syntactic knowledge. Targeted syntactic evaluation tests have been employed to
measure how well they form generalizations about syntactic phenomena in
high-resource languages such as English. However, we still lack a thorough
understanding of LMs' capacity for syntactic generalizations in low-resource
languages, which are responsible for much of the diversity of syntactic
patterns worldwide. In this study, we develop targeted syntactic evaluation
tests for three low-resource languages (Basque, Hindi, and Swahili) and use
them to evaluate five families of open-access multilingual Transformer LMs. We
find that some syntactic tasks prove relatively easy for LMs while others
(agreement in sentences containing indirect objects in Basque, agreement across
a prepositional phrase in Swahili) are challenging. We additionally uncover
issues with publicly available Transformers, including a bias toward the
habitual aspect in Hindi in multilingual BERT and underperformance compared to
similar-sized models in XGLM-4.5B.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LoResLM workshop at COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Limitations of Detecting Machine-Generated Text <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11073v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11073v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jad Doughman, Osama Mohammed Afzal, Hawau Olamide Toyin, Shady Shehata, Preslav Nakov, Zeerak Talat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent improvements in the quality of the generations by large language
models have spurred research into identifying machine-generated text. Such work
often presents high-performing detectors. However, humans and machines can
produce text in different styles and domains, yet the performance impact of
such on machine generated text detection systems remains unclear. In this
paper, we audit the classification performance for detecting machine-generated
text by evaluating on texts with varying writing styles. We find that
classifiers are highly sensitive to stylistic changes and differences in text
complexity, and in some cases degrade entirely to random classifiers. We
further find that detection systems are particularly susceptible to misclassify
easy-to-read texts while they have high performance for complex texts, leading
to concerns about the reliability of detection systems. We recommend that
future work attends to stylistic factors and reading difficulty levels of
human-written and machine-generated text.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guiding Vision-Language Model Selection for Visual Question-Answering
  Across Tasks, Domains, and Knowledge Types <span class="chip">COLING</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09269v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09269v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neelabh Sinha, Vinija Jain, Aman Chadha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Question-Answering (VQA) has become key to user experience,
particularly after improved generalization capabilities of Vision-Language
Models (VLMs). But evaluating VLMs for an application requirement using a
standardized framework in practical settings is still challenging. This paper
aims to solve that using an end-to-end framework. We present VQA360 - a novel
dataset derived from established VQA benchmarks, annotated with task types,
application domains, and knowledge types, for a comprehensive evaluation. We
also introduce GoEval, a multimodal evaluation metric developed using GPT-4o,
achieving a correlation factor of 56.71% with human judgments. Our experiments
with state-of-the-art VLMs reveal that no single model excels universally,
thus, making a right choice a key design decision. Proprietary models such as
Gemini-1.5-Pro and GPT-4o-mini generally outperform others, but open-source
models like InternVL-2-8B and CogVLM-2-Llama-3-19B also demonstrate competitive
strengths, while providing additional advantages. Our framework can also be
extended to other tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at The First Workshop of Evaluation of Multi-Modal
  Generation (EvalMG) in 31st International Conference on Computational
  Linguistics (COLING), 2025. 8 pages + references + 6 pages of Appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rumor Detection on Social Media with Temporal Propagation Structure
  Optimization <span class="chip">COLING'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08316v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08316v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Peng, Junran Wu, Ruomei Liu, Ke Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional methods for detecting rumors on social media primarily focus on
analyzing textual content, often struggling to capture the complexity of online
interactions. Recent research has shifted towards leveraging graph neural
networks to model the hierarchical conversation structure that emerges during
rumor propagation. However, these methods tend to overlook the temporal aspect
of rumor propagation and may disregard potential noise within the propagation
structure. In this paper, we propose a novel approach that incorporates
temporal information by constructing a weighted propagation tree, where the
weight of each edge represents the time interval between connected posts.
Drawing upon the theory of structural entropy, we transform this tree into a
coding tree. This transformation aims to preserve the essential structure of
rumor propagation while reducing noise. Finally, we introduce a recursive
neural network to learn from the coding tree for rumor veracity prediction.
Experimental results on two common datasets demonstrate the superiority of our
approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VickreyFeedback: Cost-efficient Data Construction for Reinforcement
  Learning from Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18417v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18417v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxi Zhang, Jiuding Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the cost-efficiency aspect of Reinforcement Learning
from Human Feedback (RLHF). RLHF leverages datasets of human preferences over
outputs of large language models (LLM)s to instill human expectations into
LLMs. Although preference annotation comes with a monetized cost, the economic
utility of a preference dataset has not been considered by far. What
exacerbates this situation is that, given complex intransitive or cyclic
relationships in preference datasets, existing algorithms for fine-tuning LLMs
are still far from capturing comprehensive preferences. This raises severe
cost-efficiency concerns in production environments, where preference data
accumulate over time. In this paper, we discuss the fine-tuning of LLMs as a
monetized economy and introduce an auction mechanism to improve the efficiency
of preference data collection in dollar terms. We show that introducing an
auction mechanism can play an essential role in enhancing the cost-efficiency
of RLHF, while maintaining satisfactory model performance. Experimental results
demonstrate that our proposed auction-based protocol is cost-effective for
fine-tuning LLMs concentrating on high-quality feedback.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Return of EM: Entity-driven Answer Set Expansion for QA Evaluation <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15650v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15650v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongryeol Lee, Minwoo Lee, Kyungmin Min, Joonsuk Park, Kyomin Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, directly using large language models (LLMs) has been shown to be
the most reliable method to evaluate QA models. However, it suffers from
limited interpretability, high cost, and environmental harm. To address these,
we propose to use soft EM with entity-driven answer set expansion. Our approach
expands the gold answer set to include diverse surface forms, based on the
observation that the surface forms often follow particular patterns depending
on the entity type. The experimental results show that our method outperforms
traditional evaluation methods by a large margin. Moreover, the reliability of
our evaluation method is comparable to that of LLM-based ones, while offering
the benefits of high interpretability and reduced environmental harm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at COLING 2025 (16 pages, 4 figures, 11 tables)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learn To be Efficient: Build Structured Sparsity in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06126v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06126v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haizhong Zheng, Xiaoyan Bai, Xueshen Liu, Z. Morley Mao, Beidi Chen, Fan Lai, Atul Prakash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved remarkable success with their
billion-level parameters, yet they incur high inference overheads. The
emergence of activation sparsity in LLMs provides a natural approach to reduce
this cost by involving only parts of the parameters for inference. However,
existing methods only focus on utilizing this naturally formed activation
sparsity in a post-training setting, overlooking the potential for further
amplifying this inherent sparsity. In this paper, we hypothesize that LLMs can
learn to be efficient by achieving more structured activation sparsity. To
achieve this, we introduce a novel training algorithm, Learn-To-be-Efficient
(LTE), designed to train efficiency-aware LLMs to learn to activate fewer
neurons and achieve a better trade-off between sparsity and performance.
Furthermore, unlike SOTA MoEfication methods, which mainly focus on ReLU-based
models, LTE can also be applied to LLMs like LLaMA using non-ReLU activations.
Extensive evaluation on language understanding, language generation, and
instruction tuning tasks show that LTE consistently outperforms SOTA baselines.
Along with our hardware-aware custom kernel implementation, LTE reduces
LLaMA2-7B inference latency by 25% at 50% sparsity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Universal <span class="highlight-title">Prompt</span> Optimizer for Safe Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10882v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10882v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongyu Wu, Hongcheng Gao, Yueze Wang, Xiang Zhang, Suhang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-Image (T2I) models have shown great performance in generating images
based on textual prompts. However, these models are vulnerable to unsafe input
to generate unsafe content like sexual, harassment and illegal-activity images.
Existing studies based on image checker, model fine-tuning and embedding
blocking are impractical in real-world applications. Hence, we propose the
first universal prompt optimizer for safe T2I (POSI) generation in black-box
scenario. We first construct a dataset consisting of toxic-clean prompt pairs
by GPT-3.5 Turbo. To guide the optimizer to have the ability of converting
toxic prompt to clean prompt while preserving semantic information, we design a
novel reward function measuring toxicity and text alignment of generated images
and train the optimizer through Proximal Policy Optimization. Experiments show
that our approach can effectively reduce the likelihood of various T2I models
in generating inappropriate images, with no significant impact on text
alignment. It is also flexible to be combined with methods to achieve better
performance. Our code is available at https://github.com/wu-zongyu/POSI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive <span class="highlight-title">Prompt</span>ing for Continual Relation Extraction: A Within-Task
  Variance Perspective <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08285v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08285v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh Le, Tien Ngoc Luu, An Nguyen The, Thanh-Thien Le, Trang Nguyen, Tung Thanh Nguyen, Linh Ngo Van, Thien Huu Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address catastrophic forgetting in Continual Relation Extraction (CRE),
many current approaches rely on memory buffers to rehearse previously learned
knowledge while acquiring new tasks. Recently, prompt-based methods have
emerged as potent alternatives to rehearsal-based strategies, demonstrating
strong empirical performance. However, upon analyzing existing prompt-based
approaches for CRE, we identified several critical limitations, such as
inaccurate prompt selection, inadequate mechanisms for mitigating forgetting in
shared parameters, and suboptimal handling of cross-task and within-task
variances. To overcome these challenges, we draw inspiration from the
relationship between prefix-tuning and mixture of experts, proposing a novel
approach that employs a prompt pool for each task, capturing variations within
each task while enhancing cross-task variances. Furthermore, we incorporate a
generative model to consolidate prior knowledge within shared parameters,
eliminating the need for explicit data storage. Extensive experiments validate
the efficacy of our approach, demonstrating superior performance over
state-of-the-art prompt-based and rehearsal-free methods in continual relation
extraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Trustful LLMs: Customizing and Grounding Text Generation with Knowledge
  Bases and Dual Decoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07870v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07870v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Zhu, Jaya Krishna Mandivarapu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although people are impressed by the content generation skills of large
language models, the use of LLMs, such as ChatGPT, is limited by the domain
grounding of the content. The correctness and groundedness of the generated
content need to be based on a verified context, such as results from
Retrieval-Augmented Generation (RAG). One important issue when adapting LLMs to
a customized domain is that the generated responses are often incomplete, or
the additions are not verified and may even be hallucinated. Prior studies on
hallucination detection have focused on evaluation metrics, which are not
easily adaptable to dynamic domains and can be vulnerable to attacks like
jail-breaking. In this work, we propose 1) a post-processing algorithm that
leverages knowledge triplets in RAG context to correct hallucinations and 2) a
dual-decoder model that fuses RAG context to guide the generation process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Frame Representation Hypothesis: Multi-Token LLM Interpretability and
  Concept-Guided Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07334v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07334v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro H. V. Valois, Lincon S. Souza, Erica K. Shimomoto, Kazuhiro Fukui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpretability is a key challenge in fostering trust for Large Language
Models (LLMs), which stems from the complexity of extracting reasoning from
model's parameters. We present the Frame Representation Hypothesis, a
theoretically robust framework grounded in the Linear Representation Hypothesis
(LRH) to interpret and control LLMs by modeling multi-token words. Prior
research explored LRH to connect LLM representations with linguistic concepts,
but was limited to single token analysis. As most words are composed of several
tokens, we extend LRH to multi-token words, thereby enabling usage on any
textual data with thousands of concepts. To this end, we propose words can be
interpreted as frames, ordered sequences of vectors that better capture
token-word relationships. Then, concepts can be represented as the average of
word frames sharing a common concept. We showcase these tools through Top-k
Concept-Guided Decoding, which can intuitively steer text generation using
concepts of choice. We verify said ideas on Llama 3.1, Gemma 2, and Phi 3
families, demonstrating gender and language biases, exposing harmful content,
but also potential to remediate them, leading to safer and more transparent
LLMs. Code is available at
https://github.com/phvv-me/frame-representation-hypothesis.git
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ If You Can't Use Them, Recycle Them: Optimizing Merging at Scale
  Mitigates Performance Tradeoffs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04144v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04144v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Khalifa, Yi-Chern Tan, Arash Ahmadian, Tom Hosking, Honglak Lee, Lu Wang, Ahmet Üstün, Tom Sherborne, Matthias Gallé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model merging has shown great promise at combining expert models, but the
benefit of merging is unclear when merging ``generalist'' models trained on
many tasks. We explore merging in the context of large (~100B) models, by
recycling checkpoints that exhibit tradeoffs among different tasks. Such
checkpoints are often created in the process of developing a frontier model,
and many suboptimal ones are usually discarded. Given a pool of model
checkpoints obtained from different training runs (e.g., different stages,
objectives, hyperparameters, and data mixtures), which naturally show tradeoffs
across different language capabilities (e.g., instruction following vs. code
generation), we investigate whether merging can recycle such suboptimal models
into a Pareto-optimal one. Our optimization algorithm tunes the weight of each
checkpoint in a linear combination, resulting in a Pareto-optimal models that
outperforms both individual models and merge-based baselines. Further analysis
shows that good merges tend to include almost all checkpoints with non-zero
weights, indicating that even seemingly bad initial checkpoints can contribute
to good final merges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revolutionizing Finance with LLMs: An <span class="highlight-title">Overview</span> of Applications and
  Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11641v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11641v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huaqin Zhao, Zhengliang Liu, Zihao Wu, Yiwei Li, Tianze Yang, Peng Shu, Shaochen Xu, Haixing Dai, Lin Zhao, Hanqi Jiang, Yi Pan, Junhao Chen, Yifan Zhou, Gengchen Mai, Ninghao Liu, Tianming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Large Language Models (LLMs) like ChatGPT have seen
considerable advancements and have been applied in diverse fields. Built on the
Transformer architecture, these models are trained on extensive datasets,
enabling them to understand and generate human language effectively. In the
financial domain, the deployment of LLMs is gaining momentum. These models are
being utilized for automating financial report generation, forecasting market
trends, analyzing investor sentiment, and offering personalized financial
advice. Leveraging their natural language processing capabilities, LLMs can
distill key insights from vast financial data, aiding institutions in making
informed investment choices and enhancing both operational efficiency and
customer satisfaction. In this study, we provide a comprehensive overview of
the emerging integration of LLMs into various financial tasks. Additionally, we
conducted holistic tests on multiple financial tasks through the combination of
natural language instructions. Our findings show that GPT-4 effectively follow
prompt instructions across various financial tasks. This survey and evaluation
of LLMs in the financial domain aim to deepen the understanding of LLMs'
current role in finance for both financial practitioners and LLM researchers,
identify new research and application prospects, and highlight how these
technologies can be leveraged to solve practical challenges in the finance
industry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI-Press: A Multi-Agent News Generating and Feedback Simulation System
  Powered by Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07561v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07561v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiawei Liu, Shiyue Yang, Xinnong Zhang, Haoyu Kuang, Libo Sun, Yihang Yang, Siming Chen, Xuanjing Huang, Zhongyu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of various social platforms has transformed journalism. The growing
demand for news content has led to the increased use of large language models
(LLMs) in news production due to their speed and cost-effectiveness. However,
LLMs still encounter limitations in professionalism and ethical judgment in
news generation. Additionally, predicting public feedback is usually difficult
before news is released. To tackle these challenges, we introduce AI-Press, an
automated news drafting and polishing system based on multi-agent collaboration
and Retrieval-Augmented Generation. We develop a feedback simulation system
that generates public feedback considering demographic distributions. Through
extensive quantitative and qualitative evaluations, our system shows
significant improvements in news-generating capabilities and verifies the
effectiveness of public feedback simulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Counting-Stars: A Multi-evidence, Position-aware, and Scalable Benchmark
  for Evaluating Long-Context Large Language Models <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11802v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11802v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyang Song, Mao Zheng, Xuan Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent efforts to develop large language models with robust
long-context capabilities, the lack of long-context benchmarks means that
relatively little is known about their performance. To alleviate this gap, in
this paper, we propose \textbf{Counting-Stars}, a multi-evidence,
position-aware, and scalable benchmark designed to evaluate the multi-evidence
retrieval capabilities of long-context LLMs. \textbf{Counting-Stars} comprises
two counting-based multiple pieces of evidence retrieval tasks: searching and
reasoning. Using Counting-Stars, we conducted experiments to evaluate several
long-context LLMs, including GPT-4 Turbo, Gemini 1.5 Pro, Claude3 Opus, GLM-4,
and Moonshot-v1. Extensive experimental results demonstrate that Gemini 1.5 Pro
achieves the best overall results, while GPT-4 Turbo exhibits the most stable
performance across various tasks. Furthermore, our analysis of these LLMs,
which have been extended to handle long-context scenarios, indicates that
significant room for improvement remains as the length of the input context and
the complexity of the tasks increase.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical <span class="highlight-title">Prompt</span>ing Taxonomy: A Universal Evaluation Framework for
  Large Language Models Aligned with Human Cognitive Principles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12644v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12644v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devichand Budagam, Ashutosh Kumar, Mahsa Khoshnoodi, Sankalp KJ, Vinija Jain, Aman Chadha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessing the effectiveness of large language models (LLMs) in performing
different tasks is crucial for understanding their strengths and weaknesses.
This paper presents Hierarchical Prompting Taxonomy (HPT), grounded on human
cognitive principles and designed to assess LLMs by examining the cognitive
demands of various tasks. The HPT utilizes the Hierarchical Prompting Framework
(HPF), which structures five unique prompting strategies in a hierarchical
order based on their cognitive requirement on LLMs when compared to human
mental capabilities. It assesses the complexity of tasks with the Hierarchical
Prompting Index (HPI), which demonstrates the cognitive competencies of LLMs
across diverse datasets and offers insights into the cognitive demands that
datasets place on different LLMs. This approach enables a comprehensive
evaluation of an LLMs problem solving abilities and the intricacy of a dataset,
offering a standardized metric for task complexity. Extensive experiments with
multiple datasets and LLMs show that HPF enhances LLM performance by 2% to 63%
compared to baseline performance, with GSM8k being the most cognitively complex
task among reasoning and coding tasks with an average HPI of 3.20 confirming
the effectiveness of HPT. To support future research and reproducibility in
this domain, the implementations of HPT and HPF are available here.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probability of Differentiation Reveals Brittleness of Homogeneity Bias
  in <span class="highlight-title">GPT</span>-4 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07329v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07329v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Messi H. J. Lee, Calvin K. Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Homogeneity bias in Large Language Models (LLMs) refers to their tendency to
homogenize the representations of some groups compared to others. Previous
studies documenting this bias have predominantly used encoder models, which may
have inadvertently introduced biases. To address this limitation, we prompted
GPT-4 to generate single word/expression completions associated with 18
situation cues-specific, measurable elements of environments that influence how
individuals perceive situations and compared the variability of these
completions using probability of differentiation. This approach directly
assessed homogeneity bias from the model's outputs, bypassing encoder models.
Across five studies, we find that homogeneity bias is highly volatile across
situation cues and writing prompts, suggesting that the bias observed in past
work may reflect those within encoder models rather than LLMs. Furthermore, we
find that homogeneity bias in LLMs is brittle, as even minor and arbitrary
changes in prompts can significantly alter the expression of biases. Future
work should further explore how variations in syntactic features and topic
choices in longer text generations influence homogeneity bias in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PediaBench: A Comprehensive Chinese Pediatric <span class="highlight-title">Dataset</span> for Benchmarking
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06287v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06287v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Zhang, Panfeng Chen, Jiali Li, Linkun Feng, Shuyu Liu, Heng Zhao, Mei Chen, Hui Li, Yanhao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of Large Language Models (LLMs) in the medical domain has
stressed a compelling need for standard datasets to evaluate their
question-answering (QA) performance. Although there have been several benchmark
datasets for medical QA, they either cover common knowledge across different
departments or are specific to another department rather than pediatrics.
Moreover, some of them are limited to objective questions and do not measure
the generation capacity of LLMs. Therefore, they cannot comprehensively assess
the QA ability of LLMs in pediatrics. To fill this gap, we construct
PediaBench, the first Chinese pediatric dataset for LLM evaluation.
Specifically, it contains 4,565 objective questions and 1,632 subjective
questions spanning 12 pediatric disease groups. It adopts an integrated scoring
criterion based on different difficulty levels to thoroughly assess the
proficiency of an LLM in instruction following, knowledge understanding,
clinical case analysis, etc. Finally, we validate the effectiveness of
PediaBench with extensive experiments on 20 open-source and commercial LLMs.
Through an in-depth analysis of experimental results, we offer insights into
the ability of LLMs to answer pediatric questions in the Chinese context,
highlighting their limitations for further improvements. Our code and data are
published at https://github.com/ACMISLab/PediaBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Generation to Judgment: Opportunities and Challenges of
  LLM-as-a-judge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16594v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16594v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, Kai Shu, Lu Cheng, Huan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessment and evaluation have long been critical challenges in artificial
intelligence (AI) and natural language processing (NLP). However, traditional
methods, whether matching-based or embedding-based, often fall short of judging
subtle attributes and delivering satisfactory results. Recent advancements in
Large Language Models (LLMs) inspire the "LLM-as-a-judge" paradigm, where LLMs
are leveraged to perform scoring, ranking, or selection across various tasks
and applications. This paper provides a comprehensive survey of LLM-based
judgment and assessment, offering an in-depth overview to advance this emerging
field. We begin by giving detailed definitions from both input and output
perspectives. Then we introduce a comprehensive taxonomy to explore
LLM-as-a-judge from three dimensions: what to judge, how to judge and where to
judge. Finally, we compile benchmarks for evaluating LLM-as-a-judge and
highlight key challenges and promising directions, aiming to provide valuable
insights and inspire future research in this promising research area. Paper
list and more resources about LLM-as-a-judge can be found at
\url{https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge} and
\url{https://llm-as-a-judge.github.io}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v3: add missing citations; 32 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Illusion3D: 3D Multiview Illusion with 2D Diffusion Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Feng, Vaibhav Sanjay, Spencer Lutz, Badour AlBahar, Songwei Ge, Jia-Bin Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatically generating multiview illusions is a compelling challenge, where
a single piece of visual content offers distinct interpretations from different
viewing perspectives. Traditional methods, such as shadow art and wire art,
create interesting 3D illusions but are limited to simple visual outputs (i.e.,
figure-ground or line drawing), restricting their artistic expressiveness and
practical versatility. Recent diffusion-based illusion generation methods can
generate more intricate designs but are confined to 2D images. In this work, we
present a simple yet effective approach for creating 3D multiview illusions
based on user-provided text prompts or images. Our method leverages a
pre-trained text-to-image diffusion model to optimize the textures and geometry
of neural 3D representations through differentiable rendering. When viewed from
multiple angles, this produces different interpretations. We develop several
techniques to improve the quality of the generated 3D multiview illusions. We
demonstrate the effectiveness of our approach through extensive experiments and
showcase illusion generation with diverse 3D forms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://3d-multiview-illusion.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FreeScale: Unleashing the Resolution of Diffusion Models via Tuning-Free
  Scale Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Qiu, Shiwei Zhang, Yujie Wei, Ruihang Chu, Hangjie Yuan, Xiang Wang, Yingya Zhang, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual diffusion models achieve remarkable progress, yet they are typically
trained at limited resolutions due to the lack of high-resolution data and
constrained computation resources, hampering their ability to generate
high-fidelity images or videos at higher resolutions. Recent efforts have
explored tuning-free strategies to exhibit the untapped potential
higher-resolution visual generation of pre-trained models. However, these
methods are still prone to producing low-quality visual content with repetitive
patterns. The key obstacle lies in the inevitable increase in high-frequency
information when the model generates visual content exceeding its training
resolution, leading to undesirable repetitive patterns deriving from the
accumulated errors. To tackle this challenge, we propose FreeScale, a
tuning-free inference paradigm to enable higher-resolution visual generation
via scale fusion. Specifically, FreeScale processes information from different
receptive scales and then fuses it by extracting desired frequency components.
Extensive experiments validate the superiority of our paradigm in extending the
capabilities of higher-resolution visual generation for both image and video
models. Notably, compared with the previous best-performing method, FreeScale
unlocks the generation of 8k-resolution images for the first time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: http://haonanqiu.com/projects/FreeScale.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Doe-1: Closed-Loop Autonomous Driving with Large World Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenzhao Zheng, Zetian Xia, Yuanhui Huang, Sicheng Zuo, Jie Zhou, Jiwen Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end autonomous driving has received increasing attention due to its
potential to learn from large amounts of data. However, most existing methods
are still open-loop and suffer from weak scalability, lack of high-order
interactions, and inefficient decision-making. In this paper, we explore a
closed-loop framework for autonomous driving and propose a large Driving wOrld
modEl (Doe-1) for unified perception, prediction, and planning. We formulate
autonomous driving as a next-token generation problem and use multi-modal
tokens to accomplish different tasks. Specifically, we use free-form texts
(i.e., scene descriptions) for perception and generate future predictions
directly in the RGB space with image tokens. For planning, we employ a
position-aware tokenizer to effectively encode action into discrete tokens. We
train a multi-modal transformer to autoregressively generate perception,
prediction, and planning tokens in an end-to-end and unified manner.
Experiments on the widely used nuScenes dataset demonstrate the effectiveness
of Doe-1 in various tasks including visual question-answering,
action-conditioned video generation, and motion planning. Code:
https://github.com/wzzheng/Doe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at: https://github.com/wzzheng/Doe</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GenEx: Generating an Explorable World 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taiming Lu, Tianmin Shu, Junfei Xiao, Luoxin Ye, Jiahao Wang, Cheng Peng, Chen Wei, Daniel Khashabi, Rama Chellappa, Alan Yuille, Jieneng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding, navigating, and exploring the 3D physical real world has long
been a central challenge in the development of artificial intelligence. In this
work, we take a step toward this goal by introducing GenEx, a system capable of
planning complex embodied world exploration, guided by its generative
imagination that forms priors (expectations) about the surrounding
environments. GenEx generates an entire 3D-consistent imaginative environment
from as little as a single RGB image, bringing it to life through panoramic
video streams. Leveraging scalable 3D world data curated from Unreal Engine,
our generative model is rounded in the physical world. It captures a continuous
360-degree environment with little effort, offering a boundless landscape for
AI agents to explore and interact with. GenEx achieves high-quality world
generation, robust loop consistency over long trajectories, and demonstrates
strong 3D capabilities such as consistency and active 3D mapping. Powered by
generative imagination of the world, GPT-assisted agents are equipped to
perform complex embodied tasks, including both goal-agnostic exploration and
goal-driven navigation. These agents utilize predictive expectation regarding
unseen parts of the physical world to refine their beliefs, simulate different
outcomes based on potential decisions, and make more informed choices. In
summary, we demonstrate that GenEx provides a transformative platform for
advancing embodied AI in imaginative spaces and brings potential for extending
these capabilities to real-world exploration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: GenEx.world</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniDrag: Enabling Motion Control for Omnidirectional Image-to-Video
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiqi Li, Shijie Zhao, Chong Mou, Xuhan Sheng, Zhenyu Zhang, Qian Wang, Junlin Li, Li Zhang, Jian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As virtual reality gains popularity, the demand for controllable creation of
immersive and dynamic omnidirectional videos (ODVs) is increasing. While
previous text-to-ODV generation methods achieve impressive results, they
struggle with content inaccuracies and inconsistencies due to reliance solely
on textual inputs. Although recent motion control techniques provide
fine-grained control for video generation, directly applying these methods to
ODVs often results in spatial distortion and unsatisfactory performance,
especially with complex spherical motions. To tackle these challenges, we
propose OmniDrag, the first approach enabling both scene- and object-level
motion control for accurate, high-quality omnidirectional image-to-video
generation. Building on pretrained video diffusion models, we introduce an
omnidirectional control module, which is jointly fine-tuned with temporal
attention layers to effectively handle complex spherical motion. In addition,
we develop a novel spherical motion estimator that accurately extracts
motion-control signals and allows users to perform drag-style ODV generation by
simply drawing handle and target points. We also present a new dataset, named
Move360, addressing the scarcity of ODV data with large scene and object
motions. Experiments demonstrate the significant superiority of OmniDrag in
achieving holistic scene-level and fine-grained object-level control for ODV
generation. The project page is available at
https://lwq20020127.github.io/OmniDrag.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoRACLR: Contrastive Adaptation for Customization of Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enis Simsar, Thomas Hofmann, Federico Tombari, Pinar Yanardag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-to-image customization have enabled high-fidelity,
context-rich generation of personalized images, allowing specific concepts to
appear in a variety of scenarios. However, current methods struggle with
combining multiple personalized models, often leading to attribute entanglement
or requiring separate training to preserve concept distinctiveness. We present
LoRACLR, a novel approach for multi-concept image generation that merges
multiple LoRA models, each fine-tuned for a distinct concept, into a single,
unified model without additional individual fine-tuning. LoRACLR uses a
contrastive objective to align and merge the weight spaces of these models,
ensuring compatibility while minimizing interference. By enforcing distinct yet
cohesive representations for each concept, LoRACLR enables efficient, scalable
model composition for high-quality, multi-concept image synthesis. Our results
highlight the effectiveness of LoRACLR in accurately merging multiple concepts,
advancing the capabilities of personalized image generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://loraclr.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Camera Movement Control from Real-World Drone Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09620v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09620v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunzhong Hou, Liang Zheng, Philip Torr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study seeks to automate camera movement control for filming existing
subjects into attractive videos, contrasting with the creation of non-existent
content by directly generating the pixels. We select drone videos as our test
case due to their rich and challenging motion patterns, distinctive viewing
angles, and precise controls. Existing AI videography methods struggle with
limited appearance diversity in simulation training, high costs of recording
expert operations, and difficulties in designing heuristic-based goals to cover
all scenarios. To avoid these issues, we propose a scalable method that
involves collecting real-world training data to improve diversity, extracting
camera trajectories automatically to minimize annotation costs, and training an
effective architecture that does not rely on heuristics. Specifically, we
collect 99k high-quality trajectories by running 3D reconstruction on online
videos, connecting camera poses from consecutive frames to formulate 3D camera
paths, and using Kalman filter to identify and remove low-quality data.
Moreover, we introduce DVGFormer, an auto-regressive transformer that leverages
the camera path and images from all past frames to predict camera movement in
the next frame. We evaluate our system across 38 synthetic natural scenes and 7
real city 3D scans. We show that our system effectively learns to perform
challenging camera movements such as navigating through obstacles, maintaining
low altitude to increase perceived speed, and orbiting towers and buildings,
which are very useful for recording high-quality videos. Data and code are
available at dvgformer.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linyi Jin, Richard Tucker, Zhengqi Li, David Fouhey, Noah Snavely, Aleksander Holynski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning to understand dynamic 3D scenes from imagery is crucial for
applications ranging from robotics to scene reconstruction. Yet, unlike other
problems where large-scale supervised training has enabled rapid progress,
directly supervising methods for recovering 3D motion remains challenging due
to the fundamental difficulty of obtaining ground truth annotations. We present
a system for mining high-quality 4D reconstructions from internet stereoscopic,
wide-angle videos. Our system fuses and filters the outputs of camera pose
estimation, stereo depth estimation, and temporal tracking methods into
high-quality dynamic 3D reconstructions. We use this method to generate
large-scale data in the form of world-consistent, pseudo-metric 3D point clouds
with long-term motion trajectories. We demonstrate the utility of this data by
training a variant of DUSt3R to predict structure and 3D motion from real-world
image pairs, showing that training on our reconstructed data enables
generalization to diverse real-world scenes. Project page:
https://stereo4d.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SnapGen: Taming High-Resolution Text-to-Image Models for Mobile Devices
  with Efficient Architectures and Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongting Hu, Jierun Chen, Xijie Huang, Huseyin Coskun, Arpit Sahni, Aarush Gupta, Anujraaj Goyal, Dishani Lahiri, Rajesh Singh, Yerlan Idelbayev, Junli Cao, Yanyu Li, Kwang-Ting Cheng, S. -H. Gary Chan, Mingming Gong, Sergey Tulyakov, Anil Kag, Yanwu Xu, Jian Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing text-to-image (T2I) diffusion models face several limitations,
including large model sizes, slow runtime, and low-quality generation on mobile
devices. This paper aims to address all of these challenges by developing an
extremely small and fast T2I model that generates high-resolution and
high-quality images on mobile platforms. We propose several techniques to
achieve this goal. First, we systematically examine the design choices of the
network architecture to reduce model parameters and latency, while ensuring
high-quality generation. Second, to further improve generation quality, we
employ cross-architecture knowledge distillation from a much larger model,
using a multi-level approach to guide the training of our model from scratch.
Third, we enable a few-step generation by integrating adversarial guidance with
knowledge distillation. For the first time, our model SnapGen, demonstrates the
generation of 1024x1024 px images on a mobile device around 1.4 seconds. On
ImageNet-1K, our model, with only 372M parameters, achieves an FID of 2.06 for
256x256 px generation. On T2I benchmarks (i.e., GenEval and DPG-Bench), our
model with merely 379M parameters, surpasses large-scale models with billions
of parameters at a significantly smaller size (e.g., 7x smaller than SDXL, 14x
smaller than IF-XL).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via
  Multimodal LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuofan Zong, Dongzhi Jiang, Bingqi Ma, Guanglu Song, Hao Shao, Dazhong Shen, Yu Liu, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Significant achievements in personalization of diffusion models have been
witnessed. Conventional tuning-free methods mostly encode multiple reference
images by averaging their image embeddings as the injection condition, but such
an image-independent operation cannot perform interaction among images to
capture consistent visual elements within multiple references. Although the
tuning-based Low-Rank Adaptation (LoRA) can effectively extract consistent
elements within multiple images through the training process, it necessitates
specific finetuning for each distinct image group. This paper introduces
EasyRef, a novel plug-and-play adaptation method that enables diffusion models
to be conditioned on multiple reference images and the text prompt. To
effectively exploit consistent visual elements within multiple images, we
leverage the multi-image comprehension and instruction-following capabilities
of the multimodal large language model (MLLM), prompting it to capture
consistent visual elements based on the instruction. Besides, injecting the
MLLM's representations into the diffusion process through adapters can easily
generalize to unseen domains, mining the consistent visual elements within
unseen data. To mitigate computational costs and enhance fine-grained detail
preservation, we introduce an efficient reference aggregation strategy and a
progressive training scheme. Finally, we introduce MRBench, a new
multi-reference image generation benchmark. Experimental results demonstrate
EasyRef surpasses both tuning-free methods like IP-Adapter and tuning-based
methods like LoRA, achieving superior aesthetic quality and robust zero-shot
generalization across diverse domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ V2PE: Improving Multimodal Long-Context Capability of Vision-Language
  Models with Variable Visual Position Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junqi Ge, Ziyi Chen, Jintao Lin, Jinguo Zhu, Xihui Liu, Jifeng Dai, Xizhou Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) have shown promising capabilities in handling
various multimodal tasks, yet they struggle in long-context scenarios,
particularly in tasks involving videos, high-resolution images, or lengthy
image-text documents. In our work, we first conduct an empirical analysis of
the long-context capabilities of VLMs using our augmented long-context
multimodal datasets. Our findings reveal that directly applying the positional
encoding mechanism used for textual tokens to visual tokens is suboptimal, and
VLM performance degrades sharply when the position encoding exceeds the model's
context window. To address this, we propose Variable Visual Position Encoding
(V2PE), a novel positional encoding approach that employs variable and smaller
increments for visual tokens, enabling more efficient management of long
multimodal sequences. Our experiments demonstrate the effectiveness of V2PE to
enhances VLMs' ability to effectively understand and reason over long
multimodal contexts. We further integrate V2PE with our augmented long-context
multimodal datasets to fine-tune the open-source VLM, InternVL2. The fine-tuned
model achieves strong performance on both standard and long-context multimodal
tasks. Notably, when the sequence length of the training dataset is increased
to 256K tokens, the model is capable of processing multimodal sequences up to
1M tokens, highlighting its potential for real-world long-context applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code and models will be available at
  https://github.com/OpenGVLab/V2PE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context Canvas: Enhancing Text-to-Image Diffusion Models with Knowledge
  Graph-Based RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kavana Venkatesh, Yusuf Dalva, Ismini Lourentzou, Pinar Yanardag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel approach to enhance the capabilities of text-to-image
models by incorporating a graph-based RAG. Our system dynamically retrieves
detailed character information and relational data from the knowledge graph,
enabling the generation of visually accurate and contextually rich images. This
capability significantly improves upon the limitations of existing T2I models,
which often struggle with the accurate depiction of complex or culturally
specific subjects due to dataset constraints. Furthermore, we propose a novel
self-correcting mechanism for text-to-image models to ensure consistency and
fidelity in visual outputs, leveraging the rich context from the graph to guide
corrections. Our qualitative and quantitative experiments demonstrate that
Context Canvas significantly enhances the capabilities of popular models such
as Flux, Stable Diffusion, and DALL-E, and improves the functionality of
ControlNet for fine-grained image editing tasks. To our knowledge, Context
Canvas represents the first application of graph-based RAG in enhancing T2I
models, representing a significant advancement for producing high-fidelity,
context-aware multi-faceted images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://context-canvas.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FluxSpace: Disentangled Semantic Editing in Rectified Flow <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuf Dalva, Kavana Venkatesh, Pinar Yanardag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rectified flow models have emerged as a dominant approach in image
generation, showcasing impressive capabilities in high-quality image synthesis.
However, despite their effectiveness in visual generation, rectified flow
models often struggle with disentangled editing of images. This limitation
prevents the ability to perform precise, attribute-specific modifications
without affecting unrelated aspects of the image. In this paper, we introduce
FluxSpace, a domain-agnostic image editing method leveraging a representation
space with the ability to control the semantics of images generated by
rectified flow transformers, such as Flux. By leveraging the representations
learned by the transformer blocks within the rectified flow models, we propose
a set of semantically interpretable representations that enable a wide range of
image editing tasks, from fine-grained image editing to artistic creation. This
work offers a scalable and effective image editing approach, along with its
disentanglement capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://fluxspace.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Olympus: A Universal Task Router for Computer Vision Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanze Lin, Yunsheng Li, Dongdong Chen, Weijian Xu, Ronald Clark, Philip H. S. Torr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Olympus, a new approach that transforms Multimodal Large
Language Models (MLLMs) into a unified framework capable of handling a wide
array of computer vision tasks. Utilizing a controller MLLM, Olympus delegates
over 20 specialized tasks across images, videos, and 3D objects to dedicated
modules. This instruction-based routing enables complex workflows through
chained actions without the need for training heavy generative models. Olympus
easily integrates with existing MLLMs, expanding their capabilities with
comparable performance. Experimental results demonstrate that Olympus achieves
an average routing accuracy of 94.75% across 20 tasks and precision of 91.82%
in chained action scenarios, showcasing its effectiveness as a universal task
router that can solve a diverse range of computer vision tasks. Project page:
https://github.com/yuanze-lin/Olympus_page
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PVC: Progressive Visual Token Compression for Unified Image and Video
  Processing in Large Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu Yang, Xuan Dong, Xizhou Zhu, Weijie Su, Jiahao Wang, Hao Tian, Zhe Chen, Wenhai Wang, Lewei Lu, Jifeng Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (VLMs) have been extended to understand both
images and videos. Visual token compression is leveraged to reduce the
considerable token length of visual inputs. To meet the needs of different
tasks, existing high-performance models usually process images and videos
separately with different token compression strategies, limiting the
capabilities of combining images and videos. To this end, we extend each image
into a "static" video and introduce a unified token compression strategy called
Progressive Visual Token Compression (PVC), where the tokens of each frame are
progressively encoded and adaptively compressed to supplement the information
not extracted from previous frames. Video tokens are efficiently compressed
with exploiting the inherent temporal redundancy. Images are repeated as static
videos, and the spatial details can be gradually supplemented in multiple
frames. PVC unifies the token compressing of images and videos. With a limited
number of tokens per frame (64 tokens by default), spatial details and temporal
changes can still be preserved. Experiments show that our model achieves
state-of-the-art performance across various video understanding benchmarks,
including long video tasks and fine-grained short video tasks. Meanwhile, our
unified token compression strategy incurs no performance loss on image
benchmarks, particularly in detail-sensitive tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representing Long Volumetric Video with Temporal Gaussian Hierarchy <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Xu, Yinghao Xu, Zhiyuan Yu, Sida Peng, Jiaming Sun, Hujun Bao, Xiaowei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper aims to address the challenge of reconstructing long volumetric
videos from multi-view RGB videos. Recent dynamic view synthesis methods
leverage powerful 4D representations, like feature grids or point cloud
sequences, to achieve high-quality rendering results. However, they are
typically limited to short (1~2s) video clips and often suffer from large
memory footprints when dealing with longer videos. To solve this issue, we
propose a novel 4D representation, named Temporal Gaussian Hierarchy, to
compactly model long volumetric videos. Our key observation is that there are
generally various degrees of temporal redundancy in dynamic scenes, which
consist of areas changing at different speeds. Motivated by this, our approach
builds a multi-level hierarchy of 4D Gaussian primitives, where each level
separately describes scene regions with different degrees of content change,
and adaptively shares Gaussian primitives to represent unchanged scene content
over different temporal segments, thus effectively reducing the number of
Gaussian primitives. In addition, the tree-like structure of the Gaussian
hierarchy allows us to efficiently represent the scene at a particular moment
with a subset of Gaussian primitives, leading to nearly constant GPU memory
usage during the training or rendering regardless of the video length.
Extensive experimental results demonstrate the superiority of our method over
alternative methods in terms of training cost, rendering speed, and storage
usage. To our knowledge, this work is the first approach capable of efficiently
handling minutes of volumetric video data while maintaining state-of-the-art
rendering quality. Our project page is available at:
https://zju3dv.github.io/longvolcap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGGRAPH Asia 2024 (TOG). Project page:
  https://zju3dv.github.io/longvolcap</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spectral Image Tokenizer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Esteves, Mohammed Suhail, Ameesh Makadia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image tokenizers map images to sequences of discrete tokens, and are a
crucial component of autoregressive transformer-based image generation. The
tokens are typically associated with spatial locations in the input image,
arranged in raster scan order, which is not ideal for autoregressive modeling.
In this paper, we propose to tokenize the image spectrum instead, obtained from
a discrete wavelet transform (DWT), such that the sequence of tokens represents
the image in a coarse-to-fine fashion. Our tokenizer brings several advantages:
1) it leverages that natural images are more compressible at high frequencies,
2) it can take and reconstruct images of different resolutions without
retraining, 3) it improves the conditioning for next-token prediction --
instead of conditioning on a partial line-by-line reconstruction of the image,
it takes a coarse reconstruction of the full image, 4) it enables partial
decoding where the first few generated tokens can reconstruct a coarse version
of the image, 5) it enables autoregressive models to be used for image
upsampling. We evaluate the tokenizer reconstruction metrics as well as
multiscale image generation, text-guided image upsampling and editing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feat2GS: Probing Visual Foundation Models with Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Chen, Xingyu Chen, Anpei Chen, Gerard Pons-Moll, Yuliang Xiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given that visual foundation models (VFMs) are trained on extensive datasets
but often limited to 2D images, a natural question arises: how well do they
understand the 3D world? With the differences in architecture and training
protocols (i.e., objectives, proxy tasks), a unified framework to fairly and
comprehensively probe their 3D awareness is urgently needed. Existing works on
3D probing suggest single-view 2.5D estimation (e.g., depth and normal) or
two-view sparse 2D correspondence (e.g., matching and tracking). Unfortunately,
these tasks ignore texture awareness, and require 3D data as ground-truth,
which limits the scale and diversity of their evaluation set. To address these
issues, we introduce Feat2GS, which readout 3D Gaussians attributes from VFM
features extracted from unposed images. This allows us to probe 3D awareness
for geometry and texture via novel view synthesis, without requiring 3D data.
Additionally, the disentanglement of 3DGS parameters - geometry
($\boldsymbol{x}, \alpha, \Sigma$) and texture ($\boldsymbol{c}$) - enables
separate analysis of texture and geometry awareness. Under Feat2GS, we conduct
extensive experiments to probe the 3D awareness of several VFMs, and
investigate the ingredients that lead to a 3D aware VFM. Building on these
findings, we develop several variants that achieve state-of-the-art across
diverse datasets. This makes Feat2GS useful for probing VFMs, and as a
simple-yet-effective baseline for novel-view synthesis. Code and data will be
made available at https://fanegg.github.io/Feat2GS/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://fanegg.github.io/Feat2GS/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SynerGen-VL: Towards Synergistic Image Understanding and Generation with
  Vision Experts and Token Folding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Li, Changyao Tian, Jie Shao, Xizhou Zhu, Zhaokai Wang, Jinguo Zhu, Wenhan Dou, Xiaogang Wang, Hongsheng Li, Lewei Lu, Jifeng Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable success of Large Language Models (LLMs) has extended to the
multimodal domain, achieving outstanding performance in image understanding and
generation. Recent efforts to develop unified Multimodal Large Language Models
(MLLMs) that integrate these capabilities have shown promising results.
However, existing approaches often involve complex designs in model
architecture or training pipeline, increasing the difficulty of model training
and scaling. In this paper, we propose SynerGen-VL, a simple yet powerful
encoder-free MLLM capable of both image understanding and generation. To
address challenges identified in existing encoder-free unified MLLMs, we
introduce the token folding mechanism and the vision-expert-based progressive
alignment pretraining strategy, which effectively support high-resolution image
understanding while reducing training complexity. After being trained on
large-scale mixed image-text data with a unified next-token prediction
objective, SynerGen-VL achieves or surpasses the performance of existing
encoder-free unified MLLMs with comparable or smaller parameter sizes, and
narrows the gap with task-specific state-of-the-art models, highlighting a
promising path toward future unified MLLMs. Our code and models shall be
released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Multimodal Large Language Models See Like Humans? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaying Lin, Shuquan Ye, Rynson W. H. Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have achieved impressive results on
various vision tasks, leveraging recent advancements in large language models.
However, a critical question remains unaddressed: do MLLMs perceive visual
information similarly to humans? Current benchmarks lack the ability to
evaluate MLLMs from this perspective. To address this challenge, we introduce
HVSBench, a large-scale benchmark designed to assess the alignment between
MLLMs and the human visual system (HVS) on fundamental vision tasks that mirror
human vision. HVSBench curated over 85K multimodal samples, spanning 13
categories and 5 fields in HVS, including Prominence, Subitizing, Prioritizing,
Free-Viewing, and Searching. Extensive experiments demonstrate the
effectiveness of our benchmark in providing a comprehensive evaluation of
MLLMs. Specifically, we evaluate 13 MLLMs, revealing that even the best models
show significant room for improvement, with most achieving only moderate
results. Our experiments reveal that HVSBench presents a new and significant
challenge for cutting-edge MLLMs. We believe that HVSBench will facilitate
research on human-aligned and explainable MLLMs, marking a key step in
understanding how MLLMs perceive and process visual information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://jiaying.link/HVSBench/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hidden Biases of End-to-End Driving <span class="highlight-title">Dataset</span>s <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Zimmerlin, Jens Beißwenger, Bernhard Jaeger, Andreas Geiger, Kashyap Chitta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end driving systems have made rapid progress, but have so far not been
applied to the challenging new CARLA Leaderboard 2.0. Further, while there is a
large body of literature on end-to-end architectures and training strategies,
the impact of the training dataset is often overlooked. In this work, we make a
first attempt at end-to-end driving for Leaderboard 2.0. Instead of
investigating architectures, we systematically analyze the training dataset,
leading to new insights: (1) Expert style significantly affects downstream
policy performance. (2) In complex data sets, the frames should not be weighted
on the basis of simplistic criteria such as class frequencies. (3) Instead,
estimating whether a frame changes the target labels compared to previous
frames can reduce the size of the dataset without removing important
information. By incorporating these findings, our model ranks first and second
respectively on the map and sensors tracks of the 2024 CARLA Challenge, and
sets a new state-of-the-art on the Bench2Drive test routes. Finally, we uncover
a design flaw in the current evaluation metrics and propose a modification for
future challenges. Our dataset, code, and pre-trained models are publicly
available at https://github.com/autonomousvision/carla_garage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report for the CVPR 2024 Workshop on Foundation Models for
  Autonomous Systems. Runner-up of the track 'CARLA Autonomous Driving
  Challenge' in the 2024 Autonomous Grand Challenge
  (https://opendrivelab.com/challenge2024/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TimeRefine: Temporal Grounding with Time Refining Video LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xizi Wang, Feng Cheng, Ziyang Wang, Huiyu Wang, Md Mohaiminul Islam, Lorenzo Torresani, Mohit Bansal, Gedas Bertasius, David Crandall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video temporal grounding aims to localize relevant temporal boundaries in a
video given a textual prompt. Recent work has focused on enabling Video LLMs to
perform video temporal grounding via next-token prediction of temporal
timestamps. However, accurately localizing timestamps in videos remains
challenging for Video LLMs when relying solely on temporal token prediction.
Our proposed TimeRefine addresses this challenge in two ways. First, instead of
directly predicting the start and end timestamps, we reformulate the temporal
grounding task as a temporal refining task: the model first makes rough
predictions and then refines them by predicting offsets to the target segment.
This refining process is repeated multiple times, through which the model
progressively self-improves its temporal localization accuracy. Second, to
enhance the model's temporal perception capabilities, we incorporate an
auxiliary prediction head that penalizes the model more if a predicted segment
deviates further from the ground truth, thus encouraging the model to make
closer and more accurate predictions. Our plug-and-play method can be
integrated into most LLM-based temporal grounding approaches. The experimental
results demonstrate that TimeRefine achieves 3.6% and 5.0% mIoU improvements on
the ActivityNet and Charades-STA datasets, respectively. Code and pretrained
models will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Owl-1: Omni World Model for Consistent Long Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanhui Huang, Wenzhao Zheng, Yuan Gao, Xin Tao, Pengfei Wan, Di Zhang, Jie Zhou, Jiwen Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video generation models (VGMs) have received extensive attention recently and
serve as promising candidates for general-purpose large vision models. While
they can only generate short videos each time, existing methods achieve long
video generation by iteratively calling the VGMs, using the last-frame output
as the condition for the next-round generation. However, the last frame only
contains short-term fine-grained information about the scene, resulting in
inconsistency in the long horizon. To address this, we propose an Omni World
modeL (Owl-1) to produce long-term coherent and comprehensive conditions for
consistent long video generation. As videos are observations of the underlying
evolving world, we propose to model the long-term developments in a latent
space and use VGMs to film them into videos. Specifically, we represent the
world with a latent state variable which can be decoded into explicit video
observations. These observations serve as a basis for anticipating temporal
dynamics which in turn update the state variable. The interaction between
evolving dynamics and persistent state enhances the diversity and consistency
of the long videos. Extensive experiments show that Owl-1 achieves comparable
performance with SOTA methods on VBench-I2V and VBench-Long, validating its
ability to generate high-quality video observations. Code:
https://github.com/huang-yh/Owl.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at: https://github.com/huang-yh/Owl</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RatBodyFormer: Rodent Body Surface from Keypoints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayaka Higami, Karin Oshima, Tomoyo Isoguchi Shiramatsu, Hirokazu Takahashi, Shohei Nobuhara, Ko Nishino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rat behavior modeling goes to the heart of many scientific studies, yet the
textureless body surface evades automatic analysis as it literally has no
keypoints that detectors can find. The movement of the body surface, however,
is a rich source of information for deciphering the rat behavior. We introduce
two key contributions to automatically recover densely 3D sampled rat body
surface points, passively. The first is RatDome, a novel multi-camera system
for rat behavior capture, and a large-scale dataset captured with it that
consists of pairs of 3D keypoints and 3D body surface points. The second is
RatBodyFormer, a novel network to transform detected keypoints to 3D body
surface points. RatBodyFormer is agnostic to the exact locations of the 3D body
surface points in the training data and is trained with masked-learning. We
experimentally validate our framework with a number of real-world experiments.
Our results collectively serve as a novel foundation for automated rat behavior
analysis and will likely have far-reaching implications for biomedical and
neuroscientific research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LiftImage3D: Lifting Any Single Image to 3D Gaussians with Video
  Generation Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yabo Chen, Chen Yang, Jiemin Fang, Xiaopeng Zhang, Lingxi Xie, Wei Shen, Wenrui Dai, Hongkai Xiong, Qi Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Single-image 3D reconstruction remains a fundamental challenge in computer
vision due to inherent geometric ambiguities and limited viewpoint information.
Recent advances in Latent Video Diffusion Models (LVDMs) offer promising 3D
priors learned from large-scale video data. However, leveraging these priors
effectively faces three key challenges: (1) degradation in quality across large
camera motions, (2) difficulties in achieving precise camera control, and (3)
geometric distortions inherent to the diffusion process that damage 3D
consistency. We address these challenges by proposing LiftImage3D, a framework
that effectively releases LVDMs' generative priors while ensuring 3D
consistency. Specifically, we design an articulated trajectory strategy to
generate video frames, which decomposes video sequences with large camera
motions into ones with controllable small motions. Then we use robust neural
matching models, i.e. MASt3R, to calibrate the camera poses of generated frames
and produce corresponding point clouds. Finally, we propose a distortion-aware
3D Gaussian splatting representation, which can learn independent distortions
between frames and output undistorted canonical Gaussians. Extensive
experiments demonstrate that LiftImage3D achieves state-of-the-art performance
on two challenging datasets, i.e. LLFF, DL3DV, and Tanks and Temples, and
generalizes well to diverse in-the-wild images, from cartoon illustrations to
complex real-world scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://liftimage3d.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for
  Long-term Streaming Video and Audio Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pan Zhang, Xiaoyi Dong, Yuhang Cao, Yuhang Zang, Rui Qian, Xilin Wei, Lin Chen, Yifei Li, Junbo Niu, Shuangrui Ding, Qipeng Guo, Haodong Duan, Xin Chen, Han Lv, Zheng Nie, Min Zhang, Bin Wang, Wenwei Zhang, Xinyue Zhang, Jiaye Ge, Wei Li, Jingwen Li, Zhongying Tu, Conghui He, Xingcheng Zhang, Kai Chen, Yu Qiao, Dahua Lin, Jiaqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating AI systems that can interact with environments over long periods,
similar to human cognition, has been a longstanding research goal. Recent
advancements in multimodal large language models (MLLMs) have made significant
strides in open-world understanding. However, the challenge of continuous and
simultaneous streaming perception, memory, and reasoning remains largely
unexplored. Current MLLMs are constrained by their sequence-to-sequence
architecture, which limits their ability to process inputs and generate
responses simultaneously, akin to being unable to think while perceiving.
Furthermore, relying on long contexts to store historical data is impractical
for long-term interactions, as retaining all information becomes costly and
inefficient. Therefore, rather than relying on a single foundation model to
perform all functions, this project draws inspiration from the concept of the
Specialized Generalist AI and introduces disentangled streaming perception,
reasoning, and memory mechanisms, enabling real-time interaction with streaming
video and audio input. The proposed framework InternLM-XComposer2.5-OmniLive
(IXC2.5-OL) consists of three key modules: (1) Streaming Perception Module:
Processes multimodal information in real-time, storing key details in memory
and triggering reasoning in response to user queries. (2) Multi-modal Long
Memory Module: Integrates short-term and long-term memory, compressing
short-term memories into long-term ones for efficient retrieval and improved
accuracy. (3) Reasoning Module: Responds to queries and executes reasoning
tasks, coordinating with the perception and memory modules. This project
simulates human-like cognition, enabling multimodal large language models to
provide continuous and adaptive service over time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Github Repo:
  https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-OmniLive</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural LightRig: Unlocking Accurate Object Normal and Material
  Estimation with Multi-Light Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zexin He, Tengfei Wang, Xin Huang, Xingang Pan, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recovering the geometry and materials of objects from a single image is
challenging due to its under-constrained nature. In this paper, we present
Neural LightRig, a novel framework that boosts intrinsic estimation by
leveraging auxiliary multi-lighting conditions from 2D diffusion priors.
Specifically, 1) we first leverage illumination priors from large-scale
diffusion models to build our multi-light diffusion model on a synthetic
relighting dataset with dedicated designs. This diffusion model generates
multiple consistent images, each illuminated by point light sources in
different directions. 2) By using these varied lighting images to reduce
estimation uncertainty, we train a large G-buffer model with a U-Net backbone
to accurately predict surface normals and materials. Extensive experiments
validate that our approach significantly outperforms state-of-the-art methods,
enabling accurate surface normal and PBR material estimation with vivid
relighting effects. Code and dataset are available on our project page at
https://projects.zxhezexin.com/neural-lightrig.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://projects.zxhezexin.com/neural-lightrig</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fiona Ryan, Ajay Bati, Sangmin Lee, Daniel Bolya, Judy Hoffman, James M. Rehg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of gaze target estimation, which aims to predict where
a person is looking in a scene. Predicting a person's gaze target requires
reasoning both about the person's appearance and the contents of the scene.
Prior works have developed increasingly complex, hand-crafted pipelines for
gaze target estimation that carefully fuse features from separate scene
encoders, head encoders, and auxiliary models for signals like depth and pose.
Motivated by the success of general-purpose feature extractors on a variety of
visual tasks, we propose Gaze-LLE, a novel transformer framework that
streamlines gaze target estimation by leveraging features from a frozen DINOv2
encoder. We extract a single feature representation for the scene, and apply a
person-specific positional prompt to decode gaze with a lightweight module. We
demonstrate state-of-the-art performance across several gaze benchmarks and
provide extensive analysis to validate our design choices. Our code is
available at: http://github.com/fkryan/gazelle .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OLA-VLM: Elevating Visual Perception in Multimodal LLMs with Auxiliary
  Embedding Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jitesh Jain, Zhengyuan Yang, Humphrey Shi, Jianfeng Gao, Jianwei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The standard practice for developing contemporary MLLMs is to feed features
from vision encoder(s) into the LLM and train with natural language
supervision. In this work, we posit an overlooked opportunity to optimize the
intermediate LLM representations through a vision perspective (objective),
i.e., solely natural language supervision is sub-optimal for the MLLM's visual
understanding ability. To that end, we propose OLA-VLM, the first approach
distilling knowledge into the LLM's hidden representations from a set of target
visual representations. Firstly, we formulate the objective during the
pretraining stage in MLLMs as a coupled optimization of predictive visual
embedding and next text-token prediction. Secondly, we investigate MLLMs
trained solely with natural language supervision and identify a positive
correlation between the quality of visual representations within these models
and their downstream performance. Moreover, upon probing our OLA-VLM, we
observe improved representation quality owing to the embedding optimization.
Thirdly, we demonstrate that our OLA-VLM outperforms the single and
multi-encoder baselines, proving our approach's superiority over explicitly
feeding the corresponding features to the LLM. Particularly, OLA-VLM boosts
performance by an average margin of up to 2.5% on various benchmarks, with a
notable improvement of 8.7% on the Depth task in CV-Bench. Our code is
open-sourced at https://github.com/SHI-Labs/OLA-VLM .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://praeclarumjj3.github.io/ola_vlm/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neptune: The Long Orbit to Benchmarking Long Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arsha Nagrani, Mingda Zhang, Ramin Mehran, Rachel Hornung, Nitesh Bharadwaj Gundavarapu, Nilpa Jha, Austin Myers, Xingyi Zhou, Boqing Gong, Cordelia Schmid, Mikhail Sirotenko, Yukun Zhu, Tobias Weyand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes a semi-automatic pipeline to generate challenging
question-answer-decoy sets for understanding long videos. Many existing video
datasets and models are focused on short clips (10s-30s). While some long video
datasets do exist, they can often be solved by powerful image models applied
per frame (and often to very few frames) in a video, and are usually manually
annotated at high cost. In order to mitigate both these problems, we propose a
scalable dataset creation pipeline which leverages large models (VLMs and
LLMs), to automatically generate dense, time-aligned video captions, as well as
tough question answer decoy sets for video segments (up to 15 minutes in
length). Our dataset Neptune covers a broad range of long video reasoning
abilities and consists of a subset that emphasizes multimodal reasoning. Since
existing metrics for open-ended question answering are either rule-based or may
rely on proprietary models, we provide a new open source model-based metric GEM
to score open-ended responses on Neptune. Benchmark evaluations reveal that
most current open-source long video models perform poorly on Neptune,
particularly on questions testing temporal ordering, counting and state
changes. Through Neptune, we aim to spur the development of more advanced
models capable of understanding long videos. The dataset is available at
https://github.com/google-deepmind/neptune
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FreeSplatter: Pose-free Gaussian Splatting for Sparse-view 3D
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiale Xu, Shenghua Gao, Ying Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing sparse-view reconstruction models heavily rely on accurate known
camera poses. However, deriving camera extrinsics and intrinsics from
sparse-view images presents significant challenges. In this work, we present
FreeSplatter, a highly scalable, feed-forward reconstruction framework capable
of generating high-quality 3D Gaussians from uncalibrated sparse-view images
and recovering their camera parameters in mere seconds. FreeSplatter is built
upon a streamlined transformer architecture, comprising sequential
self-attention blocks that facilitate information exchange among multi-view
image tokens and decode them into pixel-wise 3D Gaussian primitives. The
predicted Gaussian primitives are situated in a unified reference frame,
allowing for high-fidelity 3D modeling and instant camera parameter estimation
using off-the-shelf solvers. To cater to both object-centric and scene-level
reconstruction, we train two model variants of FreeSplatter on extensive
datasets. In both scenarios, FreeSplatter outperforms state-of-the-art
baselines in terms of reconstruction quality and pose estimation accuracy.
Furthermore, we showcase FreeSplatter's potential in enhancing the productivity
of downstream applications, such as text/image-to-3D content creation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://bluestyle97.github.io/projects/freesplatter/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video Creation by Demonstration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Sun, Hao Zhou, Liangzhe Yuan, Jennifer J. Sun, Yandong Li, Xuhui Jia, Hartwig Adam, Bharath Hariharan, Long Zhao, Ting Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore a novel video creation experience, namely Video Creation by
Demonstration. Given a demonstration video and a context image from a different
scene, we generate a physically plausible video that continues naturally from
the context image and carries out the action concepts from the demonstration.
To enable this capability, we present $\delta$-Diffusion, a self-supervised
training approach that learns from unlabeled videos by conditional future frame
prediction. Unlike most existing video generation controls that are based on
explicit signals, we adopts the form of implicit latent control for maximal
flexibility and expressiveness required by general videos. By leveraging a
video foundation model with an appearance bottleneck design on top, we extract
action latents from demonstration videos for conditioning the generation
process with minimal appearance leakage. Empirically, $\delta$-Diffusion
outperforms related baselines in terms of both human preference and large-scale
machine evaluations, and demonstrates potentials towards interactive world
simulation. Sampled video generation results are available at
https://delta-diffusion.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page at https://delta-diffusion.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exemplar Masking for Multimodal Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Lun Lee, Chen-Yu Lee, Wei-Chen Chiu, Yi-Hsuan Tsai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal incremental learning needs to digest the information from multiple
modalities while concurrently learning new knowledge without forgetting the
previously learned information. There are numerous challenges for this task,
mainly including the larger storage size of multimodal data in exemplar-based
methods and the computational requirement of finetuning on huge multimodal
models. In this paper, we leverage the parameter-efficient tuning scheme to
reduce the burden of fine-tuning and propose the exemplar masking framework to
efficiently replay old knowledge. Specifically, the non-important tokens are
masked based on the attention weights and the correlation across different
modalities, significantly reducing the storage size of an exemplar and
consequently saving more exemplars under the same memory buffer. Moreover, we
design a multimodal data augmentation technique to diversify exemplars for
replaying prior knowledge. In experiments, we not only evaluate our method in
existing multimodal datasets but also extend the ImageNet-R dataset to a
multimodal dataset as a real-world application, where captions are generated by
querying multimodal large language models (e.g., InstructBLIP). Extensive
experiments show that our exemplar masking framework is more efficient and
robust to catastrophic forgetting under the same limited memory buffer. Code is
available at https://github.com/YiLunLee/Exemplar_Masking_MCIL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/YiLunLee/Exemplar_Masking_MCIL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meshtron: High-Fidelity, Artist-Like 3D Mesh Generation at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekun Hao, David W. Romero, Tsung-Yi Lin, Ming-Yu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meshes are fundamental representations of 3D surfaces. However, creating
high-quality meshes is a labor-intensive task that requires significant time
and expertise in 3D modeling. While a delicate object often requires over
$10^4$ faces to be accurately modeled, recent attempts at generating
artist-like meshes are limited to $1.6$K faces and heavy discretization of
vertex coordinates. Hence, scaling both the maximum face count and vertex
coordinate resolution is crucial to producing high-quality meshes of realistic,
complex 3D objects. We present Meshtron, a novel autoregressive mesh generation
model able to generate meshes with up to 64K faces at 1024-level coordinate
resolution --over an order of magnitude higher face count and $8{\times}$
higher coordinate resolution than current state-of-the-art methods. Meshtron's
scalability is driven by four key components: (1) an hourglass neural
architecture, (2) truncated sequence training, (3) sliding window inference,
(4) a robust sampling strategy that enforces the order of mesh sequences. This
results in over $50{\%}$ less training memory, $2.5{\times}$ faster throughput,
and better consistency than existing works. Meshtron generates meshes of
detailed, complex 3D objects at unprecedented levels of resolution and
fidelity, closely resembling those created by professional artists, and opening
the door to more realistic generation of detailed 3D assets for animation,
gaming, and virtual environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://research.nvidia.com/labs/dir/meshtron/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SimAvatar: Simulation-Ready Avatars with Layered Hair and Clothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueting Li, Ye Yuan, Shalini De Mello, Gilles Daviet, Jonathan Leaf, Miles Macklin, Jan Kautz, Umar Iqbal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SimAvatar, a framework designed to generate simulation-ready
clothed 3D human avatars from a text prompt. Current text-driven human avatar
generation methods either model hair, clothing, and the human body using a
unified geometry or produce hair and garments that are not easily adaptable for
simulation within existing simulation pipelines. The primary challenge lies in
representing the hair and garment geometry in a way that allows leveraging
established prior knowledge from foundational image diffusion models (e.g.,
Stable Diffusion) while being simulation-ready using either physics or neural
simulators. To address this task, we propose a two-stage framework that
combines the flexibility of 3D Gaussians with simulation-ready hair strands and
garment meshes. Specifically, we first employ three text-conditioned 3D
generative models to generate garment mesh, body shape and hair strands from
the given text prompt. To leverage prior knowledge from foundational diffusion
models, we attach 3D Gaussians to the body mesh, garment mesh, as well as hair
strands and learn the avatar appearance through optimization. To drive the
avatar given a pose sequence, we first apply physics simulators onto the
garment meshes and hair strands. We then transfer the motion onto 3D Gaussians
through carefully designed mechanisms for each body part. As a result, our
synthesized avatars have vivid texture and realistic dynamic motion. To the
best of our knowledge, our method is the first to produce highly realistic,
fully simulation-ready 3D avatars, surpassing the capabilities of current
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://nvlabs.github.io/SimAvatar/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic-VLM: Simple Dynamic Visual Token Compression for VideoLLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Wang, Yuxiang Nie, Yongjie Ye, Deng GuanYu, Yanjie Wang, Shuai Li, Haiyang Yu, Jinghui Lu, Can Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of Large Vision-Language Models (LVLMs) for analyzing images
and videos is an exciting and rapidly evolving field. In recent years, we've
seen significant growth in high-quality image-text datasets for fine-tuning
image understanding, but there is still a lack of comparable datasets for
videos. Additionally, many VideoLLMs are extensions of single-image VLMs, which
may not efficiently handle the complexities of longer videos. In this study, we
introduce a large-scale synthetic dataset created from proprietary models,
using carefully designed prompts to tackle a wide range of questions. We also
explore a dynamic visual token compression architecture that strikes a balance
between computational efficiency and performance. Our proposed \model{}
achieves state-of-the-art results across various video tasks and shows
impressive generalization, setting new baselines in multi-image understanding.
Notably, \model{} delivers an absolute improvement of 2.7\% over
LLaVA-OneVision on VideoMME and 10.7\% on MuirBench. Codes are available at
https://github.com/Hon-Wong/ByteVideoLLM
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Modern LLMs Act as Agent Cores in Radiology~Environments? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09529v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09529v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiaoyu Zheng, Chaoyi Wu, Pengcheng Qiu, Lisong Dai, Ya Zhang, Yanfeng Wang, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in large language models (LLMs) have paved the way for LLM-based
agent systems that offer enhanced accuracy and interpretability across various
domains. Radiology, with its complex analytical requirements, is an ideal field
for the application of these agents. This paper aims to investigate the
pre-requisite question for building concrete radiology agents which is, `Can
modern LLMs act as agent cores in radiology environments?' To investigate it,
we introduce RadABench with three-fold contributions: First, we present
RadABench-Data, a comprehensive synthetic evaluation dataset for LLM-based
agents, generated from an extensive taxonomy encompassing 6 anatomies, 5
imaging modalities, 10 tool categories, and 11 radiology tasks. Second, we
propose RadABench-EvalPlat, a novel evaluation platform for agents featuring a
prompt-driven workflow and the capability to simulate a wide range of radiology
toolsets. Third, we assess the performance of 7 leading LLMs on our benchmark
from 5 perspectives with multiple metrics. Our findings indicate that while
current LLMs demonstrate strong capabilities in many areas, they are still not
sufficiently advanced to serve as the central agent core in a fully operational
radiology agent system. Additionally, we identify key factors influencing the
performance of LLM-based agent cores, offering insights for clinicians on how
to apply agent systems in real-world radiology practices effectively. All of
our code and data are open-sourced in
https://github.com/MAGIC-AI4Med/RadABench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages,7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient and Comprehensive Feature Extraction in Large Vision-Language
  Model for Clinical Pathology Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengxuming Zhang, Weihan Li, Tianhong Gao, Jiacong Hu, Haoming Luo, Mingli Song, Xiuming Zhang, Zunlei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pathological diagnosis is vital for determining disease characteristics,
guiding treatment, and assessing prognosis, relying heavily on detailed,
multi-scale analysis of high-resolution whole slide images (WSI). However,
traditional pure vision models face challenges of redundant feature extraction,
whereas existing large vision-language models (LVLMs) are limited by input
resolution constraints, hindering their efficiency and accuracy. To overcome
these issues, we propose two innovative strategies: the mixed task-guided
feature enhancement, which directs feature extraction toward lesion-related
details across scales, and the prompt-guided detail feature completion, which
integrates coarse- and fine-grained features from WSI based on specific prompts
without compromising inference speed. Leveraging a comprehensive dataset of
490,000 samples from diverse pathology tasks-including cancer detection,
grading, vascular and neural invasion identification, and so on-we trained the
pathology-specialized LVLM, OmniPath. Extensive experiments demonstrate that
this model significantly outperforms existing methods in diagnostic accuracy
and efficiency, offering an interactive, clinically aligned approach for
auxiliary diagnosis in a wide range of pathology applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agent-based Video Trimming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingfeng Yang, Zhenyuan Chen, Xiang Li, Peiyang Jia, Liangqu Long, Jian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As information becomes more accessible, user-generated videos are increasing
in length, placing a burden on viewers to sift through vast content for
valuable insights. This trend underscores the need for an algorithm to extract
key video information efficiently. Despite significant advancements in
highlight detection, moment retrieval, and video summarization, current
approaches primarily focus on selecting specific time intervals, often
overlooking the relevance between segments and the potential for segment
arranging. In this paper, we introduce a novel task called Video Trimming (VT),
which focuses on detecting wasted footage, selecting valuable segments, and
composing them into a final video with a coherent story. To address this task,
we propose Agent-based Video Trimming (AVT), structured into three phases:
Video Structuring, Clip Filtering, and Story Composition. Specifically, we
employ a Video Captioning Agent to convert video slices into structured textual
descriptions, a Filtering Module to dynamically discard low-quality footage
based on the structured information of each clip, and a Video Arrangement Agent
to select and compile valid clips into a coherent final narrative. For
evaluation, we develop a Video Evaluation Agent to assess trimmed videos,
conducting assessments in parallel with human evaluations. Additionally, we
curate a new benchmark dataset for video trimming using raw user videos from
the internet. As a result, AVT received more favorable evaluations in user
studies and demonstrated superior mAP and precision on the YouTube Highlights,
TVSum, and our own dataset for the highlight detection task. The code and
models are available at https://ylingfeng.github.io/AVT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GEAL: Generalizable 3D Affordance Learning with Cross-Modal Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyue Lu, Lingdong Kong, Tianxin Huang, Gim Hee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying affordance regions on 3D objects from semantic cues is essential
for robotics and human-machine interaction. However, existing 3D affordance
learning methods struggle with generalization and robustness due to limited
annotated data and a reliance on 3D backbones focused on geometric encoding,
which often lack resilience to real-world noise and data corruption. We propose
GEAL, a novel framework designed to enhance the generalization and robustness
of 3D affordance learning by leveraging large-scale pre-trained 2D models. We
employ a dual-branch architecture with Gaussian splatting to establish
consistent mappings between 3D point clouds and 2D representations, enabling
realistic 2D renderings from sparse point clouds. A granularity-adaptive fusion
module and a 2D-3D consistency alignment module further strengthen cross-modal
alignment and knowledge transfer, allowing the 3D branch to benefit from the
rich semantics and generalization capacity of 2D models. To holistically assess
the robustness, we introduce two new corruption-based benchmarks: PIAD-C and
LASO-C. Extensive experiments on public datasets and our benchmarks show that
GEAL consistently outperforms existing methods across seen and novel object
categories, as well as corrupted data, demonstrating robust and adaptable
affordance prediction under diverse conditions. Code and corruption datasets
have been made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 8 figures, 12 tables; Project Page at
  https://dylanorange.github.io/projects/geal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision <span class="highlight-title">Transformer</span>s for Efficient Indoor Pathloss Radio Map Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edvard Ghukasyan, Hrant Khachatrian, Rafayel Mkrtchyan, Theofanis P. Raptis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers (ViTs) have demonstrated remarkable success in achieving
state-of-the-art performance across various image-based tasks and beyond. In
this study, we employ a ViT-based neural network to address the problem of
indoor pathloss radio map prediction. The network's generalization ability is
evaluated across diverse settings, including unseen buildings, frequencies, and
antennas with varying radiation patterns. By leveraging extensive data
augmentation techniques and pretrained DINOv2 weights, we achieve promising
results, even under the most challenging scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work partly supported by the RA Science Committee grant No. 22rl-052
  (DISTAL) and the EU under Italian National Recovery and Resilience Plan of
  NextGenerationEU on "Telecommunications of the Future" (PE00000001 - program
  "RESTART")</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhisheng Zhong, Chengyao Wang, Yuqi Liu, Senqiao Yang, Longxiang Tang, Yuechen Zhang, Jingyao Li, Tianyuan Qu, Yanwei Li, Yukang Chen, Shaozuo Yu, Sitong Wu, Eric Lo, Shu Liu, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Multi-modal Large Language Models (MLLMs) evolve, expanding beyond
single-domain capabilities is essential to meet the demands for more versatile
and efficient AI. However, previous omni-models have insufficiently explored
speech, neglecting its integration with multi-modality. We introduce Lyra, an
efficient MLLM that enhances multimodal abilities, including advanced
long-speech comprehension, sound understanding, cross-modality efficiency, and
seamless speech interaction. To achieve efficiency and speech-centric
capabilities, Lyra employs three strategies: (1) leveraging existing
open-source large models and a proposed multi-modality LoRA to reduce training
costs and data requirements; (2) using a latent multi-modality regularizer and
extractor to strengthen the relationship between speech and other modalities,
thereby enhancing model performance; and (3) constructing a high-quality,
extensive dataset that includes 1.5M multi-modal (language, vision, audio) data
samples and 12K long speech samples, enabling Lyra to handle complex long
speech inputs and achieve more robust omni-cognition. Compared to other
omni-methods, Lyra achieves state-of-the-art performance on various
vision-language, vision-speech, and speech-language benchmarks, while also
using fewer computational resources and less training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video Seal: Open and Efficient Video Watermarking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Fernandez, Hady Elsahar, I. Zeki Yalniz, Alexandre Mourachko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of AI-generated content and sophisticated video editing
tools has made it both important and challenging to moderate digital platforms.
Video watermarking addresses these challenges by embedding imperceptible
signals into videos, allowing for identification. However, the rare open tools
and methods often fall short on efficiency, robustness, and flexibility. To
reduce these gaps, this paper introduces Video Seal, a comprehensive framework
for neural video watermarking and a competitive open-sourced model. Our
approach jointly trains an embedder and an extractor, while ensuring the
watermark robustness by applying transformations in-between, e.g., video
codecs. This training is multistage and includes image pre-training, hybrid
post-training and extractor fine-tuning. We also introduce temporal watermark
propagation, a technique to convert any image watermarking model to an
efficient video watermarking model without the need to watermark every
high-resolution frame. We present experimental results demonstrating the
effectiveness of the approach in terms of speed, imperceptibility, and
robustness. Video Seal achieves higher robustness compared to strong baselines
especially under challenging distortions combining geometric transformations
and video compression. Additionally, we provide new insights such as the impact
of video compression during training, and how to compare methods operating on
different payloads. Contributions in this work - including the codebase,
models, and a public demo - are open-sourced under permissive licenses to
foster further research and development in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at https://github.com/facebookresearch/videoseal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ New keypoint-based approach for recognising British Sign Language (BSL)
  from sequences <span class="chip">ICCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oishi Deb, KR Prajwal, Andrew Zisserman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a novel keypoint-based classification model
designed to recognise British Sign Language (BSL) words within continuous
signing sequences. Our model's performance is assessed using the BOBSL dataset,
revealing that the keypoint-based approach surpasses its RGB-based counterpart
in computational efficiency and memory usage. Furthermore, it offers expedited
training times and demands fewer computational resources. To the best of our
knowledge, this is the inaugural application of a keypoint-based model for BSL
word classification, rendering direct comparisons with existing works
unavailable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Computer Vision (ICCV) - HANDS Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OFTSR: One-Step Flow for Image Super-Resolution with Tunable
  Fidelity-Realism Trade-offs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanzhi Zhu, Ruiqing Wang, Shilin Lu, Junnan Li, Hanshu Yan, Kai Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in diffusion and flow-based generative models have
demonstrated remarkable success in image restoration tasks, achieving superior
perceptual quality compared to traditional deep learning approaches. However,
these methods either require numerous sampling steps to generate high-quality
images, resulting in significant computational overhead, or rely on model
distillation, which usually imposes a fixed fidelity-realism trade-off and thus
lacks flexibility. In this paper, we introduce OFTSR, a novel flow-based
framework for one-step image super-resolution that can produce outputs with
tunable levels of fidelity and realism. Our approach first trains a conditional
flow-based super-resolution model to serve as a teacher model. We then distill
this teacher model by applying a specialized constraint. Specifically, we force
the predictions from our one-step student model for same input to lie on the
same sampling ODE trajectory of the teacher model. This alignment ensures that
the student model's single-step predictions from initial states match the
teacher's predictions from a closer intermediate state. Through extensive
experiments on challenging datasets including FFHQ (256$\times$256), DIV2K, and
ImageNet (256$\times$256), we demonstrate that OFTSR achieves state-of-the-art
performance for one-step image super-resolution, while having the ability to
flexibly tune the fidelity-realism trade-off. Code and pre-trained models are
available at https://github.com/yuanzhi-zhu/OFTSR and
https://huggingface.co/Yuanzhi/OFTSR, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embeddings are all you need! Achieving High Performance Medical Image
  Classification through Training-Free Embedding Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raj Hansini Khoiwal, Alan B. McMillan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing artificial intelligence (AI) and machine learning (ML) models for
medical imaging typically involves extensive training and testing on large
datasets, consuming significant computational time, energy, and resources.
There is a need for more efficient methods that can achieve comparable or
superior diagnostic performance without the associated resource burden. We
investigated the feasibility of replacing conventional training procedures with
an embedding-based approach that leverages concise and semantically meaningful
representations of medical images. Using pre-trained foundational
models-specifically, convolutional neural networks (CNN) like ResNet and
multimodal models like Contrastive Language-Image Pre-training (CLIP)-we
generated image embeddings for multi-class classification tasks. Simple linear
classifiers were then applied to these embeddings. The approach was evaluated
across diverse medical imaging modalities, including retinal images,
mammography, dermatoscopic images, and chest radiographs. Performance was
compared to benchmark models trained and tested using traditional methods. The
embedding-based models surpassed the benchmark area under the receiver
operating characteristic curve (AUC-ROC) scores by up to 87 percentage in
multi-class classification tasks across the various medical imaging modalities.
Notably, CLIP embedding models achieved the highest AUC-ROC scores,
demonstrating superior classification performance while significantly reducing
computational demands. Our study indicates that leveraging embeddings from
pre-trained foundational models can effectively replace conventional,
resource-intensive training and testing procedures in medical image analysis.
This embedding-based approach offers a more efficient alternative for image
segmentation, classification, and prediction, potentially accelerating AI
technology integration into clinical practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MOS: Model Surgery for <span class="highlight-title">Pre-Train</span>ed Model-Based Class-Incremental
  Learning <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai-Long Sun, Da-Wei Zhou, Hanbin Zhao, Le Gan, De-Chuan Zhan, Han-Jia Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-Incremental Learning (CIL) requires models to continually acquire
knowledge of new classes without forgetting old ones. Despite Pre-trained
Models (PTMs) have shown excellent performance in CIL, catastrophic forgetting
still occurs as the model learns new concepts. Existing work seeks to utilize
lightweight components to adjust the PTM, while the forgetting phenomenon still
comes from {\em parameter and retrieval} levels. Specifically, iterative
updates of the model result in parameter drift, while mistakenly retrieving
irrelevant modules leads to the mismatch during inference. To this end, we
propose MOdel Surgery (MOS) to rescue the model from forgetting previous
knowledge. By training task-specific adapters, we continually adjust the PTM to
downstream tasks. To mitigate parameter-level forgetting, we present an adapter
merging approach to learn task-specific adapters, which aims to bridge the gap
between different components while reserve task-specific information. Besides,
to address retrieval-level forgetting, we introduce a training-free
self-refined adapter retrieval mechanism during inference, which leverages the
model's inherent ability for better adapter retrieval. By jointly rectifying
the model with those steps, MOS can robustly resist catastrophic forgetting in
the learning process. Extensive experiments on seven benchmark datasets
validate MOS's state-of-the-art performance. Code is available at:
https://github.com/sun-hailong/AAAI25-MOS
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2025. Code is available at:
  https://github.com/sun-hailong/AAAI25-MOS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AT<span class="highlight-title">Prompt</span>: Textual <span class="highlight-title">Prompt</span> Learning with Embedded Attributes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Li, Yibing Song, Penghai Zhao, Ming-Ming Cheng, Xiang Li, Jian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Textual-based prompt learning methods primarily employ multiple learnable
soft prompts and hard class tokens in a cascading manner as text prompt inputs,
aiming to align image and text (category) spaces for downstream tasks. However,
current training is restricted to aligning images with predefined known
categories and cannot be associated with unknown categories. In this work, we
propose utilizing universal attributes as a bridge to enhance the alignment
between images and unknown categories. Specifically, we introduce an
Attribute-embedded Textual Prompt learning method for vision-language models,
named ATPrompt. This approach expands the learning space of soft prompts from
the original one-dimensional category level into the multi-dimensional
attribute level by incorporating multiple universal attribute tokens into the
learnable soft prompts. Through this modification, we transform the text prompt
from a category-centric form to an attribute-category hybrid form. To finalize
the attributes for downstream tasks, we propose a differentiable attribute
search method that learns to identify representative and suitable attributes
from a candidate pool summarized by a large language model. As an easy-to-use
plug-in technique, ATPrompt can seamlessly replace the existing prompt format
of textual-based methods, offering general improvements at a negligible
computational cost. Extensive experiments on 11 datasets demonstrate the
effectiveness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report. Project Page: https://zhengli97.github.io/ATPrompt/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Robust and Fair Vision Learning in Open-World Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thanh-Dat Truong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dissertation presents four key contributions toward fairness and
robustness in vision learning. First, to address the problem of large-scale
data requirements, the dissertation presents a novel Fairness Domain Adaptation
approach derived from two major novel research findings of Bijective Maximum
Likelihood and Fairness Adaptation Learning. Second, to enable the capability
of open-world modeling of vision learning, this dissertation presents a novel
Open-world Fairness Continual Learning Framework. The success of this research
direction is the result of two research lines, i.e., Fairness Continual
Learning and Open-world Continual Learning. Third, since visual data are often
captured from multiple camera views, robust vision learning methods should be
capable of modeling invariant features across views. To achieve this desired
goal, the research in this thesis will present a novel Geometry-based
Cross-view Adaptation framework to learn robust feature representations across
views. Finally, with the recent increase in large-scale videos and multimodal
data, understanding the feature representations and improving the robustness of
large-scale visual foundation models is critical. Therefore, this thesis will
present novel Transformer-based approaches to improve the robust feature
representations against multimodal and temporal data. Then, a novel Domain
Generalization Approach will be presented to improve the robustness of visual
foundation models. The research's theoretical analysis and experimental results
have shown the effectiveness of the proposed approaches, demonstrating their
superior performance compared to prior studies. The contributions in this
dissertation have advanced the fairness and robustness of machine vision
learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD Dissertation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Music Generation with Explicit Bridges and Retrieval
  Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baisen Wang, Le Zhuo, Zhaokai Wang, Chenxi Bao, Wu Chengjing, Xuecheng Nie, Jiao Dai, Jizhong Han, Yue Liao, Si Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal music generation aims to produce music from diverse input
modalities, including text, videos, and images. Existing methods use a common
embedding space for multimodal fusion. Despite their effectiveness in other
modalities, their application in multimodal music generation faces challenges
of data scarcity, weak cross-modal alignment, and limited controllability. This
paper addresses these issues by using explicit bridges of text and music for
multimodal alignment. We introduce a novel method named Visuals Music Bridge
(VMB). Specifically, a Multimodal Music Description Model converts visual
inputs into detailed textual descriptions to provide the text bridge; a
Dual-track Music Retrieval module that combines broad and targeted retrieval
strategies to provide the music bridge and enable user control. Finally, we
design an Explicitly Conditioned Music Generation framework to generate music
based on the two bridges. We conduct experiments on video-to-music,
image-to-music, text-to-music, and controllable music generation tasks, along
with experiments on controllability. The results demonstrate that VMB
significantly enhances music quality, modality, and customization alignment
compared to previous methods. VMB sets a new standard for interpretable and
expressive multimodal music generation with applications in various multimedia
fields. Demos and code are available at https://github.com/wbs2788/VMB.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Plug-and-Play Algorithm for 3D Video Super-Resolution of Single-Photon
  LiDAR data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09427v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09427v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alice Ruget, Lewis Wilson, Jonathan Leach, Rachael Tobin, Aongus Mccarthy, Gerald S. Buller, Steve Mclaughlin, Abderrahim Halimi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Single-photon avalanche diodes (SPADs) are advanced sensors capable of
detecting individual photons and recording their arrival times with picosecond
resolution using time-correlated Single-Photon Counting detection techniques.
They are used in various applications, such as LiDAR, and can capture
high-speed sequences of binary single-photon images, offering great potential
for reconstructing 3D environments with high motion dynamics. To complement
single-photon data, they are often paired with conventional passive cameras,
which capture high-resolution (HR) intensity images at a lower frame rate.
However, 3D reconstruction from SPAD data faces challenges. Aggregating
multiple binary measurements improves precision and reduces noise but can cause
motion blur in dynamic scenes. Additionally, SPAD arrays often have lower
resolution than passive cameras. To address these issues, we propose a novel
computational imaging algorithm to improve the 3D reconstruction of moving
scenes from SPAD data by addressing the motion blur and increasing the native
spatial resolution. We adopt a plug-and-play approach within an optimization
scheme alternating between guided video super-resolution of the 3D scene, and
precise image realignment using optical flow. Experiments on synthetic data
show significantly improved image resolutions across various signal-to-noise
ratios and photon levels. We validate our method using real-world SPAD
measurements on three practical situations with dynamic objects. First on
fast-moving scenes in laboratory conditions at short range; second very low
resolution imaging of people with a consumer-grade SPAD sensor from
STMicroelectronics; and finally, HR imaging of people walking outdoors in
daylight at a range of 325 meters under eye-safe illumination conditions using
a short-wave infrared SPAD camera. These results demonstrate the robustness and
versatility of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learned Compression for Compressed Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dan Jacobellis, Neeraja J. Yadwadkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern sensors produce increasingly rich streams of high-resolution data. Due
to resource constraints, machine learning systems discard the vast majority of
this information via resolution reduction. Compressed-domain learning allows
models to operate on compact latent representations, allowing higher effective
resolution for the same budget. However, existing compression systems are not
ideal for compressed learning. Linear transform coding and end-to-end learned
compression systems reduce bitrate, but do not uniformly reduce dimensionality;
thus, they do not meaningfully increase efficiency. Generative autoencoders
reduce dimensionality, but their adversarial or perceptual objectives lead to
significant information loss. To address these limitations, we introduce WaLLoC
(Wavelet Learned Lossy Compression), a neural codec architecture that combines
linear transform coding with nonlinear dimensionality-reducing autoencoders.
WaLLoC sandwiches a shallow, asymmetric autoencoder and entropy bottleneck
between an invertible wavelet packet transform. Across several key metrics,
WaLLoC outperforms the autoencoders used in state-of-the-art latent diffusion
models. WaLLoC does not require perceptual or adversarial losses to represent
high-frequency detail, providing compatibility with modalities beyond RGB
images and stereo audio. WaLLoC's encoder consists almost entirely of linear
operations, making it exceptionally efficient and suitable for mobile
computing, remote sensing, and learning directly from compressed data. We
demonstrate WaLLoC's capability for compressed-domain learning across several
tasks, including image classification, colorization, document understanding,
and music source separation. Our code, experiments, and pre-trained audio and
image codecs are available at https://ut-sysml.org/walloc
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as paper to 2025 IEEE Data Compression Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiEYE: <span class="highlight-title">Dataset</span> and Benchmark for OCT-Enhanced Retinal Disease
  Recognition from Fundus Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lehan Wang, Chongchong Qi, Chubin Ou, Lin An, Mei Jin, Xiangbin Kong, Xiaomeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing multi-modal learning methods on fundus and OCT images mostly require
both modalities to be available and strictly paired for training and testing,
which appears less practical in clinical scenarios. To expand the scope of
clinical applications, we formulate a novel setting, "OCT-enhanced disease
recognition from fundus images", that allows for the use of unpaired
multi-modal data during the training phase and relies on the widespread fundus
photographs for testing. To benchmark this setting, we present the first large
multi-modal multi-class dataset for eye disease diagnosis, MultiEYE, and
propose an OCT-assisted Conceptual Distillation Approach (OCT-CoDA), which
employs semantically rich concepts to extract disease-related knowledge from
OCT images and leverage them into the fundus model. Specifically, we regard the
image-concept relation as a link to distill useful knowledge from the OCT
teacher model to the fundus student model, which considerably improves the
diagnostic performance based on fundus images and formulates the cross-modal
knowledge transfer into an explainable process. Through extensive experiments
on the multi-disease classification task, our proposed OCT-CoDA demonstrates
remarkable results and interpretability, showing great potential for clinical
application. Our dataset and code are available at
https://github.com/xmed-lab/MultiEYE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE TMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzheng Liu, Siyan Dong, Shuzhe Wang, Yingda Yin, Yanchao Yang, Qingnan Fan, Baoquan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce \textbf{SLAM3R}, a novel and effective monocular
RGB SLAM system for real-time and high-quality dense 3D reconstruction. SLAM3R
provides an end-to-end solution by seamlessly integrating local 3D
reconstruction and global coordinate registration through feed-forward neural
networks. Given an input video, the system first converts it into overlapping
clips using a sliding window mechanism. Unlike traditional pose
optimization-based methods, SLAM3R directly regresses 3D pointmaps from RGB
images in each window and progressively aligns and deforms these local
pointmaps to create a globally consistent scene reconstruction - all without
explicitly solving any camera parameters. Experiments across datasets
consistently show that SLAM3R achieves state-of-the-art reconstruction accuracy
and completeness while maintaining real-time performance at 20+ FPS. Code and
weights at: \url{https://github.com/PKU-VCL-3DV/SLAM3R}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UFO: Enhancing Diffusion-Based Video Generation with a Uniform Frame
  Organizer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Delong Liu, Zhaohui Hou, Mingjie Zhan, Shihao Han, Zhicheng Zhao, Fei Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, diffusion-based video generation models have achieved significant
success. However, existing models often suffer from issues like weak
consistency and declining image quality over time. To overcome these
challenges, inspired by aesthetic principles, we propose a non-invasive plug-in
called Uniform Frame Organizer (UFO), which is compatible with any
diffusion-based video generation model. The UFO comprises a series of adaptive
adapters with adjustable intensities, which can significantly enhance the
consistency between the foreground and background of videos and improve image
quality without altering the original model parameters when integrated. The
training for UFO is simple, efficient, requires minimal resources, and supports
stylized training. Its modular design allows for the combination of multiple
UFOs, enabling the customization of personalized video generation models.
Furthermore, the UFO also supports direct transferability across different
models of the same specification without the need for specific retraining. The
experimental results indicate that UFO effectively enhances video generation
quality and demonstrates its superiority in public video generation benchmarks.
The code will be publicly available at https://github.com/Delong-liu-bupt/UFO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code:https://github.com/Delong-liu-bupt/UFO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ All You Need in Knowledge Distillation Is a Tailored Coordinate System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Zhou, Ke Zhu, Jianxin Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Distillation (KD) is essential in transferring dark knowledge from
a large teacher to a small student network, such that the student can be much
more efficient than the teacher but with comparable accuracy. Existing KD
methods, however, rely on a large teacher trained specifically for the target
task, which is both very inflexible and inefficient. In this paper, we argue
that a SSL-pretrained model can effectively act as the teacher and its dark
knowledge can be captured by the coordinate system or linear subspace where the
features lie in. We then need only one forward pass of the teacher, and then
tailor the coordinate system (TCS) for the student network. Our TCS method is
teacher-free and applies to diverse architectures, works well for KD and
practical few-shot learning, and allows cross-architecture distillation with
large capacity gap. Experiments show that TCS achieves significantly higher
accuracy than state-of-the-art KD methods, while only requiring roughly half of
their training time and GPU memory costs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Stage Segmentation and Cascade Classification Methods for
  Improving Cardiac MRI Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09386v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09386v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vitalii Slobodzian, Pavlo Radiuk, Oleksander Barmak, Iurii Krak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The segmentation and classification of cardiac magnetic resonance imaging are
critical for diagnosing heart conditions, yet current approaches face
challenges in accuracy and generalizability. In this study, we aim to further
advance the segmentation and classification of cardiac magnetic resonance
images by introducing a novel deep learning-based approach. Using a multi-stage
process with U-Net and ResNet models for segmentation, followed by Gaussian
smoothing, the method improved segmentation accuracy, achieving a Dice
coefficient of 0.974 for the left ventricle and 0.947 for the right ventricle.
For classification, a cascade of deep learning classifiers was employed to
distinguish heart conditions, including hypertrophic cardiomyopathy, myocardial
infarction, and dilated cardiomyopathy, achieving an average accuracy of 97.2%.
The proposed approach outperformed existing models, enhancing segmentation
accuracy and classification precision. These advancements show promise for
clinical applications, though further validation and interpretation across
diverse imaging protocols is necessary.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Cardiac MRI, heart pathology, deep learning, segmentation, Gaussian
  smoothing, classification, cascade</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Graphical Models for Vision-Language Compositional Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fiorenzo Parascandolo, Nicholas Moratelli, Enver Sangineto, Lorenzo Baraldi, Rita Cucchiara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has empirically shown that Vision-Language Models (VLMs) struggle
to fully understand the compositional properties of the human language, usually
modeling an image caption as a "bag of words". As a result, they perform poorly
on compositional tasks, which require a deeper understanding of the different
entities of a sentence (subject, verb, etc.) jointly with their mutual
relationships in order to be solved. In this paper, we model the dependency
relations among textual and visual tokens using a Causal Graphical Model (CGM),
built using a dependency parser, and we train a decoder conditioned by the VLM
visual encoder. Differently from standard autoregressive or parallel
predictions, our decoder's generative process is partially-ordered following
the CGM structure. This structure encourages the decoder to learn only the main
causal dependencies in a sentence discarding spurious correlations. Using
extensive experiments on five compositional benchmarks, we show that our method
significantly outperforms all the state-of-the-art compositional approaches by
a large margin, and it also improves over methods trained using much larger
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DisPose: Disentangling Pose Guidance for Controllable Human Image
  Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongxiang Li, Yaowei Li, Yuhang Yang, Junjie Cao, Zhihong Zhu, Xuxin Cheng, Long Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controllable human image animation aims to generate videos from reference
images using driving videos. Due to the limited control signals provided by
sparse guidance (e.g., skeleton pose), recent works have attempted to introduce
additional dense conditions (e.g., depth map) to ensure motion alignment.
However, such strict dense guidance impairs the quality of the generated video
when the body shape of the reference character differs significantly from that
of the driving video. In this paper, we present DisPose to mine more
generalizable and effective control signals without additional dense input,
which disentangles the sparse skeleton pose in human image animation into
motion field guidance and keypoint correspondence. Specifically, we generate a
dense motion field from a sparse motion field and the reference image, which
provides region-level dense guidance while maintaining the generalization of
the sparse pose control. We also extract diffusion features corresponding to
pose keypoints from the reference image, and then these point features are
transferred to the target pose to provide distinct identity information. To
seamlessly integrate into existing models, we propose a plug-and-play hybrid
ControlNet that improves the quality and consistency of generated videos while
freezing the existing model parameters. Extensive qualitative and quantitative
experiments demonstrate the superiority of DisPose compared to current methods.
Code:
\hyperlink{https://github.com/lihxxx/DisPose}{https://github.com/lihxxx/DisPose}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantitative Evaluation of Motif Sets in Time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daan Van Wesenbeeck, Aras Yurtman, Wannes Meert, Hendrik Blockeel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time Series Motif Discovery (TSMD), which aims at finding recurring patterns
in time series, is an important task in numerous application domains, and many
methods for this task exist. These methods are usually evaluated qualitatively.
A few metrics for quantitative evaluation, where discovered motifs are compared
to some ground truth, have been proposed, but they typically make implicit
assumptions that limit their applicability. This paper introduces PROM, a
broadly applicable metric that overcomes those limitations, and TSMD-Bench, a
benchmark for quantitative evaluation of time series motif discovery.
Experiments with PROM and TSMD-Bench show that PROM provides a more
comprehensive evaluation than existing metrics, that TSMD-Bench is a more
challenging benchmark than earlier ones, and that the combination can help
understand the relative performance of TSMD methods. More generally, the
proposed approach enables large-scale, systematic performance comparisons in
this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MaskTerial: A Foundation Model for Automated 2D Material Flake Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan-Lucas Uslu, Alexey Nekrasov, Alexander Hermans, Bernd Beschoten, Bastian Leibe, Lutz Waldecker, Christoph Stampfer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detection and classification of exfoliated two-dimensional (2D) material
flakes from optical microscope images can be automated using computer vision
algorithms. This has the potential to increase the accuracy and objectivity of
classification and the efficiency of sample fabrication, and it allows for
large-scale data collection. Existing algorithms often exhibit challenges in
identifying low-contrast materials and typically require large amounts of
training data. Here, we present a deep learning model, called MaskTerial, that
uses an instance segmentation network to reliably identify 2D material flakes.
The model is extensively pre-trained using a synthetic data generator, that
generates realistic microscopy images from unlabeled data. This results in a
model that can to quickly adapt to new materials with as little as 5 to 10
images. Furthermore, an uncertainty estimation model is used to finally
classify the predictions based on optical contrast. We evaluate our method on
eight different datasets comprising five different 2D materials and demonstrate
significant improvements over existing techniques in the detection of
low-contrast materials such as hexagonal boron nitride.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Driven Autoregressive State Space Models for Medical Image
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bilal Kabas, Fuat Arslan, Valiyeh A. Nezhad, Saban Ozturk, Emine U. Saritas, Tolga Çukur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image reconstruction from undersampled acquisitions is an ill-posed
problem that involves inversion of the imaging operator linking measurement and
image domains. In recent years, physics-driven (PD) models have gained
prominence in learning-based reconstruction given their enhanced balance
between efficiency and performance. For reconstruction, PD models cascade
data-consistency modules that enforce fidelity to acquired data based on the
imaging operator, with network modules that process feature maps to alleviate
image artifacts due to undersampling. Success in artifact suppression
inevitably depends on the ability of the network modules to tease apart
artifacts from underlying tissue structures, both of which can manifest
contextual relations over broad spatial scales. Convolutional modules that
excel at capturing local correlations are relatively insensitive to non-local
context. While transformers promise elevated sensitivity to non-local context,
practical implementations often suffer from a suboptimal trade-off between
local and non-local sensitivity due to intrinsic model complexity. Here, we
introduce a novel physics-driven autoregressive state space model (MambaRoll)
for enhanced fidelity in medical image reconstruction. In each cascade of an
unrolled architecture, MambaRoll employs an autoregressive framework based on
physics-driven state space modules (PSSM), where PSSMs efficiently aggregate
contextual features at a given spatial scale while maintaining fidelity to
acquired data, and autoregressive prediction of next-scale feature maps from
earlier spatial scales enhance capture of multi-scale contextual features.
Demonstrations on accelerated MRI and sparse-view CT reconstructions indicate
that MambaRoll outperforms state-of-the-art PD methods based on convolutional,
transformer and conventional SSM modules.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computer-Aided Osteoporosis Diagnosis Using Transfer Learning with
  Enhanced Features from Stacked Deep Learning Modules 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayesha Siddiqua, Rakibul Hasan, Anichur Rahman, Abu Saleh Musa Miah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knee osteoporosis weakens the bone tissue in the knee joint, increasing
fracture risk. Early detection through X-ray images enables timely intervention
and improved patient outcomes. While some researchers have focused on
diagnosing knee osteoporosis through manual radiology evaluation and
traditional machine learning using hand-crafted features, these methods often
struggle with performance and efficiency due to reliance on manual feature
extraction and subjective interpretation. In this study, we propose a
computer-aided diagnosis (CAD) system for knee osteoporosis, combining transfer
learning with stacked feature enhancement deep learning blocks. Initially, knee
X-ray images are preprocessed, and features are extracted using a pre-trained
Convolutional Neural Network (CNN). These features are then enhanced through
five sequential Conv-RELU-MaxPooling blocks. The Conv2D layers detect low-level
features, while the ReLU activations introduce non-linearity, allowing the
network to learn complex patterns. MaxPooling layers down-sample the features,
retaining the most important spatial information. This sequential processing
enables the model to capture complex, high-level features related to bone
structure, joint deformation, and osteoporotic markers. The enhanced features
are passed through a classification module to differentiate between healthy and
osteoporotic knee conditions. Extensive experiments on three individual
datasets and a combined dataset demonstrate that our model achieves 97.32%,
98.24%, 97.27%, and 98.00% accuracy for OKX Kaggle Binary, KXO-Mendeley
Multi-Class, OKX Kaggle Multi-Class, and the combined dataset, respectively,
showing an improvement of around 2% over existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Conditional Latent Diffusion Models Effective for Image Restoration? <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunchen Yuan, Junyuan Xiao, Xinjie Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in image restoration increasingly employ conditional
latent diffusion models (CLDMs). While these models have demonstrated notable
performance improvements in recent years, this work questions their suitability
for IR tasks. CLDMs excel in capturing high-level semantic correlations, making
them effective for tasks like text-to-image generation with spatial
conditioning. However, in IR, where the goal is to enhance image perceptual
quality, these models face difficulty of modeling the relationship between
degraded images and ground truth images using a low-level representation. To
support our claims, we compare state-of-the-art CLDMs with traditional image
restoration models through extensive experiments. Results reveal that despite
the scaling advantages of CLDMs, they suffer from high distortion and semantic
deviation, especially in cases with minimal degradation, where traditional
methods outperform them. Additionally, we perform empirical studies to examine
the impact of various CLDM design elements on their restoration performance. We
hope this finding inspires a reexamination of current CLDM-based IR solutions,
opening up more opportunities in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 12 figures, submitted to IEEE / CVF Computer Vision and
  Pattern Recognition Conference (CVPR 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ T-SVG: Text-Driven Stereoscopic Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiao Jin, Xiaodong Chen, Wu Liu, Tao Mei, Yongdong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of stereoscopic videos has opened new horizons in multimedia,
particularly in extended reality (XR) and virtual reality (VR) applications,
where immersive content captivates audiences across various platforms. Despite
its growing popularity, producing stereoscopic videos remains challenging due
to the technical complexities involved in generating stereo parallax. This
refers to the positional differences of objects viewed from two distinct
perspectives and is crucial for creating depth perception. This complex process
poses significant challenges for creators aiming to deliver convincing and
engaging presentations. To address these challenges, this paper introduces the
Text-driven Stereoscopic Video Generation (T-SVG) system. This innovative,
model-agnostic, zero-shot approach streamlines video generation by using text
prompts to create reference videos. These videos are transformed into 3D point
cloud sequences, which are rendered from two perspectives with subtle parallax
differences, achieving a natural stereoscopic effect. T-SVG represents a
significant advancement in stereoscopic content creation by integrating
state-of-the-art, training-free techniques in text-to-video generation, depth
estimation, and video inpainting. Its flexible architecture ensures high
efficiency and user-friendliness, allowing seamless updates with newer models
without retraining. By simplifying the production pipeline, T-SVG makes
stereoscopic video generation accessible to a broader audience, demonstrating
its potential to revolutionize the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FAMNet: Frequency-aware Matching Network for Cross-domain Few-shot
  Medical Image Segmentation <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuntian Bo, Yazhou Zhu, Lunbo Li, Haofeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing few-shot medical image segmentation (FSMIS) models fail to address a
practical issue in medical imaging: the domain shift caused by different
imaging techniques, which limits the applicability to current FSMIS tasks. To
overcome this limitation, we focus on the cross-domain few-shot medical image
segmentation (CD-FSMIS) task, aiming to develop a generalized model capable of
adapting to a broader range of medical image segmentation scenarios with
limited labeled data from the novel target domain. Inspired by the
characteristics of frequency domain similarity across different domains, we
propose a Frequency-aware Matching Network (FAMNet), which includes two key
components: a Frequency-aware Matching (FAM) module and a Multi-Spectral Fusion
(MSF) module. The FAM module tackles two problems during the meta-learning
phase: 1) intra-domain variance caused by the inherent support-query bias, due
to the different appearances of organs and lesions, and 2) inter-domain
variance caused by different medical imaging techniques. Additionally, we
design an MSF module to integrate the different frequency features decoupled by
the FAM module, and further mitigate the impact of inter-domain variance on the
model's segmentation performance. Combining these two modules, our FAMNet
surpasses existing FSMIS models and Cross-domain Few-shot Semantic Segmentation
models on three cross-domain datasets, achieving state-of-the-art performance
in the CD-FSMIS task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 39th Annual AAAI Conference on Artificial
  Intelligence (AAAI-25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Sentiment Analysis based on Video and Audio Inputs <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Fernandez, Suzan Awinat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the abundance of current researches working on the sentiment analysis
from videos and audios, finding the best model that gives the highest accuracy
rate is still considered a challenge for researchers in this field. The main
objective of this paper is to prove the usability of emotion recognition models
that take video and audio inputs. The datasets used to train the models are the
CREMA-D dataset for audio and the RAVDESS dataset for video. The fine-tuned
models that been used are: Facebook/wav2vec2-large for audio and the
Google/vivit-b-16x2-kinetics400 for video. The avarage of the probabilities for
each emotion generated by the two previous models is utilized in the decision
making framework. After disparity in the results, if one of the models gets
much higher accuracy, another test framework is created. The methods used are
the Weighted Average method, the Confidence Level Threshold method, the Dynamic
Weighting Based on Confidence method, and the Rule-Based Logic method. This
limited approach gives encouraging results that make future research into these
methods viable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented as a full paper in the 15th International Conference on
  Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2024) October
  28-30, 2024, Leuven, Belgium</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Attribution-Based Neural Network Explainability through
  Relative Absolute Magnitude Layer-Wise Relevance Propagation and
  Multi-Component Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davor Vukadin, Petar Afrić, Marin Šilić, Goran Delač
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancement in deep-neural network performance led to the development
of new state-of-the-art approaches in numerous areas. However, the black-box
nature of neural networks often prohibits their use in areas where model
explainability and model transparency are crucial. Over the years, researchers
proposed many algorithms to aid neural network understanding and provide
additional information to the human expert. One of the most popular methods
being Layer-Wise Relevance Propagation (LRP). This method assigns local
relevance based on the pixel-wise decomposition of nonlinear classifiers. With
the rise of attribution method research, there has emerged a pressing need to
assess and evaluate their performance. Numerous metrics have been proposed,
each assessing an individual property of attribution methods such as
faithfulness, robustness or localization. Unfortunately, no single metric is
deemed optimal for every case, and researchers often use several metrics to
test the quality of the attribution maps. In this work, we address the
shortcomings of the current LRP formulations and introduce a novel method for
determining the relevance of input neurons through layer-wise relevance
propagation. Furthermore, we apply this approach to the recently developed
Vision Transformer architecture and evaluate its performance against existing
methods on two image classification datasets, namely ImageNet and PascalVOC.
Our results clearly demonstrate the advantage of our proposed method.
Furthermore, we discuss the insufficiencies of current evaluation metrics for
attribution-based explainability and propose a new evaluation metric that
combines the notions of faithfulness, robustness and contrastiveness. We
utilize this new metric to evaluate the performance of various
attribution-based methods. Our code is available at:
https://github.com/davor10105/relative-absolute-magnitude-propagation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 16 figures, 13 tables, ACM Transactions on Intelligence
  Systems and Technology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GoHD: Gaze-oriented and Highly Disentangled Portrait Animation with
  Rhythmic Poses and Realistic Expression <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Zhou, Weize Quan, Hailin Shi, Wei Li, Lili Wang, Dong-ming Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-driven talking head generation necessitates seamless integration of
audio and visual data amidst the challenges posed by diverse input portraits
and intricate correlations between audio and facial motions. In response, we
propose a robust framework GoHD designed to produce highly realistic,
expressive, and controllable portrait videos from any reference identity with
any motion. GoHD innovates with three key modules: Firstly, an animation module
utilizing latent navigation is introduced to improve the generalization ability
across unseen input styles. This module achieves high disentanglement of motion
and identity, and it also incorporates gaze orientation to rectify unnatural
eye movements that were previously overlooked. Secondly, a conformer-structured
conditional diffusion model is designed to guarantee head poses that are aware
of prosody. Thirdly, to estimate lip-synchronized and realistic expressions
from the input audio within limited training data, a two-stage training
strategy is devised to decouple frequent and frame-wise lip motion distillation
from the generation of other more temporally dependent but less audio-related
motions, e.g., blinks and frowns. Extensive experiments validate GoHD's
advanced generalization capabilities, demonstrating its effectiveness in
generating realistic talking face results on arbitrary subjects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InstanceCap: Improving Text-to-Video Generation via Instance-aware
  Structured Caption 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiehan Fan, Kepan Nan, Rui Xie, Penghao Zhou, Zhenheng Yang, Chaoyou Fu, Xiang Li, Jian Yang, Ying Tai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-video generation has evolved rapidly in recent years, delivering
remarkable results. Training typically relies on video-caption paired data,
which plays a crucial role in enhancing generation performance. However,
current video captions often suffer from insufficient details, hallucinations
and imprecise motion depiction, affecting the fidelity and consistency of
generated videos. In this work, we propose a novel instance-aware structured
caption framework, termed InstanceCap, to achieve instance-level and
fine-grained video caption for the first time. Based on this scheme, we design
an auxiliary models cluster to convert original video into instances to enhance
instance fidelity. Video instances are further used to refine dense prompts
into structured phrases, achieving concise yet precise descriptions.
Furthermore, a 22K InstanceVid dataset is curated for training, and an
enhancement pipeline that tailored to InstanceCap structure is proposed for
inference. Experimental results demonstrate that our proposed InstanceCap
significantly outperform previous models, ensuring high fidelity between
captions and videos while reducing hallucinations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Multimodal Large Language Model with Pixel-Level Insight for
  Biomedicine <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoshuang Huang, Lingdong Shen, Jia Liu, Fangxin Shang, Hongxiang Li, Haifeng Huang, Yehui Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Multimodal Large Language Models (MLLM) have achieved
notable advancements, demonstrating the feasibility of developing an
intelligent biomedical assistant. However, current biomedical MLLMs
predominantly focus on image-level understanding and restrict interactions to
textual commands, thus limiting their capability boundaries and the flexibility
of usage. In this paper, we introduce a novel end-to-end multimodal large
language model for the biomedical domain, named MedPLIB, which possesses
pixel-level understanding. Excitingly, it supports visual question answering
(VQA), arbitrary pixel-level prompts (points, bounding boxes, and free-form
shapes), and pixel-level grounding. We propose a novel Mixture-of-Experts (MoE)
multi-stage training strategy, which divides MoE into separate training phases
for a visual-language expert model and a pixel-grounding expert model, followed
by fine-tuning using MoE. This strategy effectively coordinates multitask
learning while maintaining the computational cost at inference equivalent to
that of a single expert model. To advance the research of biomedical MLLMs, we
introduce the Medical Complex Vision Question Answering Dataset (MeCoVQA),
which comprises an array of 8 modalities for complex medical imaging question
answering and image region understanding. Experimental results indicate that
MedPLIB has achieved state-of-the-art outcomes across multiple medical visual
language tasks. More importantly, in zero-shot evaluations for the pixel
grounding task, MedPLIB leads the best small and large models by margins of
19.7 and 15.6 respectively on the mDice metric. The codes, data, and model
checkpoints will be made publicly available at
https://github.com/ShawnHuang497/MedPLIB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text-Video Multi-Grained Integration for Video Moment Montage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihui Yin, Ye Ma, Xipeng Cao, Bo Wang, Quan Chen, Peng Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of online short video platforms has driven a surge in user
demand for short video editing. However, manually selecting, cropping, and
assembling raw footage into a coherent, high-quality video remains laborious
and time-consuming. To accelerate this process, we focus on a user-friendly new
task called Video Moment Montage (VMM), which aims to accurately locate the
corresponding video segments based on a pre-provided narration text and then
arrange these video clips to create a complete video that aligns with the
corresponding descriptions. The challenge lies in extracting precise temporal
segments while ensuring intra-sentence and inter-sentence context consistency,
as a single script sentence may require trimming and assembling multiple video
clips. To address this problem, we present a novel \textit{Text-Video
Multi-Grained Integration} method (TV-MGI) that efficiently fuses text features
from the script with both shot-level and frame-level video features, which
enables the global and fine-grained alignment between the video content and the
corresponding textual descriptions in the script. To facilitate further
research in this area, we introduce the Multiple Sentences with Shots Dataset
(MSSD), a large-scale dataset designed explicitly for the VMM task. We conduct
extensive experiments on the MSSD dataset to demonstrate the effectiveness of
our framework compared to baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LatentSync: Audio Conditioned Latent Diffusion Models for Lip Sync 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunyu Li, Chao Zhang, Weikai Xu, Jinghui Xie, Weiguo Feng, Bingyue Peng, Weiwei Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LatentSync, an end-to-end lip sync framework based on audio
conditioned latent diffusion models without any intermediate motion
representation, diverging from previous diffusion-based lip sync methods based
on pixel space diffusion or two-stage generation. Our framework can leverage
the powerful capabilities of Stable Diffusion to directly model complex
audio-visual correlations. Additionally, we found that the diffusion-based lip
sync methods exhibit inferior temporal consistency due to the inconsistency in
the diffusion process across different frames. We propose Temporal
REPresentation Alignment (TREPA) to enhance temporal consistency while
preserving lip-sync accuracy. TREPA uses temporal representations extracted by
large-scale self-supervised video models to align the generated frames with the
ground truth frames. Furthermore, we observe the commonly encountered SyncNet
convergence issue and conduct comprehensive empirical studies, identifying key
factors affecting SyncNet convergence in terms of model architecture, training
hyperparameters, and data preprocessing methods. We significantly improve the
accuracy of SyncNet from 91% to 94% on the HDTF test set. Since we did not
change the overall training framework of SyncNet, our experience can also be
applied to other lip sync and audio-driven portrait animation methods that
utilize SyncNet. Based on the above innovations, our method outperforms
state-of-the-art lip sync methods across various metrics on the HDTF and
VoxCeleb2 datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FD2-Net: Frequency-Driven Feature Decomposition Network for
  Infrared-Visible Object Detection <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Li, Di Wang, Zhangyuan Hu, Shaofeng Li, Weiping Ni, Lin Zhao, Quan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Infrared-visible object detection (IVOD) seeks to harness the complementary
information in infrared and visible images, thereby enhancing the performance
of detectors in complex environments. However, existing methods often neglect
the frequency characteristics of complementary information, such as the
abundant high-frequency details in visible images and the valuable
low-frequency thermal information in infrared images, thus constraining
detection performance. To solve this problem, we introduce a novel
Frequency-Driven Feature Decomposition Network for IVOD, called FD2-Net, which
effectively captures the unique frequency representations of complementary
information across multimodal visual spaces. Specifically, we propose a feature
decomposition encoder, wherein the high-frequency unit (HFU) utilizes discrete
cosine transform to capture representative high-frequency features, while the
low-frequency unit (LFU) employs dynamic receptive fields to model the
multi-scale context of diverse objects. Next, we adopt a parameter-free
complementary strengths strategy to enhance multimodal features through
seamless inter-frequency recoupling. Furthermore, we innovatively design a
multimodal reconstruction mechanism that recovers image details lost during
feature extraction, further leveraging the complementary information from
infrared and visible images to enhance overall representational capacity.
Extensive experiments demonstrate that FD2-Net outperforms state-of-the-art
(SOTA) models across various IVOD benchmarks, i.e. LLVIP (96.2% mAP), FLIR
(82.9% mAP), and M3FD (83.5% mAP).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work is accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VLMs meet UDA: Boosting Transferability of Open Vocabulary Segmentation
  with Unsupervised Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roberto Alcover-Couso, Marcos Escudero-Viñolo, Juan C. SanMiguel, Jesus Bescos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmentation models are typically constrained by the categories defined
during training. To address this, researchers have explored two independent
approaches: adapting Vision-Language Models (VLMs) and leveraging synthetic
data. However, VLMs often struggle with granularity, failing to disentangle
fine-grained concepts, while synthetic data-based methods remain limited by the
scope of available datasets.
  This paper proposes enhancing segmentation accuracy across diverse domains by
integrating Vision-Language reasoning with key strategies for Unsupervised
Domain Adaptation (UDA). First, we improve the fine-grained segmentation
capabilities of VLMs through multi-scale contextual data, robust text
embeddings with prompt augmentation, and layer-wise fine-tuning in our proposed
Foundational-Retaining Open Vocabulary Semantic Segmentation (FROVSS)
framework. Next, we incorporate these enhancements into a UDA framework by
employing distillation to stabilize training and cross-domain mixed sampling to
boost adaptability without compromising generalization. The resulting
UDA-FROVSS framework is the first UDA approach to effectively adapt across
domains without requiring shared categories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Foundation Models and Adaptive Feature Selection: A Synergistic Approach
  to Video Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Bhargav Rongali, Mohamad Hassan N C, Ankit Jha, Neha Bhargava, Saurabh Prasad, Biplab Banerjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper tackles the intricate challenge of video question-answering
(VideoQA). Despite notable progress, current methods fall short of effectively
integrating questions with video frames and semantic object-level abstractions
to create question-aware video representations. We introduce Local-Global
Question Aware Video Embedding (LGQAVE), which incorporates three major
innovations to integrate multi-modal knowledge better and emphasize semantic
visual concepts relevant to specific questions. LGQAVE moves beyond traditional
ad-hoc frame sampling by utilizing a cross-attention mechanism that precisely
identifies the most relevant frames concerning the questions. It captures the
dynamics of objects within these frames using distinct graphs, grounding them
in question semantics with the miniGPT model. These graphs are processed by a
question-aware dynamic graph transformer (Q-DGT), which refines the outputs to
develop nuanced global and local video representations. An additional
cross-attention module integrates these local and global embeddings to generate
the final video embeddings, which a language model uses to generate answers.
Extensive evaluations across multiple benchmarks demonstrate that LGQAVE
significantly outperforms existing models in delivering accurate multi-choice
and open-ended answers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UADet: A Remarkably Simple Yet Effective Uncertainty-Aware Open-Set
  Object Detection Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silin Cheng, Yuanpei Liu, Kai Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle the challenging problem of Open-Set Object Detection (OSOD), which
aims to detect both known and unknown objects in unlabelled images. The main
difficulty arises from the absence of supervision for these unknown classes,
making it challenging to distinguish them from the background. Existing OSOD
detectors either fail to properly exploit or inadequately leverage the abundant
unlabeled unknown objects in training data, restricting their performance. To
address these limitations, we propose UADet, an Uncertainty-Aware Open-Set
Object Detector that considers appearance and geometric uncertainty. By
integrating these uncertainty measures, UADet effectively reduces the number of
unannotated instances incorrectly utilized or omitted by previous methods.
Extensive experiments on OSOD benchmarks demonstrate that UADet substantially
outperforms previous state-of-the-art (SOTA) methods in detecting both known
and unknown objects, achieving a 1.8x improvement in unknown recall while
maintaining high performance on known classes. When extended to Open World
Object Detection (OWOD), our method shows significant advantages over the
current SOTA method, with average improvements of 13.8% and 6.9% in unknown
recall on M-OWODB and S-OWODB benchmarks, respectively. Extensive results
validate the effectiveness of our uncertainty-aware approach across different
open-set scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DASK: Distribution Rehearsing via Adaptive Style Kernel Learning for
  Exemplar-Free Lifelong Person Re-Identification <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunlun Xu, Chenghao Jiang, Peixi Xiong, Yuxin Peng, Jiahuan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lifelong person re-identification (LReID) is an important but challenging
task that suffers from catastrophic forgetting due to significant domain gaps
between training steps. Existing LReID approaches typically rely on data replay
and knowledge distillation to mitigate this issue. However, data replay methods
compromise data privacy by storing historical exemplars, while knowledge
distillation methods suffer from limited performance due to the cumulative
forgetting of undistilled knowledge. To overcome these challenges, we propose a
novel paradigm that models and rehearses the distribution of the old domains to
enhance knowledge consolidation during the new data learning, possessing a
strong anti-forgetting capacity without storing any exemplars. Specifically, we
introduce an exemplar-free LReID method called Distribution Rehearsing via
Adaptive Style Kernel Learning (DASK). DASK includes a Distribution Rehearser
Learning mechanism that learns to transform arbitrary distribution data into
the current data style at each learning step. To enhance the style transfer
capacity of DRL, an Adaptive Kernel Prediction network is explored to achieve
an instance-specific distribution adjustment. Additionally, we design a
Distribution Rehearsing-driven LReID Training module, which rehearses old
distribution based on the new data via the old AKPNet model, achieving
effective new-old knowledge accumulation under a joint knowledge consolidation
scheme. Experimental results show our DASK outperforms the existing methods by
3.6%-6.8% and 4.5%-6.5% on anti-forgetting and generalization capacity,
respectively. Our code is available at
https://github.com/zhoujiahuan1991/AAAI2025-DASK
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in Proceedings of the 39th AAAI Conference on Artificial Intelligence
  (AAAI-25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ USDRL: Unified Skeleton-Based Dense Representation Learning with
  Multi-Grained Feature Decorrelation <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09220v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09220v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanjiang Weng, Hongsong Wang, Junbo He, Lei He, Guosen Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has achieved great success in skeleton-based
representation learning recently. However, the prevailing methods are
predominantly negative-based, necessitating additional momentum encoder and
memory bank to get negative samples, which increases the difficulty of model
training. Furthermore, these methods primarily concentrate on learning a global
representation for recognition and retrieval tasks, while overlooking the rich
and detailed local representations that are crucial for dense prediction tasks.
To alleviate these issues, we introduce a Unified Skeleton-based Dense
Representation Learning framework based on feature decorrelation, called USDRL,
which employs feature decorrelation across temporal, spatial, and instance
domains in a multi-grained manner to reduce redundancy among dimensions of the
representations to maximize information extraction from features. Additionally,
we design a Dense Spatio-Temporal Encoder (DSTE) to capture fine-grained action
representations effectively, thereby enhancing the performance of dense
prediction tasks. Comprehensive experiments, conducted on the benchmarks
NTU-60, NTU-120, PKU-MMD I, and PKU-MMD II, across diverse downstream tasks
including action recognition, action retrieval, and action detection,
conclusively demonstrate that our approach significantly outperforms the
current state-of-the-art (SOTA) approaches. Our code and models are available
at https://github.com/wengwanjiang/USDRL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Implicit Neural Representations via Symmetric Power
  Transformation <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixiang Zhang, Shuzhao Xie, Chengwei Ren, Shijia Ge, Mingzi Wang, Zhi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose symmetric power transformation to enhance the capacity of Implicit
Neural Representation~(INR) from the perspective of data transformation. Unlike
prior work utilizing random permutation or index rearrangement, our method
features a reversible operation that does not require additional storage
consumption. Specifically, we first investigate the characteristics of data
that can benefit the training of INR, proposing the Range-Defined Symmetric
Hypothesis, which posits that specific range and symmetry can improve the
expressive ability of INR. Based on this hypothesis, we propose a nonlinear
symmetric power transformation to achieve both range-defined and symmetric
properties simultaneously. We use the power coefficient to redistribute data to
approximate symmetry within the target range. To improve the robustness of the
transformation, we further design deviation-aware calibration and adaptive soft
boundary to address issues of extreme deviation boosting and continuity
breaking. Extensive experiments are conducted to verify the performance of the
proposed method, demonstrating that our transformation can reliably improve INR
compared with other data transformations. We also conduct 1D audio, 2D image
and 3D video fitting tasks to demonstrate the effectiveness and applicability
of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ eCARLA-scenes: A synthetically generated <span class="highlight-title">dataset</span> for event-based optical
  flow prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jad Mansour, Hayat Rajani, Rafael Garcia, Nuno Gracias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The joint use of event-based vision and Spiking Neural Networks (SNNs) is
expected to have a large impact in robotics in the near future, in tasks such
as, visual odometry and obstacle avoidance. While researchers have used
real-world event datasets for optical flow prediction (mostly captured with
Unmanned Aerial Vehicles (UAVs)), these datasets are limited in diversity,
scalability, and are challenging to collect. Thus, synthetic datasets offer a
scalable alternative by bridging the gap between reality and simulation. In
this work, we address the lack of datasets by introducing eWiz, a comprehensive
library for processing event-based data. It includes tools for data loading,
augmentation, visualization, encoding, and generation of training data, along
with loss functions and performance metrics. We further present a synthetic
event-based datasets and data generation pipelines for optical flow prediction
tasks. Built on top of eWiz, eCARLA-scenes makes use of the CARLA simulator to
simulate self-driving car scenarios. The ultimate goal of this dataset is the
depiction of diverse environments while laying a foundation for advancing
event-based camera applications in autonomous field vehicle navigation, paving
the way for using SNNs on neuromorphic hardware such as the Intel Loihi.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Action Localization with Cross Layer Task Decoupling and
  Refinement <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Li, Di Liu, Jun Kong, Sen Li, Hui Xu, Jianzhong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal action localization (TAL) involves dual tasks to classify and
localize actions within untrimmed videos. However, the two tasks often have
conflicting requirements for features. Existing methods typically employ
separate heads for classification and localization tasks but share the same
input feature, leading to suboptimal performance. To address this issue, we
propose a novel TAL method with Cross Layer Task Decoupling and Refinement
(CLTDR). Based on the feature pyramid of video, CLTDR strategy integrates
semantically strong features from higher pyramid layers and detailed
boundary-aware boundary features from lower pyramid layers to effectively
disentangle the action classification and localization tasks. Moreover, the
multiple features from cross layers are also employed to refine and align the
disentangled classification and regression results. At last, a lightweight
Gated Multi-Granularity (GMG) module is proposed to comprehensively extract and
aggregate video features at instant, local, and global temporal granularities.
Benefiting from the CLTDR and GMG modules, our method achieves state-of-the-art
performance on five challenging benchmarks: THUMOS14, MultiTHUMOS,
EPIC-KITCHENS-100, ActivityNet-1.3, and HACS. Our code and pre-trained models
are publicly available at: https://github.com/LiQiang0307/CLTDR-GMG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accuracy Improvements for Convolutional and Differential Distance
  Function Approximations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Belyaev, Pierre-Alain Fayolle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a bounded domain, we deal with the problem of estimating the distance
function from the internal points of the domain to the boundary of the domain.
Convolutional and differential distance estimation schemes are considered and,
for both the schemes, accuracy improvements are proposed and evaluated.
Asymptotics of Laplace integrals and Taylor series extrapolations are used to
achieve the improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MVC-VPR: Mutual Learning of Viewpoint Classification and Visual Place
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiwen Gu, Xufei Wang, Fenglin Zhang, Junqiao Zhao, Siyue Tao, Chen Ye, Tiantian Feng, Changjun Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Place Recognition (VPR) aims to robustly identify locations by
leveraging image retrieval based on descriptors encoded from environmental
images. However, drastic appearance changes of images captured from different
viewpoints at the same location pose incoherent supervision signals for
descriptor learning, which severely hinder the performance of VPR. Previous
work proposes classifying images based on manually defined rules or ground
truth labels for viewpoints, followed by descriptor training based on the
classification results. However, not all datasets have ground truth labels of
viewpoints and manually defined rules may be suboptimal, leading to degraded
descriptor performance.To address these challenges, we introduce the mutual
learning of viewpoint self-classification and VPR. Starting from coarse
classification based on geographical coordinates, we progress to finer
classification of viewpoints using simple clustering techniques. The dataset is
partitioned in an unsupervised manner while simultaneously training a
descriptor extractor for place recognition. Experimental results show that this
approach almost perfectly partitions the dataset based on viewpoints, thus
achieving mutually reinforcing effects. Our method even excels state-of-the-art
(SOTA) methods that partition datasets using ground truth labels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ExpRDiff: Short-exposure Guided Diffusion Model for Realistic Local
  Motion Deblurring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongbao Yang, Jiangxin Dong, Jinhui Tang, Jinshan Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Removing blur caused by moving objects is challenging, as the moving objects
are usually significantly blurry while the static background remains clear.
Existing methods that rely on local blur detection often suffer from
inaccuracies and cannot generate satisfactory results when focusing solely on
blurred regions. To overcome these problems, we first design a context-based
local blur detection module that incorporates additional contextual information
to improve the identification of blurry regions. Considering that modern
smartphones are equipped with cameras capable of providing short-exposure
images, we develop a blur-aware guided image restoration method that utilizes
sharp structural details from short-exposure images, facilitating accurate
reconstruction of heavily blurred regions. Furthermore, to restore images
realistically and visually-pleasant, we develop a short-exposure guided
diffusion model that explores useful features from short-exposure images and
blurred regions to better constrain the diffusion process. Finally, we
formulate the above components into a simple yet effective network, named
ExpRDiff. Experimental results show that ExpRDiff performs favorably against
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://github.com/yzb1997/ExpRDiff</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAD: Region-Aware Diffusion Models for Image Inpainting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sora Kim, Sungho Suh, Minsik Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have achieved remarkable success in image generation, with
applications broadening across various domains. Inpainting is one such
application that can benefit significantly from diffusion models. Existing
methods either hijack the reverse process of a pretrained diffusion model or
cast the problem into a larger framework, \ie, conditioned generation. However,
these approaches often require nested loops in the generation process or
additional components for conditioning. In this paper, we present region-aware
diffusion models (RAD) for inpainting with a simple yet effective reformulation
of the vanilla diffusion models. RAD utilizes a different noise schedule for
each pixel, which allows local regions to be generated asynchronously while
considering the global image context. A plain reverse process requires no
additional components, enabling RAD to achieve inference time up to 100 times
faster than the state-of-the-art approaches. Moreover, we employ low-rank
adaptation (LoRA) to fine-tune RAD based on other pretrained diffusion models,
reducing computational burdens in training as well. Experiments demonstrated
that RAD provides state-of-the-art results both qualitatively and
quantitatively, on the FFHQ, LSUN Bedroom, and ImageNet datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the effectiveness of Rotation-Equivariance in U-Net: A Benchmark for
  Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robin Ghyselinck, Valentin Delchevalerie, Bruno Dumas, Benoît Frénay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous studies have recently focused on incorporating different variations
of equivariance in Convolutional Neural Networks (CNNs). In particular,
rotation-equivariance has gathered significant attention due to its relevance
in many applications related to medical imaging, microscopic imaging, satellite
imaging, industrial tasks, etc. While prior research has primarily focused on
enhancing classification tasks with rotation equivariant CNNs, their impact on
more complex architectures, such as U-Net for image segmentation, remains
scarcely explored. Indeed, previous work interested in integrating
rotation-equivariance into U-Net architecture have focused on solving specific
applications with a limited scope. In contrast, this paper aims to provide a
more exhaustive evaluation of rotation equivariant U-Net for image segmentation
across a broader range of tasks. We benchmark their effectiveness against
standard U-Net architectures, assessing improvements in terms of performance
and sustainability (i.e., computational cost). Our evaluation focuses on
datasets whose orientation of objects of interest is arbitrary in the image
(e.g., Kvasir-SEG), but also on more standard segmentation datasets (such as
COCO-Stuff) as to explore the wider applicability of rotation equivariance
beyond tasks undoubtedly concerned by rotation equivariance. The main
contribution of this work is to provide insights into the trade-offs and
advantages of integrating rotation equivariance for segmentation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weighted Poisson-disk Resampling on Large-Scale Point Clouds <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianhe Jiao, Chenlei Lv, Junli Zhao, Ran Yi, Yu-Hui Wen, Zhenkuan Pan, Zhongke Wu, Yong-jin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For large-scale point cloud processing, resampling takes the important role
of controlling the point number and density while keeping the geometric
consistency. % in related tasks. However, current methods cannot balance such
different requirements. Particularly with large-scale point clouds, classical
methods often struggle with decreased efficiency and accuracy. To address such
issues, we propose a weighted Poisson-disk (WPD) resampling method to improve
the usability and efficiency for the processing. We first design an initial
Poisson resampling with a voxel-based estimation strategy. It is able to
estimate a more accurate radius of the Poisson-disk while maintaining high
efficiency. Then, we design a weighted tangent smoothing step to further
optimize the Voronoi diagram for each point. At the same time, sharp features
are detected and kept in the optimized results with isotropic property.
Finally, we achieve a resampling copy from the original point cloud with the
specified point number, uniform density, and high-quality geometric
consistency. Experiments show that our method significantly improves the
performance of large-scale point cloud resampling for different applications,
and provides a highly practical solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DECOR:Decomposition and Projection of Text Embeddings for Text-to-Image
  Customization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geonhui Jang, Jin-Hwa Kim, Yong-Hyun Park, Junho Kim, Gayoung Lee, Yonghyun Jeong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image (T2I) models can effectively capture the content or style of
reference images to perform high-quality customization. A representative
technique for this is fine-tuning using low-rank adaptations (LoRA), which
enables efficient model customization with reference images. However,
fine-tuning with a limited number of reference images often leads to
overfitting, resulting in issues such as prompt misalignment or content
leakage. These issues prevent the model from accurately following the input
prompt or generating undesired objects during inference. To address this
problem, we examine the text embeddings that guide the diffusion model during
inference. This study decomposes the text embedding matrix and conducts a
component analysis to understand the embedding space geometry and identify the
cause of overfitting. Based on this, we propose DECOR, which projects text
embeddings onto a vector space orthogonal to undesired token vectors, thereby
reducing the influence of unwanted semantics in the text embeddings.
Experimental results demonstrate that DECOR outperforms state-of-the-art
customization models and achieves Pareto frontier performance across text and
visual alignment evaluation metrics. Furthermore, it generates images more
faithful to the input prompts, showcasing its effectiveness in addressing
overfitting and enhancing text-to-image customization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ YingSound: Video-Guided Sound Effects Generation with Multi-modal
  Chain-of-Thought Controls 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09168v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09168v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Chen, Haomin Zhang, Xinhan Di, Haoyu Wang, Sizhe Shan, Junjie Zheng, Yunming Liang, Yihan Fan, Xinfa Zhu, Wenjie Tian, Yihua Wang, Chaofan Ding, Lei Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating sound effects for product-level videos, where only a small amount
of labeled data is available for diverse scenes, requires the production of
high-quality sounds in few-shot settings. To tackle the challenge of limited
labeled data in real-world scenes, we introduce YingSound, a foundation model
designed for video-guided sound generation that supports high-quality audio
generation in few-shot settings. Specifically, YingSound consists of two major
modules. The first module uses a conditional flow matching transformer to
achieve effective semantic alignment in sound generation across audio and
visual modalities. This module aims to build a learnable audio-visual
aggregator (AVA) that integrates high-resolution visual features with
corresponding audio features at multiple stages. The second module is developed
with a proposed multi-modal visual-audio chain-of-thought (CoT) approach to
generate finer sound effects in few-shot settings. Finally, an
industry-standard video-to-audio (V2A) dataset that encompasses various
real-world scenarios is presented. We show that YingSound effectively generates
high-quality synchronized sounds across diverse conditional inputs through
automated evaluations and human studies. Project Page:
\url{https://giantailab.github.io/yingsound/}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pinpoint Counterfactuals: Reducing social bias in foundation models via
  localized counterfactual generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kirill Sirotkin, Marcos Escudero-Viñolo, Pablo Carballeira, Mayug Maniparambil, Catarina Barata, Noel E. O'Connor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models trained on web-scraped datasets propagate societal biases
to downstream tasks. While counterfactual generation enables bias analysis,
existing methods introduce artifacts by modifying contextual elements like
clothing and background. We present a localized counterfactual generation
method that preserves image context by constraining counterfactual
modifications to specific attribute-relevant regions through automated masking
and guided inpainting. When applied to the Conceptual Captions dataset for
creating gender counterfactuals, our method results in higher visual and
semantic fidelity than state-of-the-art alternatives, while maintaining the
performance of models trained using only real data on non-human-centric tasks.
Models fine-tuned with our counterfactuals demonstrate measurable bias
reduction across multiple metrics, including a decrease in gender
classification disparity and balanced person preference scores, while
preserving ImageNet zero-shot performance. The results establish a framework
for creating balanced datasets that enable both accurate bias profiling and
effective mitigation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Adversarial Attacks on Traffic Sign Classifiers beyond
  Standard Baselines <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Svetlana Pavlitska, Leopold Müller, J. Marius Zöllner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks on traffic sign classification models were among the
first successfully tried in the real world. Since then, the research in this
area has been mainly restricted to repeating baseline models, such as LISA-CNN
or GTSRB-CNN, and similar experiment settings, including white and black
patches on traffic signs. In this work, we decouple model architectures from
the datasets and evaluate on further generic models to make a fair comparison.
Furthermore, we compare two attack settings, inconspicuous and visible, which
are usually regarded without direct comparison. Our results show that standard
baselines like LISA-CNN or GTSRB-CNN are significantly more susceptible than
the generic ones. We, therefore, suggest evaluating new attacks on a broader
spectrum of baselines in the future. Our code is available at
\url{https://github.com/KASTEL-MobilityLab/attacks-on-traffic-sign-recognition/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at ICMLA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LVMark: Robust Watermark for latent video diffusion models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        MinHyuk Jang, Youngdong Jang, JaeHyeok Lee, Kodai Kawamura, Feng Yang, Sangpil Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid advancements in generative models have made it possible to create
hyper-realistic videos. As their applicability increases, their unauthorized
use has raised significant concerns, leading to the growing demand for
techniques to protect the ownership of the generative model itself. While
existing watermarking methods effectively embed watermarks into
image-generative models, they fail to account for temporal information,
resulting in poor performance when applied to video-generative models. To
address this issue, we introduce a novel watermarking method called LVMark,
which embeds watermarks into video diffusion models. A key component of LVMark
is a selective weight modulation strategy that efficiently embeds watermark
messages into the video diffusion model while preserving the quality of the
generated videos. To accurately decode messages in the presence of malicious
attacks, we design a watermark decoder that leverages spatio-temporal
information in the 3D wavelet domain through a cross-attention module. To the
best of our knowledge, our approach is the first to highlight the potential of
video-generative model watermarking as a valuable tool for enhancing the
effectiveness of ownership protection in video-generative models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision CNNs trained to estimate spatial latents learned similar
  ventral-stream-aligned representations <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09115v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09115v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yudi Xie, Weichen Huang, Esther Alter, Jeremy Schwartz, Joshua B. Tenenbaum, James J. DiCarlo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Studies of the functional role of the primate ventral visual stream have
traditionally focused on object categorization, often ignoring -- despite much
prior evidence -- its role in estimating "spatial" latents such as object
position and pose. Most leading ventral stream models are derived by optimizing
networks for object categorization, which seems to imply that the ventral
stream is also derived under such an objective. Here, we explore an alternative
hypothesis: Might the ventral stream be optimized for estimating spatial
latents? And a closely related question: How different -- if at all -- are
representations learned from spatial latent estimation compared to
categorization? To ask these questions, we leveraged synthetic image datasets
generated by a 3D graphic engine and trained convolutional neural networks
(CNNs) to estimate different combinations of spatial and category latents. We
found that models trained to estimate just a few spatial latents achieve neural
alignment scores comparable to those trained on hundreds of categories, and the
spatial latent performance of models strongly correlates with their neural
alignment. Spatial latent and category-trained models have very similar -- but
not identical -- internal representations, especially in their early and middle
layers. We provide evidence that this convergence is partly driven by
non-target latent variability in the training data, which facilitates the
implicit learning of representations of those non-target latents. Taken
together, these results suggest that many training objectives, such as spatial
latents, can lead to similar models aligned neurally with the ventral stream.
Thus, one should not assume that the ventral stream is optimized for object
categorization only. As a field, we need to continue to sharpen our measures of
comparing models to brains to better understand the functional roles of the
ventral stream.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 20 figures, ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ResFlow: Fine-tuning Residual Optical Flow for Event-based High Temporal
  Resolution Motion Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianang Zhou, Zhiyu Zhu, Junhui Hou, Yongjian Deng, Youfu Li, Junlin Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras hold significant promise for high-temporal-resolution (HTR)
motion estimation. However, estimating event-based HTR optical flow faces two
key challenges: the absence of HTR ground-truth data and the intrinsic sparsity
of event data. Most existing approaches rely on the flow accumulation paradigms
to indirectly supervise intermediate flows, often resulting in accumulation
errors and optimization difficulties. To address these challenges, we propose a
residual-based paradigm for estimating HTR optical flow with event data. Our
approach separates HTR flow estimation into two stages: global linear motion
estimation and HTR residual flow refinement. The residual paradigm effectively
mitigates the impacts of event sparsity on optimization and is compatible with
any LTR algorithm. Next, to address the challenge posed by the absence of HTR
ground truth, we incorporate novel learning strategies. Specifically, we
initially employ a shared refiner to estimate the residual flows, enabling both
LTR supervision and HTR inference. Subsequently, we introduce regional noise to
simulate the residual patterns of intermediate flows, facilitating the
adaptation from LTR supervision to HTR inference. Additionally, we show that
the noise-based strategy supports in-domain self-supervised training.
Comprehensive experimental results demonstrate that our approach achieves
state-of-the-art accuracy in both LTR and HTR metrics, highlighting its
effectiveness and superiority.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Long-Horizon Vision-Language Navigation: Platform, Benchmark and
  Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinshuai Song, Weixing Chen, Yang Liu, Weikai Chen, Guanbin Li, Liang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Vision-Language Navigation (VLN) methods primarily focus on
single-stage navigation, limiting their effectiveness in multi-stage and
long-horizon tasks within complex and dynamic environments. To address these
limitations, we propose a novel VLN task, named Long-Horizon Vision-Language
Navigation (LH-VLN), which emphasizes long-term planning and decision
consistency across consecutive subtasks. Furthermore, to support LH-VLN, we
develop an automated data generation platform NavGen, which constructs datasets
with complex task structures and improves data utility through a bidirectional,
multi-granularity generation approach. To accurately evaluate complex tasks, we
construct the Long-Horizon Planning and Reasoning in VLN (LHPR-VLN) benchmark
consisting of 3,260 tasks with an average of 150 task steps, serving as the
first dataset specifically designed for the long-horizon vision-language
navigation task. Furthermore, we propose Independent Success Rate (ISR),
Conditional Success Rate (CSR), and CSR weight by Ground Truth (CGT) metrics,
to provide fine-grained assessments of task completion. To improve model
adaptability in complex tasks, we propose a novel Multi-Granularity Dynamic
Memory (MGDM) module that integrates short-term memory blurring with long-term
memory retrieval to enable flexible navigation in dynamic environments. Our
platform, benchmark and method supply LH-VLN with a robust data generation
pipeline, comprehensive model evaluation dataset, reasonable metrics, and a
novel VLN model, establishing a foundational framework for advancing LH-VLN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A novel Vision-Language Navigation task: Long-Horizon Vision-Language
  Navigation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DomCLP: Domain-wise Contrastive Learning with Prototype Mixup for
  Unsupervised Domain Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin-Seop Lee, Noo-ri Kim, Jee-Hyong Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) methods based on the instance discrimination
tasks with InfoNCE have achieved remarkable success. Despite their success, SSL
models often struggle to generate effective representations for unseen-domain
data. To address this issue, research on unsupervised domain generalization
(UDG), which aims to develop SSL models that can generate domain-irrelevant
features, has been conducted. Most UDG approaches utilize contrastive learning
with InfoNCE to generate representations, and perform feature alignment based
on strong assumptions to generalize domain-irrelevant common features from
multi-source domains. However, existing methods that rely on instance
discrimination tasks are not effective at extracting domain-irrelevant common
features. This leads to the suppression of domain-irrelevant common features
and the amplification of domain-relevant features, thereby hindering domain
generalization. Furthermore, strong assumptions underlying feature alignment
can lead to biased feature learning, reducing the diversity of common features.
In this paper, we propose a novel approach, DomCLP, Domain-wise Contrastive
Learning with Prototype Mixup. We explore how InfoNCE suppresses
domain-irrelevant common features and amplifies domain-relevant features. Based
on this analysis, we propose Domain-wise Contrastive Learning (DCon) to enhance
domain-irrelevant common features. We also propose Prototype Mixup Learning
(PMix) to generalize domain-irrelevant common features across multiple domains
without relying on strong assumptions. The proposed method consistently
outperforms state-of-the-art methods on the PACS and DomainNet datasets across
various label fractions, showing significant improvements. Our code will be
released. Our project page is available at https://github.com/jinsuby/DomCLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code page: https://github.com/jinsuby/DomCLP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SVasP: Self-Versatility Adversarial Style Perturbation for Cross-Domain
  Few-Shot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqian Li, Pengfei Fang, Hui Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-Domain Few-Shot Learning (CD-FSL) aims to transfer knowledge from seen
source domains to unseen target domains, which is crucial for evaluating the
generalization and robustness of models. Recent studies focus on utilizing
visual styles to bridge the domain gap between different domains. However, the
serious dilemma of gradient instability and local optimization problem occurs
in those style-based CD-FSL methods. This paper addresses these issues and
proposes a novel crop-global style perturbation method, called
\underline{\textbf{S}}elf-\underline{\textbf{V}}ersatility
\underline{\textbf{A}}dversarial \underline{\textbf{S}}tyle
\underline{\textbf{P}}erturbation (\textbf{SVasP}), which enhances the gradient
stability and escapes from poor sharp minima jointly. Specifically, SVasP
simulates more diverse potential target domain adversarial styles via
diversifying input patterns and aggregating localized crop style gradients, to
serve as global style perturbation stabilizers within one image, a concept we
refer to as self-versatility. Then a novel objective function is proposed to
maximize visual discrepancy while maintaining semantic consistency between
global, crop, and adversarial features. Having the stabilized global style
perturbation in the training phase, one can obtain a flattened minima in the
loss landscape, boosting the transferability of the model to the target
domains. Extensive experiments on multiple benchmark datasets demonstrate that
our method significantly outperforms existing state-of-the-art methods. Our
codes are available at https://github.com/liwenqianSEU/SVasP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-View Completion Models are Zero-shot Correspondence Estimators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Honggyu An, Jinhyeon Kim, Seonghoon Park, Jaewoo Jung, Jisang Han, Sunghwan Hong, Seungryong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we explore new perspectives on cross-view completion learning
by drawing an analogy to self-supervised correspondence learning. Through our
analysis, we demonstrate that the cross-attention map within cross-view
completion models captures correspondence more effectively than other
correlations derived from encoder or decoder features. We verify the
effectiveness of the cross-attention map by evaluating on both zero-shot
matching and learning-based geometric matching and multi-frame depth
estimation. Project page is available at https://cvlab-kaist.github.io/ZeroCo/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://cvlab-kaist.github.io/ZeroCo/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Efficient Framework for Enhancing Discriminative Models via Diffusion
  Techniques <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunxiao Li, Xiaoxiao Wang, Boming Miao, Chuanlong Xie, Zizhe Wang, Yao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image classification serves as the cornerstone of computer vision,
traditionally achieved through discriminative models based on deep neural
networks. Recent advancements have introduced classification methods derived
from generative models, which offer the advantage of zero-shot classification.
However, these methods suffer from two main drawbacks: high computational
overhead and inferior performance compared to discriminative models. Inspired
by the coordinated cognitive processes of rapid-slow pathway interactions in
the human brain during visual signal recognition, we propose the
Diffusion-Based Discriminative Model Enhancement Framework (DBMEF). This
framework seamlessly integrates discriminative and generative models in a
training-free manner, leveraging discriminative models for initial predictions
and endowing deep neural networks with rethinking capabilities via diffusion
models. Consequently, DBMEF can effectively enhance the classification accuracy
and generalization capability of discriminative models in a plug-and-play
manner. We have conducted extensive experiments across 17 prevalent deep model
architectures with different training methods, including both CNN-based models
such as ResNet and Transformer-based models like ViT, to demonstrate the
effectiveness of the proposed DBMEF. Specifically, the framework yields a
1.51\% performance improvement for ResNet-50 on the ImageNet dataset and 3.02\%
on the ImageNet-A dataset. In conclusion, our research introduces a novel
paradigm for image classification, demonstrating stable improvements across
different datasets and neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyperbolic-constraint Point Cloud Reconstruction from Single RGB-D
  Images <span class="chip">AAAI25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenrui Li, Zhe Yang, Wei Han, Hengyu Man, Xingtao Wang, Xiaopeng Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing desired objects and scenes has long been a primary goal in 3D
computer vision. Single-view point cloud reconstruction has become a popular
technique due to its low cost and accurate results. However, single-view
reconstruction methods often rely on expensive CAD models and complex geometric
priors. Effectively utilizing prior knowledge about the data remains a
challenge. In this paper, we introduce hyperbolic space to 3D point cloud
reconstruction, enabling the model to represent and understand complex
hierarchical structures in point clouds with low distortion. We build upon
previous methods by proposing a hyperbolic Chamfer distance and a regularized
triplet loss to enhance the relationship between partial and complete point
clouds. Additionally, we design adaptive boundary conditions to improve the
model's understanding and reconstruction of 3D structures. Our model
outperforms most existing models, and ablation studies demonstrate the
significance of our model and its components. Experimental results show that
our method significantly improves feature extraction capabilities. Our model
achieves outstanding performance in 3D reconstruction tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ContextHOI: Spatial Context Learning for Human-Object Interaction
  Detection <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingda Jia, Liming Zhao, Ge Li, Yun Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial contexts, such as the backgrounds and surroundings, are considered
critical in Human-Object Interaction (HOI) recognition, especially when the
instance-centric foreground is blurred or occluded. Recent advancements in HOI
detectors are usually built upon detection transformer pipelines. While such an
object-detection-oriented paradigm shows promise in localizing objects, its
exploration of spatial context is often insufficient for accurately recognizing
human actions. To enhance the capabilities of object detectors for HOI
detection, we present a dual-branch framework named ContextHOI, which
efficiently captures both object detection features and spatial contexts. In
the context branch, we train the model to extract informative spatial context
without requiring additional hand-craft background labels. Furthermore, we
introduce context-aware spatial and semantic supervision to the context branch
to filter out irrelevant noise and capture informative contexts. ContextHOI
achieves state-of-the-art performance on the HICO-DET and v-coco benchmarks.
For further validation, we construct a novel benchmark, HICO-ambiguous, which
is a subset of HICO-DET that contains images with occluded or impaired instance
cues. Extensive experiments across all benchmarks, complemented by
visualizations, underscore the enhancements provided by ContextHOI, especially
in recognizing interactions involving occluded or blurred instances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in proceedings of the 39th AAAI Conference on Artificial Intelligence
  (AAAI-25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Motif Guided Graph <span class="highlight-title">Transformer</span> with Combinatorial Skeleton Prototype
  Learning for Skeleton-Based Person Re-Identification <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haocong Rao, Chunyan Miao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Person re-identification (re-ID) via 3D skeleton data is a challenging task
with significant value in many scenarios. Existing skeleton-based methods
typically assume virtual motion relations between all joints, and adopt average
joint or sequence representations for learning. However, they rarely explore
key body structure and motion such as gait to focus on more important body
joints or limbs, while lacking the ability to fully mine valuable
spatial-temporal sub-patterns of skeletons to enhance model learning. This
paper presents a generic Motif guided graph transformer with Combinatorial
skeleton prototype learning (MoCos) that exploits structure-specific and
gait-related body relations as well as combinatorial features of skeleton
graphs to learn effective skeleton representations for person re-ID. In
particular, motivated by the locality within joints' structure and the
body-component collaboration in gait, we first propose the motif guided graph
transformer (MGT) that incorporates hierarchical structural motifs and gait
collaborative motifs, which simultaneously focuses on multi-order local joint
correlations and key cooperative body parts to enhance skeleton relation
learning. Then, we devise the combinatorial skeleton prototype learning (CSP)
that leverages random spatial-temporal combinations of joint nodes and skeleton
graphs to generate diverse sub-skeleton and sub-tracklet representations, which
are contrasted with the most representative features (prototypes) of each
identity to learn class-related semantics and discriminative skeleton
representations. Extensive experiments validate the superior performance of
MoCos over existing state-of-the-art models. We further show its generality
under RGB-estimated skeletons, different graph modeling, and unsupervised
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025. Codes are available at
  https://github.com/Kali-Hac/MoCos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DrivingRecon: Large 4D Gaussian Reconstruction Model For Autonomous
  Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Lu, Tianshuo Xu, Wenzhao Zheng, Yunpeng Zhang, Wei Zhan, Dalong Du, Masayoshi Tomizuka, Kurt Keutzer, Yingcong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photorealistic 4D reconstruction of street scenes is essential for developing
real-world simulators in autonomous driving. However, most existing methods
perform this task offline and rely on time-consuming iterative processes,
limiting their practical applications. To this end, we introduce the Large 4D
Gaussian Reconstruction Model (DrivingRecon), a generalizable driving scene
reconstruction model, which directly predicts 4D Gaussian from surround view
videos. To better integrate the surround-view images, the Prune and Dilate
Block (PD-Block) is proposed to eliminate overlapping Gaussian points between
adjacent views and remove redundant background points. To enhance
cross-temporal information, dynamic and static decoupling is tailored to better
learn geometry and motion features. Experimental results demonstrate that
DrivingRecon significantly improves scene reconstruction quality and novel view
synthesis compared to existing methods. Furthermore, we explore applications of
DrivingRecon in model pre-training, vehicle adaptation, and scene editing. Our
code is available at https://github.com/EnVision-Research/DriveRecon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLAVIDAL: A Large LAnguage VIsion Model for Daily Activities of Living 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09390v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09390v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominick Reilly, Rajatsubhra Chakraborty, Arkaprava Sinha, Manish Kumar Govind, Pu Wang, Francois Bremond, Le Xue, Srijan Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current Large Language Vision Models (LLVMs) trained on web videos perform
well in general video understanding but struggle with fine-grained details,
complex human-object interactions (HOI), and view-invariant representation
learning essential for Activities of Daily Living (ADL). This limitation stems
from a lack of specialized ADL video instruction-tuning datasets and
insufficient modality integration to capture discriminative action
representations. To address this, we propose a semi-automated framework for
curating ADL datasets, creating ADL-X, a multiview, multimodal RGBS
instruction-tuning dataset. Additionally, we introduce LLAVIDAL, an LLVM
integrating videos, 3D skeletons, and HOIs to model ADL's complex
spatiotemporal relationships. For training LLAVIDAL a simple joint alignment of
all modalities yields suboptimal results; thus, we propose a Multimodal
Progressive (MMPro) training strategy, incorporating modalities in stages
following a curriculum. We also establish ADL MCQ and video description
benchmarks to assess LLVM performance in ADL tasks. Trained on ADL-X, LLAVIDAL
achieves state-of-the-art performance across ADL benchmarks. Code and data will
be made publicly available at: https://adl-x.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Localizing Memorization in SSL Vision Encoders <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19069v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19069v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Wang, Adam Dziedzic, Michael Backes, Franziska Boenisch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work on studying memorization in self-supervised learning (SSL)
suggests that even though SSL encoders are trained on millions of images, they
still memorize individual data points. While effort has been put into
characterizing the memorized data and linking encoder memorization to
downstream utility, little is known about where the memorization happens inside
SSL encoders. To close this gap, we propose two metrics for localizing
memorization in SSL encoders on a per-layer (layermem) and per-unit basis
(unitmem). Our localization methods are independent of the downstream task, do
not require any label information, and can be performed in a forward pass. By
localizing memorization in various encoder architectures (convolutional and
transformer-based) trained on diverse datasets with contrastive and
non-contrastive SSL frameworks, we find that (1) while SSL memorization
increases with layer depth, highly memorizing units are distributed across the
entire encoder, (2) a significant fraction of units in SSL encoders experiences
surprisingly high memorization of individual data points, which is in contrast
to models trained under supervision, (3) atypical (or outlier) data points
cause much higher layer and unit memorization than standard data points, and
(4) in vision transformers, most memorization happens in the fully-connected
layers. Finally, we show that localizing memorization in SSL has the potential
to improve fine-tuning and to inform pruning strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Flow Fields in Attention for Controllable Person Image
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08486v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08486v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Zhou, Shikun Liu, Xiao Han, Haozhe Liu, Kam Woh Ng, Tian Xie, Yuren Cong, Hang Li, Mengmeng Xu, Juan-Manuel Pérez-Rúa, Aditya Patel, Tao Xiang, Miaojing Shi, Sen He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controllable person image generation aims to generate a person image
conditioned on reference images, allowing precise control over the person's
appearance or pose. However, prior methods often distort fine-grained textural
details from the reference image, despite achieving high overall image quality.
We attribute these distortions to inadequate attention to corresponding regions
in the reference image. To address this, we thereby propose learning flow
fields in attention (Leffa), which explicitly guides the target query to attend
to the correct reference key in the attention layer during training.
Specifically, it is realized via a regularization loss on top of the attention
map within a diffusion-based baseline. Our extensive experiments show that
Leffa achieves state-of-the-art performance in controlling appearance (virtual
try-on) and pose (pose transfer), significantly reducing fine-grained detail
distortion while maintaining high image quality. Additionally, we show that our
loss is model-agnostic and can be used to improve the performance of other
diffusion models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>github: https://github.com/franciszzj/Leffa, demo:
  https://huggingface.co/spaces/franciszzj/Leffa, model:
  https://huggingface.co/franciszzj/Leffa</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Medical Foundation Model Features in Graph Neural
  Network-Based Retrieval of Breast Histopathology Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.04211v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.04211v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nematollah Saeidi, Hossein Karshenas, Bijan Shoushtarian, Sepideh Hatamikia, Ramona Woitek, Amirreza Mahbod
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Breast cancer is the most common cancer type in women worldwide. Early
detection and appropriate treatment can significantly reduce its impact. While
histopathology examinations play a vital role in rapid and accurate diagnosis,
they often require experienced medical experts for proper recognition and
cancer grading. Automated image retrieval systems have the potential to assist
pathologists in identifying cancerous tissues, thereby accelerating the
diagnostic process. Nevertheless, proposing an accurate image retrieval model
is challenging due to considerable variability among the tissue and cell
patterns in histological images. In this work, we leverage the features from
foundation models in a novel attention-based adversarially regularized
variational graph autoencoder model for breast histological image retrieval.
Our results confirm the superior performance of models trained with foundation
model features compared to those using pre-trained convolutional neural
networks (up to 7.7% and 15.5% for mAP and mMV, respectively), with the
pre-trained general-purpose self-supervised model for computational pathology
(UNI) delivering the best overall performance. By evaluating two publicly
available histology image datasets of breast cancer, our top-performing model,
trained with UNI features, achieved average mAP/mMV scores of 96.7%/91.5% and
97.6%/94.2% for the BreakHis and BACH datasets, respectively. Our proposed
retrieval model has the potential to be used in clinical settings to enhance
diagnostic performance and ultimately benefit patients.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Altogether: Image Captioning via Re-aligning Alt-text <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hu Xu, Po-Yao Huang, Xiaoqing Ellen Tan, Ching-Feng Yeh, Jacob Kahn, Christine Jou, Gargi Ghosh, Omer Levy, Luke Zettlemoyer, Wen-tau Yih, Shang-Wen Li, Saining Xie, Christoph Feichtenhofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on creating synthetic data to improve the quality of image
captions. Existing works typically have two shortcomings. First, they caption
images from scratch, ignoring existing alt-text metadata, and second, lack
transparency if the captioners' training data (e.g. GPT) is unknown. In this
paper, we study a principled approach Altogether based on the key idea to edit
and re-align existing alt-texts associated with the images. To generate
training data, we perform human annotation where annotators start with the
existing alt-text and re-align it to the image content in multiple rounds,
consequently constructing captions with rich visual concepts. This differs from
prior work that carries out human annotation as a one-time description task
solely based on images and annotator knowledge. We train a captioner on this
data that generalizes the process of re-aligning alt-texts at scale. Our
results show our Altogether approach leads to richer image captions that also
improve text-to-image generation and zero-shot image classification tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by EMNLP 2024; Meta CLIP 1.2 Data Engine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disentangling Mean Embeddings for Better Diagnostics of Image Generators <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01314v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01314v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian G. Gruber, Pascal Tobias Ziegler, Florian Buettner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of image generators remains a challenge due to the limitations
of traditional metrics in providing nuanced insights into specific image
regions. This is a critical problem as not all regions of an image may be
learned with similar ease. In this work, we propose a novel approach to
disentangle the cosine similarity of mean embeddings into the product of cosine
similarities for individual pixel clusters via central kernel alignment.
Consequently, we can quantify the contribution of the cluster-wise performance
to the overall image generation performance. We demonstrate how this enhances
the explainability and the likelihood of identifying pixel regions of model
misbehavior across various real-world use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at Interpretable AI: Past, Present and Future Workshop at
  NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Liquid: Language Models are Scalable Multi-modal Generators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04332v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04332v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfeng Wu, Yi Jiang, Chuofan Ma, Yuliang Liu, Hengshuang Zhao, Zehuan Yuan, Song Bai, Xiang Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Liquid, an auto-regressive generation paradigm that seamlessly
integrates visual comprehension and generation by tokenizing images into
discrete codes and learning these code embeddings alongside text tokens within
a shared feature space for both vision and language. Unlike previous multimodal
large language model (MLLM), Liquid achieves this integration using a single
large language model (LLM), eliminating the need for external pretrained visual
embeddings such as CLIP. For the first time, Liquid uncovers a scaling law that
performance drop unavoidably brought by the unified training of visual and
language tasks diminishes as the model size increases. Furthermore, the unified
token space enables visual generation and comprehension tasks to mutually
enhance each other, effectively removing the typical interference seen in
earlier models. We show that existing LLMs can serve as strong foundations for
Liquid, saving 100x in training costs while outperforming Chameleon in
multimodal capabilities and maintaining language performance comparable to
mainstream LLMs like LLAMA2. Liquid also outperforms models like SD v2.1 and
SD-XL (FID of 5.47 on MJHQ-30K), excelling in both vision-language and
text-only tasks. This work demonstrates that LLMs such as LLAMA3.2 and GEMMA2
are powerful multimodal generators, offering a scalable solution for enhancing
both vision-language understanding and generation. The code and models will be
released at https://github.com/FoundationVision/Liquid.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report. Project page:
  https://github.com/FoundationVision/Liquid</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hyperspectral Imaging-Based Perception in Autonomous Driving Scenarios:
  Benchmarking Baseline Semantic Segmentation Models <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22101v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22101v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Imad Ali Shah, Jiarong Li, Martin Glavin, Edward Jones, Enda Ward, Brian Deegan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral Imaging (HSI) is known for its advantages over traditional RGB
imaging in remote sensing, agriculture, and medicine. Recently, it has gained
attention for enhancing Advanced Driving Assistance Systems (ADAS) perception.
Several HSI datasets such as HyKo, HSI-Drive, HSI-Road, and Hyperspectral City
have been made available. However, a comprehensive evaluation of semantic
segmentation models (SSM) using these datasets is lacking. To address this gap,
we evaluated the available annotated HSI datasets on four deep learning-based
baseline SSMs: DeepLab v3+, HRNet, PSPNet, and U-Net, along with its two
variants: Coordinate Attention (UNet-CA) and Convolutional Block-Attention
Module (UNet-CBAM). The original model architectures were adapted to handle the
varying spatial and spectral dimensions of the datasets. These baseline SSMs
were trained using a class-weighted loss function for individual HSI datasets
and evaluated using mean-based metrics such as intersection over union (IoU),
recall, precision, F1 score, specificity, and accuracy. Our results indicate
that UNet-CBAM, which extracts channel-wise features, outperforms other SSMs
and shows potential to leverage spectral information for enhanced semantic
segmentation. This study establishes a baseline SSM benchmark on available
annotated datasets for future evaluation of HSI-based ADAS perception. However,
limitations of current HSI datasets, such as limited dataset size, high class
imbalance, and lack of fine-grained annotations, remain significant constraints
for developing robust SSMs for ADAS applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted and Presented at IEEE WHISPERS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distribution-Level Feature Distancing for Machine Unlearning: Towards a
  Better Trade-off Between Model Utility and Forgetting <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14747v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14747v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dasol Choi, Dongbin Na
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the explosive growth of deep learning applications and increasing
privacy concerns, the right to be forgotten has become a critical requirement
in various AI industries. For example, given a facial recognition system, some
individuals may wish to remove their personal data that might have been used in
the training phase. Unfortunately, deep neural networks sometimes unexpectedly
leak personal identities, making this removal challenging. While recent machine
unlearning algorithms aim to enable models to forget specific data, we identify
an unintended utility drop-correlation collapse-in which the essential
correlations between image features and true labels weaken during the
forgetting process. To address this challenge, we propose Distribution-Level
Feature Distancing (DLFD), a novel method that efficiently forgets instances
while preserving task-relevant feature correlations. Our method synthesizes
data samples by optimizing the feature distribution to be distinctly different
from that of forget samples, achieving effective results within a single
training epoch. Through extensive experiments on facial recognition datasets,
we demonstrate that our approach significantly outperforms state-of-the-art
machine unlearning methods in both forgetting performance and model utility
preservation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures, AAAI 2025 camera ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EVQAScore: Efficient Video Question Answering Data Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06908v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06908v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Liang, Zirong Chen, Wentao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video question-answering (QA) is a core task in video understanding.
Evaluating the quality of video QA and video caption data quality for training
video large language models (VideoLLMs) is an essential challenge. Although
various methods have been proposed for assessing video caption quality, there
remains a lack of dedicated evaluation methods for Video QA. To address this
gap, we introduce EVQAScore, a reference-free method that leverages keyword
extraction to assess both video caption and video QA data quality.
Additionally, we incorporate frame sampling and rescaling techniques to enhance
the efficiency and robustness of our evaluation, this enables our score to
evaluate the quality of extremely long videos. Our approach achieves
state-of-the-art (SOTA) performance (32.8 for Kendall correlation and 42.3 for
Spearman correlation, 4.7 and 5.9 higher than the previous method PAC-S++) on
the VATEX-EVAL benchmark for video caption evaluation. Furthermore, by using
EVQAScore for data selection, we achieved SOTA results with only 12.5\% of the
original data volume, outperforming the previous SOTA method PAC-S and 100\% of
data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Robustness of Kolmogorov-Arnold Networks: An Adversarial
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13809v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13809v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tal Alter, Raz Lapid, Moshe Sipper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kolmogorov-Arnold Networks (KANs) have recently emerged as a novel approach
to function approximation, demonstrating remarkable potential in various
domains. Despite their theoretical promise, the robustness of KANs under
adversarial conditions has yet to be thoroughly examined. In this paper we
explore the adversarial robustness of KANs, with a particular focus on image
classification tasks. We assess the performance of KANs against standard white
box and black-box adversarial attacks, comparing their resilience to that of
established neural network architectures. Our experimental evaluation
encompasses a variety of standard image classification benchmark datasets and
investigates both fully connected and convolutional neural network
architectures, of three sizes: small, medium, and large. We conclude that
small- and medium-sized KANs (either fully connected or convolutional) are not
consistently more robust than their standard counterparts, but that large-sized
KANs are, by and large, more robust. This comprehensive evaluation of KANs in
adversarial scenarios offers the first in-depth analysis of KAN security,
laying the groundwork for future research in this emerging field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video Summarization using Denoising Diffusion Probabilistic Model <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08357v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08357v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zirui Shang, Yubo Zhu, Hongxi Li, Shuo Yang, Xinxiao Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video summarization aims to eliminate visual redundancy while retaining key
parts of video to construct concise and comprehensive synopses. Most existing
methods use discriminative models to predict the importance scores of video
frames. However, these methods are susceptible to annotation inconsistency
caused by the inherent subjectivity of different annotators when annotating the
same video. In this paper, we introduce a generative framework for video
summarization that learns how to generate summaries from a probability
distribution perspective, effectively reducing the interference of subjective
annotation noise. Specifically, we propose a novel diffusion summarization
method based on the Denoising Diffusion Probabilistic Model (DDPM), which
learns the probability distribution of training data through noise prediction,
and generates summaries by iterative denoising. Our method is more resistant to
subjective annotation noise, and is less prone to overfitting the training data
than discriminative methods, with strong generalization ability. Moreover, to
facilitate training DDPM with limited data, we employ an unsupervised video
summarization model to implement the earlier denoising process. Extensive
experiments on various datasets (TVSum, SumMe, and FPVSum) demonstrate the
effectiveness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DualPM: Dual Posed-Canonical Point Maps for 3D Shape and Pose
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04464v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04464v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Kaye, Tomas Jakab, Shangzhe Wu, Christian Rupprecht, Andrea Vedaldi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The choice of data representation is a key factor in the success of deep
learning in geometric tasks. For instance, DUSt3R has recently introduced the
concept of viewpoint-invariant point maps, generalizing depth prediction, and
showing that one can reduce all the key problems in the 3D reconstruction of
static scenes to predicting such point maps. In this paper, we develop an
analogous concept for a very different problem, namely, the reconstruction of
the 3D shape and pose of deformable objects. To this end, we introduce the Dual
Point Maps (DualPM), where a pair of point maps is extracted from the same
image, one associating pixels to their 3D locations on the object, and the
other to a canonical version of the object at rest pose. We also extend point
maps to amodal reconstruction, seeing through self-occlusions to obtain the
complete shape of the object. We show that 3D reconstruction and 3D pose
estimation reduce to the prediction of the DualPMs. We demonstrate empirically
that this representation is a good target for a deep network to predict;
specifically, we consider modeling horses, showing that DualPMs can be trained
purely on 3D synthetic data, consisting of a single model of a horse, while
generalizing very well to real images. With this, we improve by a large margin
previous methods for the 3D analysis and reconstruction of this type of
objects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First two authors contributed equally. Project page:
  https://dualpm.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GaussianOcc: Fully <span class="highlight-title">Self-supervised</span> and Efficient 3D Occupancy Estimation
  with Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11447v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11447v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanshui Gan, Fang Liu, Hongbin Xu, Ningkai Mo, Naoto Yokoya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce GaussianOcc, a systematic method that investigates the two
usages of Gaussian splatting for fully self-supervised and efficient 3D
occupancy estimation in surround views. First, traditional methods for
self-supervised 3D occupancy estimation still require ground truth 6D poses
from sensors during training. To address this limitation, we propose Gaussian
Splatting for Projection (GSP) module to provide accurate scale information for
fully self-supervised training from adjacent view projection. Additionally,
existing methods rely on volume rendering for final 3D voxel representation
learning using 2D signals (depth maps, semantic maps), which is both
time-consuming and less effective. We propose Gaussian Splatting from Voxel
space (GSV) to leverage the fast rendering properties of Gaussian splatting. As
a result, the proposed GaussianOcc method enables fully self-supervised (no
ground truth pose) 3D occupancy estimation in competitive performance with low
computational cost (2.7 times faster in training and 5 times faster in
rendering). The relevant code is available in
https://github.com/GANWANSHUI/GaussianOcc.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://ganwanshui.github.io/GaussianOcc/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fire Dynamic Vision: Image Segmentation and Tracking for Multi-Scale
  Fire and Plume Behavior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08984v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08984v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daryn Sagel, Bryan Quaife
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing frequency and severity of wildfires highlight the need for
accurate fire and plume spread models. We introduce an approach that
effectively isolates and tracks fire and plume behavior across various spatial
and temporal scales and image types, identifying physical phenomena in the
system and providing insights useful for developing and validating models. Our
method combines image segmentation and graph theory to delineate fire fronts
and plume boundaries. We demonstrate that the method effectively distinguishes
fires and plumes from visually similar objects. Results demonstrate the
successful isolation and tracking of fire and plume dynamics across various
image sources, ranging from synoptic-scale ($10^4$-$10^5$ m) satellite images
to sub-microscale ($10^0$-$10^1$ m) images captured close to the fire
environment. Furthermore, the methodology leverages image inpainting and
spatio-temporal dataset generation for use in statistical and machine learning
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perturb and Recover: Fine-tuning for Effective Backdoor Removal from
  CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naman Deep Singh, Francesco Croce, Matthias Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language models like CLIP have been shown to be highly effective at
linking visual perception and natural language understanding, enabling
sophisticated image-text capabilities, including strong retrieval and zero-shot
classification performance. Their widespread use, as well as the fact that CLIP
models are trained on image-text pairs from the web, make them both a
worthwhile and relatively easy target for backdoor attacks. As training
foundational models, such as CLIP, from scratch is very expensive, this paper
focuses on cleaning potentially poisoned models via fine-tuning. We first show
that existing cleaning techniques are not effective against simple structured
triggers used in Blended or BadNet backdoor attacks, exposing a critical
vulnerability for potential real-world deployment of these models. Then, we
introduce PAR, Perturb and Recover, a surprisingly simple yet effective
mechanism to remove backdoors from CLIP models. Through extensive experiments
across different encoders and types of backdoor attacks, we show that PAR
achieves high backdoor removal rate while preserving good standard performance.
Finally, we illustrate that our approach is effective even only with synthetic
text-image pairs, i.e. without access to real training data. The code and
models are available at https://github.com/nmndeep/PerturbAndRecover.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Visual Generative Priors without Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuailei Ma, Kecheng Zheng, Ying Wei, Wei Wu, Fan Lu, Yifei Zhang, Chen-Wei Xie, Biao Gong, Jiapeng Zhu, Yujun Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although text-to-image (T2I) models have recently thrived as visual
generative priors, their reliance on high-quality text-image pairs makes
scaling up expensive. We argue that grasping the cross-modality alignment is
not a necessity for a sound visual generative prior, whose focus should be on
texture modeling. Such a philosophy inspires us to study image-to-image (I2I)
generation, where models can learn from in-the-wild images in a self-supervised
manner. We first develop a pure vision-based training framework, Lumos, and
confirm the feasibility and the scalability of learning I2I models. We then
find that, as an upstream task of T2I, our I2I model serves as a more
foundational visual prior and achieves on-par or better performance than
existing T2I models using only 1/10 text-image pairs for fine-tuning. We
further demonstrate the superiority of I2I priors over T2I priors on some
text-irrelevant visual generative tasks, like image-to-3D and image-to-video.
Our project page is available at https://xiaomabufei.github.io/lumos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://xiaomabufei.github.io/lumos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MM-Tracker: Motion Mamba with Margin Loss for UAV-platform Multiple
  Object Tracking <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10485v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10485v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mufeng Yao, Jinlong Peng, Qingdong He, Bo Peng, Hao Chen, Mingmin Chi, Chao Liu, Jon Atli Benediktsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple object tracking (MOT) from unmanned aerial vehicle (UAV) platforms
requires efficient motion modeling. This is because UAV-MOT faces both local
object motion and global camera motion. Motion blur also increases the
difficulty of detecting large moving objects. Previous UAV motion modeling
approaches either focus only on local motion or ignore motion blurring effects,
thus limiting their tracking performance and speed. To address these issues, we
propose the Motion Mamba Module, which explores both local and global motion
features through cross-correlation and bi-directional Mamba Modules for better
motion modeling. To address the detection difficulties caused by motion blur,
we also design motion margin loss to effectively improve the detection accuracy
of motion blurred objects. Based on the Motion Mamba module and motion margin
loss, our proposed MM-Tracker surpasses the state-of-the-art in two widely
open-source UAV-MOT datasets. Code will be available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multi-Stage Framework for Joint Chest X-Ray Diagnosis and Visual
  Attention Prediction Using Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16970v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16970v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zirui Qiu, Hassan Rivaz, Yiming Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: As visual inspection is an inherent process during radiological
screening, the associated eye gaze data can provide valuable insights into
relevant clinical decisions. As deep learning has become the state-of-the-art
for computer-assisted diagnosis, integrating human behavior, such as eye gaze
data, into these systems is instrumental to help align machine predictions with
clinical diagnostic criteria, thus enhancing the quality of automatic
radiological diagnosis. Methods: We propose a novel deep learning framework for
joint disease diagnosis and prediction of corresponding clinical visual
attention maps for chest X-ray scans. Specifically, we introduce a new
dual-encoder multi-task UNet, which leverages both a DenseNet201 backbone and a
Residual and Squeeze-and-Excitation block-based encoder to extract diverse
features for visual attention map prediction, and a multi-scale feature-fusion
classifier to perform disease classification. To tackle the issue of
asynchronous training schedules of individual tasks in multi-task learning, we
proposed a multi-stage cooperative learning strategy, with contrastive learning
for feature encoder pretraining to boost performance. Results: Our proposed
method is shown to significantly outperform existing techniques for chest X-ray
diagnosis (AUC=0.93) and the quality of visual attention map prediction
(Correlation coefficient=0.58). Conclusion: Benefiting from the proposed
multi-task multi-stage cooperative learning, our technique demonstrates the
benefit of integrating clinicians' eye gaze into clinical AI systems to boost
performance and potentially explainability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of Artificial Intelligence in Gait-Based Neurodegenerative
  Disease Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13082v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13082v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haocong Rao, Minlin Zeng, Xuejiao Zhao, Chunyan Miao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed an increasing global population affected by
neurodegenerative diseases (NDs), which traditionally require extensive
healthcare resources and human effort for medical diagnosis and monitoring. As
a crucial disease-related motor symptom, human gait can be exploited to
characterize different NDs. The current advances in artificial intelligence
(AI) models enable automatic gait analysis for NDs identification and
classification, opening a new avenue to facilitate faster and more
cost-effective diagnosis of NDs. In this paper, we provide a comprehensive
survey on recent progress of machine learning and deep learning based AI
techniques applied to diagnosis of five typical NDs through gait. We provide an
overview of the process of AI-assisted NDs diagnosis, and present a systematic
taxonomy of existing gait data and AI models. Meanwhile, a novel quality
evaluation criterion is proposed to quantitatively assess the quality of
existing studies. Through an extensive review and analysis of 169 studies, we
present recent technical advancements, discuss existing challenges, potential
solutions, and future directions in this field. Finally, we envision the
prospective utilization of 3D skeleton data for human gait representation and
the development of more efficient AI models for NDs diagnosis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Article: 57 pages, citing 290 papers. Appendix: 30 pages. A
  up-to-date resource (papers, data, etc.) of this survey (AI4NDD) is provided
  at https://github.com/minlinzeng/AI4NDD-Survey</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Humans as Checkerboards: Calibrating Camera Motion Scale for
  World-Coordinate Human Mesh Recovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengyuan Yang, Kerui Gu, Ha Linh Nguyen, Tze Ho Elden Tse, Angela Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate camera motion estimation is essential for recovering global human
motion in world coordinates from RGB video inputs. SLAM is widely used for
estimating camera trajectory and point cloud, but monocular SLAM does so only
up to an unknown scale factor. Previous works estimate the scale factor through
optimization, but this is unreliable and time-consuming. This paper presents an
optimization-free scale calibration framework, Human as Checkerboard (HAC). HAC
innovatively leverages the human body predicted by human mesh recovery model as
a calibration reference. Specifically, it uses the absolute depth of
human-scene contact joints as references to calibrate the corresponding
relative scene depth from SLAM. HAC benefits from geometric priors encoded in
human mesh recovery models to estimate the SLAM scale and achieves precise
global human motion estimation. Simple yet powerful, our method sets a new
state-of-the-art performance for global human mesh estimation tasks, reducing
motion errors by 50% over prior local-to-global methods while using 100$\times$
less inference time than optimization-based methods. Project page:
https://martayang.github.io/HAC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 11 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving generative adversarial network inversion via fine-tuning GAN
  encoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.10201v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.10201v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Yu, Wenmin Wang, Roberto Bugiolacchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative adversarial networks (GANs) can synthesize high-quality (HQ)
images, and GAN inversion is a technique that discovers how to invert given
images back to latent space. While existing methods perform on StyleGAN
inversion, they have limited performance and are not generalized to different
GANs. To address these issues, we proposed a self-supervised method to
pre-train and fine-tune GAN encoders. First, we designed an adaptive block to
fit different encoder architectures for inverting diverse GANs. Then we
pre-train GAN encoders using synthesized images and emphasize local regions
through cropping images. Finally, we fine-tune the pre-trained GAN encoder for
inverting real images. Compared with state-of-the-art methods, our method
achieved better results that reconstructed high-quality images on mainstream
GANs. Our code and pre-trained models are available at:
https://github.com/disanda/Deep-GAN-Encoders.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08926v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08926v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Virmarie Maquiling, Sean Anthony Byrne, Diederick C. Niehorster, Marco Carminati, Enkelejda Kasneci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the transformative potential of SAM 2, a vision foundation model,
in advancing gaze estimation and eye tracking technologies. By significantly
reducing annotation time, lowering technical barriers through its ease of
deployment, and enhancing segmentation accuracy, SAM 2 addresses critical
challenges faced by researchers and practitioners. Utilizing its zero-shot
segmentation capabilities with minimal user input-a single click per video-we
tested SAM 2 on over 14 million eye images from diverse datasets, including
virtual reality setups and the world's largest unified dataset recorded using
wearable eye trackers. Remarkably, in pupil segmentation tasks, SAM 2 matches
the performance of domain-specific models trained solely on eye images,
achieving competitive mean Intersection over Union (mIoU) scores of up to 93%
without fine-tuning. Additionally, we provide our code and segmentation masks
for these widely used datasets to promote further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Virmarie Maquiling and Sean Anthony Byrne contributed equally to this
  paper, 8 pages, 3 figures, CHI Case Study, pre-print</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive Multi-scale Approach for Speech and Dynamics Synchrony
  in Talking Head Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Louis Airale, Dominique Vaufreydaz, Xavier Alameda-Pineda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Animating still face images with deep generative models using a speech input
signal is an active research topic and has seen important recent
progress.However, much of the effort has been put into lip syncing and
rendering quality while the generation of natural head motion, let alone the
audio-visual correlation between head motion and speech, has often been
neglected.In this work, we propose a multi-scale audio-visual synchrony loss
and a multi-scale autoregressive GAN to better handle short and long-term
correlation between speech and the dynamics of the head and lips.In particular,
we train a stack of syncer models on multimodal input pyramids and use these
models as guidance in a multi-scale generator network to produce audio-aligned
motion unfolding over diverse time scales.Both the pyramid of audio-visual
syncers and the generative models are trained in a low-dimensional space that
fully preserves dynamics cues.The experiments show significant improvements
over the state-of-the-art in head motion dynamics quality and especially in
multi-scale audio-visual synchrony on a collection of benchmark datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CAS-GAN for Contrast-free Angiography Synthesis <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        De-Xing Huang, Xiao-Hu Zhou, Mei-Jiang Gui, Xiao-Liang Xie, Shi-Qi Liu, Shuang-Yi Wang, Hao Li, Tian-Yu Xiang, Zeng-Guang Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Iodinated contrast agents are widely utilized in numerous interventional
procedures, yet posing substantial health risks to patients. This paper
presents CAS-GAN, a novel GAN framework that serves as a "virtual contrast
agent" to synthesize X-ray angiographies via disentanglement representation
learning and vessel semantic guidance, thereby reducing the reliance on
iodinated contrast agents during interventional procedures. Specifically, our
approach disentangles X-ray angiographies into background and vessel
components, leveraging medical prior knowledge. A specialized predictor then
learns to map the interrelationships between these components. Additionally, a
vessel semantic-guided generator and a corresponding loss function are
introduced to enhance the visual fidelity of generated images. Experimental
results on the XCAD dataset demonstrate the state-of-the-art performance of our
CAS-GAN, achieving a FID of 5.87 and a MMD of 0.016. These promising results
highlight {\tt CAS-GAN}'s potential for clinical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Symposium Series on Computational Intelligence (SSCI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Riemann-based Multi-scale Attention Reasoning Network for Text-3D
  Retrieval <span class="chip">AAAI25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13712v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13712v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenrui Li, Wei Han, Yandu Chen, Yeyu Chai, Yidan Lu, Xingtao Wang, Xiaopeng Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the challenges in acquiring paired Text-3D data and the inherent
irregularity of 3D data structures, combined representation learning of 3D
point clouds and text remains unexplored. In this paper, we propose a novel
Riemann-based Multi-scale Attention Reasoning Network (RMARN) for text-3D
retrieval. Specifically, the extracted text and point cloud features are
refined by their respective Adaptive Feature Refiner (AFR). Furthermore, we
introduce the innovative Riemann Local Similarity (RLS) module and the Global
Pooling Similarity (GPS) module. However, as 3D point cloud data and text data
often possess complex geometric structures in high-dimensional space, the
proposed RLS employs a novel Riemann Attention Mechanism to reflect the
intrinsic geometric relationships of the data. Without explicitly defining the
manifold, RMARN learns the manifold parameters to better represent the
distances between text-point cloud samples. To address the challenges of
lacking paired text-3D data, we have created the large-scale Text-3D Retrieval
dataset T3DR-HIT, which comprises over 3,380 pairs of text and point cloud
data. T3DR-HIT contains coarse-grained indoor 3D scenes and fine-grained
Chinese artifact scenes, consisting of 1,380 and over 2,000 text-3D pairs,
respectively. Experiments on our custom datasets demonstrate the superior
performance of the proposed method. Our code and proposed datasets are
available at \url{https://github.com/liwrui/RMARN}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Game4Loc: A UAV Geo-Localization Benchmark from Game Data <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16925v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16925v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Ji, Boyong He, Zhuoyue Tan, Liaoni Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The vision-based geo-localization technology for UAV, serving as a secondary
source of GPS information in addition to the global navigation satellite
systems (GNSS), can still operate independently in the GPS-denied environment.
Recent deep learning based methods attribute this as the task of image matching
and retrieval. By retrieving drone-view images in geo-tagged satellite image
database, approximate localization information can be obtained. However, due to
high costs and privacy concerns, it is usually difficult to obtain large
quantities of drone-view images from a continuous area. Existing drone-view
datasets are mostly composed of small-scale aerial photography with a strong
assumption that there exists a perfect one-to-one aligned reference image for
any query, leaving a significant gap from the practical localization scenario.
In this work, we construct a large-range contiguous area UAV geo-localization
dataset named GTA-UAV, featuring multiple flight altitudes, attitudes, scenes,
and targets using modern computer games. Based on this dataset, we introduce a
more practical UAV geo-localization task including partial matches of
cross-view paired data, and expand the image-level retrieval to the actual
localization in terms of distance (meters). For the construction of drone-view
and satellite-view pairs, we adopt a weight-based contrastive learning
approach, which allows for effective learning while avoiding additional
post-processing matching steps. Experiments demonstrate the effectiveness of
our data and training method for UAV geo-localization, as well as the
generalization capabilities to real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025, Project page: https://yux1angji.github.io/game4loc/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimized 3D Point Labeling with Leaders Using the Beams Displacement
  Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09552v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09552v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Wei, Nai Yang, Wenjia Xu, Su Ding, Li Minmin, Li You, Guo Renzhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In three-dimensional geographical scenes, adding labels with leader lines to
point features can significantly improve their visibility. Leadered labels have
a large degree of freedom in position con-figuration, but existing methods are
mostly based on limited position candidate models, which not only fail to
effectively utilize the map space but also make it difficult to consider the
relative relationships between labels. Therefore, we conceptualize the dynamic
configuration process of computing label positions as akin to solving a map
displacement problem. We use a triangulated graph to delineate spatial
relationships among labels and calculate the forces exerted on labels
considering the constraints associated with point feature labels. Then we use
the Beams Displacement Method to iteratively calculate new positions for the
labels. Our experimental outcomes demonstrate that this method effectively
mitigates label overlay issues while maintaining minimal average directional
deviation between adjacent labels. Furthermore, this method is adaptable to
various types of leader line labels. Meanwhile, we also discuss the block
processing strategy to improve the efficiency of label configuration and
analyze the impact of different proximity graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, in Chinese language, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Golden Noise for Diffusion Models: A Learning Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09502v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09502v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zikai Zhou, Shitong Shao, Lichen Bai, Zhiqiang Xu, Bo Han, Zeke Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion model is a popular paradigm that synthesizes
personalized images by providing a text prompt and a random Gaussian noise.
While people observe that some noises are ``golden noises'' that can achieve
better text-image alignment and higher human preference than others, we still
lack a machine learning framework to obtain those golden noises. To learn
golden noises for diffusion sampling, we mainly make three contributions in
this paper. First, we identify a new concept termed the \textit{noise prompt},
which aims at turning a random Gaussian noise into a golden noise by adding a
small desirable perturbation derived from the text prompt. Following the
concept, we first formulate the \textit{noise prompt learning} framework that
systematically learns ``prompted'' golden noise associated with a text prompt
for diffusion models. Second, we design a noise prompt data collection pipeline
and collect a large-scale \textit{noise prompt dataset}~(NPD) that contains
100k pairs of random noises and golden noises with the associated text prompts.
With the prepared NPD as the training dataset, we trained a small \textit{noise
prompt network}~(NPNet) that can directly learn to transform a random noise
into a golden noise. The learned golden noise perturbation can be considered as
a kind of prompt for noise, as it is rich in semantic information and tailored
to the given text prompt. Third, our extensive experiments demonstrate the
impressive effectiveness and generalization of NPNet on improving the quality
of synthesized images across various diffusion models, including SDXL,
DreamShaper-xl-v2-turbo, and Hunyuan-DiT. Moreover, NPNet is a small and
efficient controller that acts as a plug-and-play module with very limited
additional inference and computational costs, as it just provides a golden
noise instead of a random noise without accessing the original pipeline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HieraFashDiff: Hierarchical Fashion Design with Multi-stage Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07450v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07450v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhifeng Xie, Hao Li, Huiming Ding, Mengtian Li, Xinhan Di, Ying Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fashion design is a challenging and complex process.Recent works on fashion
generation and editing are all agnostic of the actual fashion design process,
which limits their usage in practice.In this paper, we propose a novel
hierarchical diffusion-based framework tailored for fashion design, coined as
HieraFashDiff. Our model is designed to mimic the practical fashion design
workflow, by unraveling the denosing process into two successive stages: 1) an
ideation stage that generates design proposals given high-level concepts and 2)
an iteration stage that continuously refines the proposals using low-level
attributes. Our model supports fashion design generation and fine-grained local
editing in a single framework. To train our model, we contribute a new dataset
of full-body fashion images annotated with hierarchical text descriptions.
Extensive evaluations show that, as compared to prior approaches, our method
can generate fashion designs and edited results with higher fidelity and better
prompt adherence, showing its promising potential to augment the practical
fashion design workflow. Code and Dataset are available at
https://github.com/haoli-zbdbc/hierafashdiff.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Veri-Car: Towards Open-world Vehicle Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06864v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06864v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrés Muñoz, Nancy Thomas, Annita Vapsi, Daniel Borrajo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many industrial and service sectors require tools to extract vehicle
characteristics from images. This is a complex task not only by the variety of
noise, and large number of classes, but also by the constant introduction of
new vehicle models to the market. In this paper, we present Veri-Car, an
information retrieval integrated approach designed to help on this task. It
leverages supervised learning techniques to accurately identify the make, type,
model, year, color, and license plate of cars. The approach also addresses the
challenge of handling open-world problems, where new car models and variations
frequently emerge, by employing a sophisticated combination of pre-trained
models, and a hierarchical multi-similarity loss. Veri-Car demonstrates robust
performance, achieving high precision and accuracy in classifying both seen and
unseen data. Additionally, it integrates an ensemble license plate detection,
and an OCR model to extract license plate numbers with impressive accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced Low-Dose CT Image Reconstruction by Domain and Task Shifting
  Gaussian Denoisers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03551v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03551v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Selig, Thomas März, Martin Storath, Andreas Weinmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computed tomography from a low radiation dose (LDCT) is challenging due to
high noise in the projection data. Popular approaches for LDCT image
reconstruction are two-stage methods, typically consisting of the filtered
backprojection (FBP) algorithm followed by a neural network for LDCT image
enhancement. Two-stage methods are attractive for their simplicity and
potential for computational efficiency, typically requiring only a single FBP
and a neural network forward pass for inference. However, the best
reconstruction quality is currently achieved by unrolled iterative methods
(Learned Primal-Dual and ItNet), which are more complex and thus have a higher
computational cost for training and inference. We propose a method combining
the simplicity and efficiency of two-stage methods with state-of-the-art
reconstruction quality. Our strategy utilizes a neural network pretrained for
Gaussian noise removal from natural grayscale images, fine-tuned for LDCT image
enhancement. We call this method FBP-DTSGD (Domain and Task Shifted Gaussian
Denoisers) as the fine-tuning is a task shift from Gaussian denoising to
enhancing LDCT images and a domain shift from natural grayscale to LDCT images.
An ablation study with three different pretrained Gaussian denoisers indicates
that the performance of FBP-DTSGD does not depend on a specific denoising
architecture, suggesting future advancements in Gaussian denoising could
benefit the method. The study also shows that pretraining on natural images
enhances LDCT reconstruction quality, especially with limited training data.
Notably, pretraining involves no additional cost, as existing pretrained models
are used. The proposed method currently holds the top mean position in the
LoDoPaB-CT challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-Shot Scene Change Detection <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11210v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11210v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyusik Cho, Dong Yeop Kim, Euntai Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel, training-free approach to scene change detection. Our
method leverages tracking models, which inherently perform change detection
between consecutive frames of video by identifying common objects and detecting
new or missing objects. Specifically, our method takes advantage of the change
detection effect of the tracking model by inputting reference and query images
instead of consecutive frames. Furthermore, we focus on the content gap and
style gap between two input images in change detection, and address both issues
by proposing adaptive content threshold and style bridging layers,
respectively. Finally, we extend our approach to video, leveraging rich
temporal information to enhance the performance of scene change detection. We
compare our approach and baseline through various experiments. While existing
train-based baseline tend to specialize only in the trained domain, our method
shows consistent performance across various domains, proving the
competitiveness of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025. Code available at: https://github.com/kyusik-cho/ZSSCD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncovering Hidden Subspaces in Video Diffusion Models Using
  Re-Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04956v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04956v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mischa Dombrowski, Hadrien Reynaud, Bernhard Kainz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Latent Video Diffusion Models can easily deceive casual observers and domain
experts alike thanks to the produced image quality and temporal consistency.
Beyond entertainment, this creates opportunities around safe data sharing of
fully synthetic datasets, which are crucial in healthcare, as well as other
domains relying on sensitive personal information. However, privacy concerns
with this approach have not fully been addressed yet, and models trained on
synthetic data for specific downstream tasks still perform worse than those
trained on real data. This discrepancy may be partly due to the sampling space
being a subspace of the training videos, effectively reducing the training data
size for downstream models. Additionally, the reduced temporal consistency when
generating long videos could be a contributing factor.
  In this paper, we first show that training privacy-preserving models in
latent space is computationally more efficient and generalize better.
Furthermore, to investigate downstream degradation factors, we propose to use a
re-identification model, previously employed as a privacy preservation filter.
We demonstrate that it is sufficient to train this model on the latent space of
the video generator. Subsequently, we use these models to evaluate the subspace
covered by synthetic video datasets and thus introduce a new way to measure the
faithfulness of generative machine learning models. We focus on a specific
application in healthcare echocardiography to illustrate the effectiveness of
our novel methods. Our findings indicate that only up to 30.8% of the training
videos are learned in latent video diffusion models, which could explain the
lack of performance when training downstream tasks on synthetic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 tables, 6 figures; v2 Acknowledgements added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image Generation Diversity Issues and How to Tame Them 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16171v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16171v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mischa Dombrowski, Weitong Zhang, Sarah Cechnicka, Hadrien Reynaud, Bernhard Kainz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative methods now produce outputs nearly indistinguishable from real
data but often fail to fully capture the data distribution. Unlike quality
issues, diversity limitations in generative models are hard to detect visually,
requiring specific metrics for assessment. In this paper, we draw attention to
the current lack of diversity in generative models and the inability of common
metrics to measure this. We achieve this by framing diversity as an image
retrieval problem, where we measure how many real images can be retrieved using
synthetic data as queries. This yields the Image Retrieval Score (IRS), an
interpretable, hyperparameter-free metric that quantifies the diversity of a
generative model's output. IRS requires only a subset of synthetic samples and
provides a statistical measure of confidence. Our experiments indicate that
current feature extractors commonly used in generative model assessment are
inadequate for evaluating diversity effectively. Consequently, we perform an
extensive search for the best feature extractors to assess diversity.
Evaluation reveals that current diffusion models converge to limited subsets of
the real distribution, with no current state-of-the-art models superpassing 77%
of the diversity of the training data. To address this limitation, we introduce
Diversity-Aware Diffusion Models (DiADM), a novel approach that improves
diversity of unconditional diffusion models without loss of image quality. We
do this by disentangling diversity from image quality by using a diversity
aware module that uses pseudo-unconditional features as input. We provide a
Python package offering unified feature extraction and metric computation to
further facilitate the evaluation of generative models
https://github.com/MischaD/beyondfid.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 tables, 12 figures; v2 added acknowledgment</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoDTS: Enhancing Sparsely Supervised Collaborative Perception with a
  Dual Teacher-Student Framework <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08344v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08344v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushan Han, Hui Zhang, Honglei Zhang, Jing Wang, Yidong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current collaborative perception methods often rely on fully annotated
datasets, which can be expensive to obtain in practical situations. To reduce
annotation costs, some works adopt sparsely supervised learning techniques and
generate pseudo labels for the missing instances. However, these methods fail
to achieve an optimal confidence threshold that harmonizes the quality and
quantity of pseudo labels. To address this issue, we propose an end-to-end
Collaborative perception Dual Teacher-Student framework (CoDTS), which employs
adaptive complementary learning to produce both high-quality and high-quantity
pseudo labels. Specifically, the Main Foreground Mining (MFM) module generates
high-quality pseudo labels based on the prediction of the static teacher.
Subsequently, the Supplement Foreground Mining (SFM) module ensures a balance
between the quality and quantity of pseudo labels by adaptively identifying
missing instances based on the prediction of the dynamic teacher. Additionally,
the Neighbor Anchor Sampling (NAS) module is incorporated to enhance the
representation of pseudo labels. To promote the adaptive complementary
learning, we implement a staged training strategy that trains the student and
dynamic teacher in a mutually beneficial manner. Extensive experiments
demonstrate that the CoDTS effectively ensures an optimal balance of pseudo
labels in both quality and quantity, establishing a new state-of-the-art in
sparsely supervised collaborative perception.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ R2G: Reasoning to Ground in 3D Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13499v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13499v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Li, Zan Wang, Wei Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Reasoning to Ground (R2G), a neural symbolic model that grounds
the target objects within 3D scenes in a reasoning manner. In contrast to prior
works, R2G explicitly models the 3D scene with a semantic concept-based scene
graph; recurrently simulates the attention transferring across object entities;
thus makes the process of grounding the target objects with the highest
probability interpretable. Specifically, we respectively embed multiple object
properties within the graph nodes and spatial relations among entities within
the edges, utilizing a predefined semantic vocabulary. To guide attention
transferring, we employ learning or prompting-based methods to analyze the
referential utterance and convert it into reasoning instructions within the
same semantic space. In each reasoning round, R2G either (1) merges current
attention distribution with the similarity between the instruction and embedded
entity properties or (2) shifts the attention across the scene graph based on
the similarity between the instruction and embedded spatial relations. The
experiments on Sr3D/Nr3D benchmarks show that R2G achieves a comparable result
with the prior works while maintaining improved interpretability, breaking a
new path for 3D language grounding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing Extended Reality with 3D Gaussian Splatting: Innovations and
  Prospects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06257v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06257v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Qiu, Binzhu Xie, Qixuan Liu, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has attracted significant attention for its
potential to revolutionize 3D representation, rendering, and interaction.
Despite the rapid growth of 3DGS research, its direct application to Extended
Reality (XR) remains underexplored. Although many studies recognize the
potential of 3DGS for XR, few have explicitly focused on or demonstrated its
effectiveness within XR environments. In this paper, we aim to synthesize
innovations in 3DGS that show specific potential for advancing XR research and
development. We conduct a comprehensive review of publicly available 3DGS
papers, with a focus on those referencing XR-related concepts. Additionally, we
perform an in-depth analysis of innovations explicitly relevant to XR and
propose a taxonomy to highlight their significance. Building on these insights,
we propose several prospective XR research areas where 3DGS can make promising
contributions, yet remain rarely touched. By investigating the intersection of
3DGS and XR, this paper provides a roadmap to push the boundaries of XR using
cutting-edge 3DGS techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE AIxVR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Survey</span> on Test-Time Adaptation under Distribution Shifts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.15361v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.15361v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Liang, Ran He, Tieniu Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning methods strive to acquire a robust model during the training
process that can effectively generalize to test samples, even in the presence
of distribution shifts. However, these methods often suffer from performance
degradation due to unknown test distributions. Test-time adaptation (TTA), an
emerging paradigm, has the potential to adapt a pre-trained model to unlabeled
data during testing, before making predictions. Recent progress in this
paradigm has highlighted the significant benefits of using unlabeled data to
train self-adapted models prior to inference. In this survey, we categorize TTA
into several distinct groups based on the form of test data, namely, test-time
domain adaptation, test-time batch adaptation, and online test-time adaptation.
For each category, we provide a comprehensive taxonomy of advanced algorithms
and discuss various learning scenarios. Furthermore, we analyze relevant
applications of TTA and discuss open challenges and promising areas for future
research. For a comprehensive list of TTA methods, kindly refer to
\url{https://github.com/tim-learn/awesome-test-time-adaptation}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Discussions, comments, and questions are all welcomed in
  \url{https://github.com/tim-learn/awesome-test-time-adaptation}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Swin2-MoSE: A New Single Image Super-Resolution Model for Remote Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.18924v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.18924v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Rossi, Vittorio Bernuzzi, Tomaso Fontanini, Massimo Bertozzi, Andrea Prati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the limitations of current optical and sensor technologies and the
high cost of updating them, the spectral and spatial resolution of satellites
may not always meet desired requirements. For these reasons, Remote-Sensing
Single-Image Super-Resolution (RS-SISR) techniques have gained significant
interest. In this paper, we propose Swin2-MoSE model, an enhanced version of
Swin2SR. Our model introduces MoE-SM, an enhanced Mixture-of-Experts (MoE) to
replace the Feed-Forward inside all Transformer block. MoE-SM is designed with
Smart-Merger, and new layer for merging the output of individual experts, and
with a new way to split the work between experts, defining a new per-example
strategy instead of the commonly used per-token one. Furthermore, we analyze
how positional encodings interact with each other, demonstrating that
per-channel bias and per-head bias can positively cooperate. Finally, we
propose to use a combination of Normalized-Cross-Correlation (NCC) and
Structural Similarity Index Measure (SSIM) losses, to avoid typical MSE loss
limitations. Experimental results demonstrate that Swin2-MoSE outperforms any
Swin derived models by up to 0.377 - 0.958 dB (PSNR) on task of 2x, 3x and 4x
resolution-upscaling (Sen2Venus and OLI2MSI datasets). It also outperforms SOTA
models by a good margin, proving to be competitive and with excellent
potential, especially for complex tasks. Additionally, an analysis of
computational costs is also performed. Finally, we show the efficacy of
Swin2-MoSE, applying it to a semantic segmentation task (SeasoNet dataset).
Code and pretrained are available on
https://github.com/IMPLabUniPr/swin2-mose/tree/official_code
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Good Grasps Only: A data engine for <span class="highlight-title">self-supervised</span> fine-tuning of pose
  estimation using grasp poses for verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11512v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11512v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frederik Hagelskjær
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a novel method for self-supervised fine-tuning of
pose estimation. Leveraging zero-shot pose estimation, our approach enables the
robot to automatically obtain training data without manual labeling. After pose
estimation the object is grasped, and in-hand pose estimation is used for data
validation. Our pipeline allows the system to fine-tune while the process is
running, removing the need for a learning phase. The motivation behind our work
lies in the need for rapid setup of pose estimation solutions. Specifically, we
address the challenging task of bin picking, which plays a pivotal role in
flexible robotic setups. Our method is implemented on a robotics work-cell, and
tested with four different objects. For all objects, our method increases the
performance and outperforms a state-of-the-art method trained on the CAD model
of the objects. Project page available at gogoengine.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TextCenGen: Attention-Guided Text-Centric Background Adaptation for
  Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11824v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11824v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Liang, Jiangqi Liu, Yifei Huang, Shiqi Jiang, Sicheng Song, Jianshen Shi, Changbo Wang, Chenhui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Text-to-image (T2I) generation have witnessed a shift
from adapting text to fixed backgrounds to creating images around text.
Traditional approaches are often limited to generate layouts within static
images for effective text placement. Our proposed approach, TextCenGen,
introduces a dynamic adaptation of the blank region for text-friendly image
generation, emphasizing text-centric design and visual harmony generation. Our
method employs force-directed attention guidance in T2I models to generate
images that strategically reserve whitespace for pre-defined text areas, even
for text or icons at the golden ratio. Observing how cross-attention maps
affect object placement, we detect and repel conflicting objects using a
force-directed graph approach, combined with a Spatial Excluding
Cross-Attention Constraint for smooth attention in whitespace areas. As a novel
task in graphic design, experiments indicate that TextCenGen outperforms
existing methods with more harmonious compositions. Furthermore, our method
significantly enhances T2I model outcomes on our specially collected prompt
datasets, catering to varied text positions. These results demonstrate the
efficacy of TextCenGen in creating more harmonious and integrated text-image
compositions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QueSTMaps: Queryable Semantic Topological Maps for 3D Scene
  Understanding <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.06442v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.06442v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yash Mehan, Kumaraditya Gupta, Rohit Jayanti, Anirudh Govil, Sourav Garg, Madhava Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic tasks such as planning and navigation require a hierarchical semantic
understanding of a scene, which could include multiple floors and rooms.
Current methods primarily focus on object segmentation for 3D scene
understanding. However, such methods struggle to segment out topological
regions like "kitchen" in the scene. In this work, we introduce a two-step
pipeline to solve this problem. First, we extract a topological map, i.e.,
floorplan of the indoor scene using a novel multi-channel occupancy
representation. Then, we generate CLIP-aligned features and semantic labels for
every room instance based on the objects it contains using a self-attention
transformer. Our language-topology alignment supports natural language
querying, e.g., a "place to cook" locates the "kitchen". We outperform the
current state-of-the-art on room segmentation by ~20% and room classification
by ~12%. Our detailed qualitative analysis and ablation studies provide
insights into the problem of joint structural and semantic 3D scene
understanding. Project Page: quest-maps.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2024 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS) as Oral Presentation. Also presented at the 2nd
  Workshop on Open-Vocabulary 3D Scene Understanding (OpenSUN3D) at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A simple thinking about the application of the attention mechanism in
  medical ultrasound image segmentation task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00919v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00919v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gongping Chen, Rui Wang, Xiaotao Yin, Liang Cui, Yu Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The AI-based assisted diagnosis programs have been widely investigated on
medical ultrasound images. Complex scenario of ultrasound image, in which the
coupled interference of internal and external factors is severe, brings a
unique challenge for localize the object region automatically and precisely in
ultrasound images. In this study, we seek to propose a more general and robust
Benchmark Attention Adaptive Framework (BAAF) to assist doctors segment or
diagnose lesions and tissues in ultrasound images more quickly and accurately.
Different from existing attention schemes, the BAAF consists of a parallel
hybrid attention module (PHAM) and an adaptive calibration mechanism (ACM).
Specifically, BAAF first coarsely calibrates the input features from the
channel and spatial dimensions, and then adaptively selects more robust lesion
or tissue characterizations from the coarse-calibrated feature maps. The design
of BAAF further optimizes the "what" and "where" focus and selection problems
in CNNs and seeks to improve the segmentation accuracy of lesions or tissues in
medical ultrasound images. The method is evaluated on four medical ultrasound
segmentation tasks, and the adequate experimental results demonstrate the
remarkable performance improvement over existing state-of-the-art methods. In
addition, the comparison with existing attention mechanisms also demonstrates
the superiority of BAAF. This work provides the possibility for automated
medical ultrasound assisted diagnosis and reduces reliance on human accuracy
and precision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Archaeoscape: Bringing Aerial Laser Scanning Archaeology to the Deep
  Learning Era <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05203v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05203v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yohann Perron, Vladyslav Sydorov, Adam P. Wijker, Damian Evans, Christophe Pottier, Loic Landrieu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Airborne Laser Scanning (ALS) technology has transformed modern archaeology
by unveiling hidden landscapes beneath dense vegetation. However, the lack of
expert-annotated, open-access resources has hindered the analysis of ALS data
using advanced deep learning techniques. We address this limitation with
Archaeoscape (available at https://archaeoscape.ai/data/2024/), a novel
large-scale archaeological ALS dataset spanning 888 km$^2$ in Cambodia with
31,141 annotated archaeological features from the Angkorian period.
Archaeoscape is over four times larger than comparable datasets, and the first
ALS archaeology resource with open-access data, annotations, and models.
  We benchmark several recent segmentation models to demonstrate the benefits
of modern vision techniques for this problem and highlight the unique
challenges of discovering subtle human-made structures under dense jungle
canopies. By making Archaeoscape available in open access, we hope to bridge
the gap between traditional archaeology and modern computer vision methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 - Datasets & Benchmarks Track (spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Move and Act: Enhanced Object Manipulation and Background Integrity for
  Image Editing <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17847v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17847v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Jiang, Mingbao Lin, Fei Chao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current methods commonly utilize three-branch structures of inversion,
reconstruction, and editing, to tackle consistent image editing task. However,
these methods lack control over the generation position of the edited object
and have issues with background preservation. To overcome these limitations, we
propose a tuning-free method with only two branches: inversion and editing.
This approach allows users to simultaneously edit the object's action and
control the generation position of the edited object. Additionally, it achieves
improved background preservation. Specifically, we transfer the edited object
information to the target area and repair or preserve the background of other
areas during the inversion process at a specific time step. In the editing
stage, we use the image features in self-attention to query the key and value
of the corresponding time step in the inversion to achieve consistent image
editing. Impressive image editing results and quantitative evaluation
demonstrate the effectiveness of our method. The code is available at
https://github.com/mobiushy/move-act.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Foundational Large Language Models for Materials Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vaibhav Mishra, Somaditya Singh, Dhruv Ahlawat, Mohd Zaki, Vaibhav Bihani, Hargun Singh Grover, Biswajit Mishra, Santiago Miret,  Mausam, N. M. Anoop Krishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Materials discovery and development are critical for addressing global
challenges. Yet, the exponential growth in materials science literature
comprising vast amounts of textual data has created significant bottlenecks in
knowledge extraction, synthesis, and scientific reasoning. Large Language
Models (LLMs) offer unprecedented opportunities to accelerate materials
research through automated analysis and prediction. Still, their effective
deployment requires domain-specific adaptation for understanding and solving
domain-relevant tasks. Here, we present LLaMat, a family of foundational models
for materials science developed through continued pretraining of LLaMA models
on an extensive corpus of materials literature and crystallographic data.
Through systematic evaluation, we demonstrate that LLaMat excels in
materials-specific NLP and structured information extraction while maintaining
general linguistic capabilities. The specialized LLaMat-CIF variant
demonstrates unprecedented capabilities in crystal structure generation,
predicting stable crystals with high coverage across the periodic table.
Intriguingly, despite LLaMA-3's superior performance in comparison to LLaMA-2,
we observe that LLaMat-2 demonstrates unexpectedly enhanced domain-specific
performance across diverse materials science tasks, including structured
information extraction from text and tables, more particularly in crystal
structure generation, a potential adaptation rigidity in overtrained LLMs.
Altogether, the present work demonstrates the effectiveness of domain
adaptation towards developing practically deployable LLM copilots for materials
research. Beyond materials science, our findings reveal important
considerations for domain adaptation of LLMs, such as model selection, training
methodology, and domain-specific performance, which may influence the
development of specialized scientific AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPRec: Leveraging Self-Play to Debias Preference Alignment for Large
  Language Model-based Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chongming Gao, Ruijun Chen, Shuai Yuan, Kexin Huang, Yuanqing Yu, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have attracted significant attention in
recommendation systems. Current LLM-based recommender systems primarily rely on
supervised fine-tuning (SFT) to train the model for recommendation tasks.
However, relying solely on positive samples limits the model's ability to align
with user satisfaction and expectations. To address this, researchers have
introduced Direct Preference Optimization (DPO), which explicitly aligns
recommendations with user preferences using offline preference ranking data.
Despite its advantages, our theoretical analysis reveals that DPO inherently
biases the model towards a few items, exacerbating the filter bubble issue and
ultimately degrading user experience. In this paper, we propose SPRec, a novel
self-play recommendation framework designed to mitigate over-recommendation and
improve fairness without requiring additional data or manual intervention. In
each self-play iteration, the model undergoes an SFT step followed by a DPO
step, treating offline interaction data as positive samples and the predicted
outputs from the previous iteration as negative samples. This effectively
re-weights the DPO loss function using the model's logits, adaptively
suppressing biased items. Extensive experiments on multiple real-world datasets
demonstrate SPRec's effectiveness in enhancing recommendation accuracy and
addressing fairness concerns.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Text Embedding Meets Large Language Model: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijie Nie, Zhangchi Feng, Mingxin Li, Cunwang Zhang, Yanzhao Zhang, Dingkun Long, Richong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text embedding has become a foundational technology in natural language
processing (NLP) during the deep learning era, driving advancements across a
wide array of downstream tasks. While many natural language understanding
challenges can now be modeled using generative paradigms and leverage the
robust generative and comprehension capabilities of large language models
(LLMs), numerous practical applications, such as semantic matching, clustering,
and information retrieval, continue to rely on text embeddings for their
efficiency and effectiveness. In this survey, we categorize the interplay
between LLMs and text embeddings into three overarching themes: (1)
LLM-augmented text embedding, enhancing traditional embedding methods with
LLMs; (2) LLMs as text embedders, utilizing their innate capabilities for
embedding generation; and (3) Text embedding understanding with LLMs,
leveraging LLMs to analyze and interpret embeddings. By organizing these
efforts based on interaction patterns rather than specific downstream
applications, we offer a novel and systematic overview of contributions from
various research and application domains in the era of LLMs. Furthermore, we
highlight the unresolved challenges that persisted in the pre-LLM era with
pre-trained language models (PLMs) and explore the emerging obstacles brought
forth by LLMs. Building on this analysis, we outline prospective directions for
the evolution of text embedding, addressing both theoretical and practical
opportunities in the rapidly advancing landscape of NLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Quality of Video Gaming Experience Using Global-Scale
  Telemetry Data and Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongyang Zhang, Jinhe Wen, Zixi Chen, Dara Arbab, Sruti Sahani, Bijan Arbab, Haojian Jin, Tauhidur Rahman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Frames Per Second (FPS) significantly affects the gaming experience.
Providing players with accurate FPS estimates prior to purchase benefits both
players and game developers. However, we have a limited understanding of how to
predict a game's FPS performance on a specific device. In this paper, we first
conduct a comprehensive analysis of a wide range of factors that may affect
game FPS on a global-scale dataset to identify the determinants of FPS. This
includes player-side and game-side characteristics, as well as country-level
socio-economic statistics. Furthermore, recognizing that accurate FPS
predictions require extensive user data, which raises privacy concerns, we
propose a federated learning-based model to ensure user privacy. Each player
and game is assigned a unique learnable knowledge kernel that gradually
extracts latent features for improved accuracy. We also introduce a novel
training and prediction scheme that allows these kernels to be dynamically
plug-and-play, effectively addressing cold start issues. To train this model
with minimal bias, we collected a large telemetry dataset from 224 countries
and regions, 100,000 users, and 835 games. Our model achieved a mean
Wasserstein distance of 0.469 between predicted and ground truth FPS
distributions, outperforming all baseline methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 11 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Flexible Plug-and-Play Module for Generating Variable-Length 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liyang He, Yuren Zhang, Rui Li, Zhenya Huang, Runze Wu, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep supervised hashing has become a pivotal technique in large-scale image
retrieval, offering significant benefits in terms of storage and search
efficiency. However, existing deep supervised hashing models predominantly
focus on generating fixed-length hash codes. This approach fails to address the
inherent trade-off between efficiency and effectiveness when using hash codes
of varying lengths. To determine the optimal hash code length for a specific
task, multiple models must be trained for different lengths, leading to
increased training time and computational overhead. Furthermore, the current
paradigm overlooks the potential relationships between hash codes of different
lengths, limiting the overall effectiveness of the models. To address these
challenges, we propose the Nested Hash Layer (NHL), a plug-and-play module
designed for existing deep supervised hashing models. The NHL framework
introduces a novel mechanism to simultaneously generate hash codes of varying
lengths in a nested manner. To tackle the optimization conflicts arising from
the multiple learning objectives associated with different code lengths, we
further propose an adaptive weights strategy that dynamically monitors and
adjusts gradients during training. Additionally, recognizing that the
structural information in longer hash codes can provide valuable guidance for
shorter hash codes, we develop a long-short cascade self-distillation method
within the NHL to enhance the overall quality of the generated hash codes.
Extensive experiments demonstrate that NHL not only accelerates the training
process but also achieves superior retrieval performance across various deep
hashing models. Our code is publicly available at
https://github.com/hly1998/NHL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Goal-Conditioned Supervised Learning for Multi-Objective Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08911v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08911v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijun Li, Hilaf Hasson, Jing Hu, Joydeep Ghosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-objective learning endeavors to concurrently optimize multiple
objectives using a single model, aiming to achieve high and balanced
performance across these diverse objectives. However, it often involves a more
complex optimization problem, particularly when navigating potential conflicts
between objectives, leading to solutions with higher memory requirements and
computational complexity. This paper introduces a Multi-Objective
Goal-Conditioned Supervised Learning (MOGCSL) framework for automatically
learning to achieve multiple objectives from offline sequential data. MOGCSL
extends the conventional Goal-Conditioned Supervised Learning (GCSL) method to
multi-objective scenarios by redefining goals from one-dimensional scalars to
multi-dimensional vectors. The need for complex architectures and optimization
constraints can be naturally eliminated. MOGCSL benefits from filtering out
uninformative or noisy instances that do not achieve desirable long-term
rewards. It also incorporates a novel goal-choosing algorithm to model and
select "high" achievable goals for inference.
  While MOGCSL is quite general, we focus on its application to the next action
prediction problem in commercial-grade recommender systems. In this context,
any viable solution needs to be reasonably scalable and also be robust to large
amounts of noisy data that is characteristic of this application space. We show
that MOGCSL performs admirably on both counts. Specifically, extensive
experiments conducted on real-world recommendation datasets validate its
efficacy and efficiency. Also, analysis and experiments are included to explain
its strength in discounting the noisier portions of training data in
recommender systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MOPI-HFRS: A Multi-objective Personalized Health-aware Food
  Recommendation System with LLM-enhanced Interpretation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheyuan Zhang, Zehong Wang, Tianyi Ma, Varun Sameer Taneja, Sofia Nelson, Nhi Ha Lan Le, Keerthiram Murugesan, Mingxuan Ju, Nitesh V Chawla, Chuxu Zhang, Yanfang Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prevalence of unhealthy eating habits has become an increasingly
concerning issue in the United States. However, major food recommendation
platforms (e.g., Yelp) continue to prioritize users' dietary preferences over
the healthiness of their choices. Although efforts have been made to develop
health-aware food recommendation systems, the personalization of such systems
based on users' specific health conditions remains under-explored. In addition,
few research focus on the interpretability of these systems, which hinders
users from assessing the reliability of recommendations and impedes the
practical deployment of these systems. In response to this gap, we first
establish two large-scale personalized health-aware food recommendation
benchmarks at the first attempt. We then develop a novel framework,
Multi-Objective Personalized Interpretable Health-aware Food Recommendation
System (MOPI-HFRS), which provides food recommendations by jointly optimizing
the three objectives: user preference, personalized healthiness and nutritional
diversity, along with an large language model (LLM)-enhanced reasoning module
to promote healthy dietary knowledge through the interpretation of recommended
results. Specifically, this holistic graph learning framework first utilizes
two structure learning and a structure pooling modules to leverage both
descriptive features and health data. Then it employs Pareto optimization to
achieve designed multi-facet objectives. Finally, to further promote the
healthy dietary knowledge and awareness, we exploit an LLM by utilizing
knowledge-infusion, prompting the LLMs with knowledge obtained from the
recommendation model for interpretation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HGCH: A Hyperbolic Graph Convolution Network Model for Heterogeneous
  Collaborative Graph Recommendation <span class="chip">CIKM '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02961v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02961v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Zhang, Ning Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User-item interaction data in collaborative filtering and graph modeling
tasks often exhibit power-law characteristics, which suggest the suitability of
hyperbolic space modeling. Hyperbolic Graph Convolution Neural Networks (HGCNs)
are a novel technique that leverages the advantages of GCN and hyperbolic
space, and then achieves remarkable results. However, existing HGCN methods
have several drawbacks: they fail to fully leverage hyperbolic space properties
due to arbitrary embedding initialization and imprecise tangent space
aggregation; they overlook auxiliary information that could enrich the
collaborative graph; and their training convergence is slow due to margin
ranking loss and random negative sampling. To overcome these challenges, we
propose Hyperbolic Graph Collaborative for Heterogeneous Recommendation (HGCH),
an enhanced HGCN-based model for collaborative filtering that integrates
diverse side information into a heterogeneous collaborative graph and improves
training convergence speed. HGCH first preserves the long-tailed nature of the
graph by initializing node embeddings with power law prior; then it aggregates
neighbors in hyperbolic space using the gyromidpoint method for accurate
computation; finally, it fuses multiple embeddings from different hyperbolic
spaces by the gate fusion with prior. Moreover, HGCH employs a hyperbolic
user-specific negative sampling to speed up convergence. We evaluate HGCH on
four real datasets, and the results show that HGCH achieves competitive results
and outperforms leading baselines, including HGCNs. Extensive ablation studies
further confirm its effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 33rd ACM International Conference on Information
  and Knowledge Management (CIKM '24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large language models as oracles for instantiating ontologies with
  domain-specific knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.04108v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.04108v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni Ciatto, Andrea Agiollo, Matteo Magnini, Andrea Omicini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background. Endowing intelligent systems with semantic data commonly requires
designing and instantiating ontologies with domain-specific knowledge.
Especially in the early phases, those activities are typically performed
manually by human experts possibly leveraging on their own experience. The
resulting process is therefore time-consuming, error-prone, and often biased by
the personal background of the ontology designer. Objective. To mitigate that
issue, we propose a novel domain-independent approach to automatically
instantiate ontologies with domain-specific knowledge, by leveraging on large
language models (LLMs) as oracles. Method. Starting from (i) an initial schema
composed by inter-related classes and properties and (ii) a set of query
templates, our method queries the LLM multiple times, and generates instances
for both classes and properties from its replies. Thus, the ontology is
automatically filled with domain-specific knowledge, compliant to the initial
schema. As a result, the ontology is quickly and automatically enriched with
manifold instances, which experts may consider to keep, adjust, discard, or
complement according to their own needs and expertise. Contribution. We
formalise our method in general way and instantiate it over various LLMs, as
well as on a concrete case study. We report experiments rooted in the
nutritional domain where an ontology of food meals and their ingredients is
automatically instantiated from scratch, starting from a categorisation of
meals and their relationships. There, we analyse the quality of the generated
ontologies and compare ontologies attained by exploiting different LLMs.
Experimentally, our approach achieves a quality metric that is up to five times
higher than the state-of-the-art, while reducing erroneous entities and
relations by up to ten times. Finally, we provide a SWOT analysis of the
proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Writing Style Matters: An Examination of Bias and Fairness in
  Information Retrieval Systems <span class="chip">WSDM 25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.13173v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.13173v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongliu Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Language Model technologies has opened new
opportunities, but also introduced new challenges related to bias and fairness.
This paper explores the uncharted territory of potential biases in
state-of-the-art universal text embedding models towards specific document and
query writing styles within Information Retrieval (IR) systems. Our
investigation reveals that different embedding models exhibit different
preferences of document writing style, while more informal and emotive styles
are less favored by most embedding models. In terms of query writing styles,
many embedding models tend to match the style of the query with the style of
the retrieved documents, but some show a consistent preference for specific
styles. Text embedding models fine-tuned on synthetic data generated by LLMs
display a consistent preference for certain style of generated data. These
biases in text embedding based IR systems can inadvertently silence or
marginalize certain communication styles, thereby posing a significant threat
to fairness in information retrieval. Finally, we also compare the answer
styles of Retrieval Augmented Generation (RAG) systems based on different LLMs
and find out that most text embedding models are biased towards LLM's answer
styles when used as evaluation metrics for answer correctness. This study sheds
light on the critical issue of writing style based bias in IR systems, offering
valuable insights for the development of more fair and robust models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the Eighteenth ACM International Conference on Web
  Search and Data Mining (WSDM 25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GAN<span class="highlight-title">Prompt</span>: Enhancing Robustness in LLM-Based Recommendations with
  GAN-Enhanced Diversity <span class="highlight-title">Prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09671v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09671v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Li, Chuang Zhao, Hongke Zhao, Likang Wu, Ming HE
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Large Language Models (LLMs) have demonstrated remarkable
proficiency in comprehending and generating natural language, with a growing
prevalence in the domain of recommendation systems. However, LLMs still face a
significant challenge called prompt sensitivity, which refers to that it is
highly susceptible to the influence of prompt words. This inconsistency in
response to minor alterations in prompt input may compromise the accuracy and
resilience of recommendation models. To address this issue, this paper proposes
GANPrompt, a multi-dimensional LLMs prompt diversity framework based on
Generative Adversarial Networks (GANs). The framework enhances the model's
adaptability and stability to diverse prompts by integrating GANs generation
techniques with the deep semantic understanding capabilities of LLMs. GANPrompt
first trains a generator capable of producing diverse prompts by analysing
multidimensional user behavioural data. These diverse prompts are then used to
train the LLMs to improve its performance in the face of unseen prompts.
Furthermore, to ensure a high degree of diversity and relevance of the prompts,
this study introduces a mathematical theory-based diversity constraint
mechanism that optimises the generated prompts to ensure that they are not only
superficially distinct, but also semantically cover a wide range of user
intentions. Through extensive experiments on multiple datasets, we demonstrate
the effectiveness of the proposed framework, especially in improving the
adaptability and robustness of recommendation systems in complex and dynamic
environments. The experimental results demonstrate that GANPrompt yields
substantial enhancements in accuracy and robustness relative to existing
state-of-the-art methodologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task-level Distributionally Robust Optimization for Large Language
  Model-based Dense Retrieval <span class="chip">AAAI25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10613v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10613v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyuan Ma, Yongliang Ma, Xing Wu, Zhenpeng Su, Ming Zhou, Songlin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model-based Dense Retrieval (LLM-DR) optimizes over numerous
heterogeneous fine-tuning collections from different domains. However, the
discussion about its training data distribution is still minimal. Previous
studies rely on empirically assigned dataset choices or sampling ratios, which
inevitably lead to sub-optimal retrieval performances. In this paper, we
propose a new task-level Distributionally Robust Optimization (tDRO) algorithm
for LLM-DR fine-tuning, targeted at improving the universal domain
generalization ability by end-to-end reweighting the data distribution of each
task. The tDRO parameterizes the domain weights and updates them with scaled
domain gradients. The optimized weights are then transferred to the LLM-DR
fine-tuning to train more robust retrievers. Experiments show optimal
improvements in large-scale retrieval benchmarks and reduce up to 30% dataset
usage after applying our optimization algorithm with a series of
different-sized LLM-DR models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI25. Source code is available at
  https://github.com/tdro-llm/tdro</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Informational Role of Online Recommendations: Evidence from a Field
  Experiment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.14219v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.14219v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Aridor, Duarte Goncalves, Daniel Kluver, Ruoyan Kong, Joseph Konstan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We conduct a field experiment on a movie-recommendation platform to
investigate whether and how online recommendations influence consumption
choices. Using a within-subjects design, our experiment measures the causal
effect of recommendations on consumption and decomposes the relative importance
of two economic mechanisms: expanding consumers' consideration sets and
providing information about their idiosyncratic match value. We find that the
informational component exerts a stronger influence - recommendations shape
consumer beliefs, which in turn drive consumption, particularly among less
experienced consumers. Our findings and experimental design provide valuable
insights for the economic evaluation and optimisation of online recommendation
systems.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Doe-1: Closed-Loop Autonomous Driving with Large World Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenzhao Zheng, Zetian Xia, Yuanhui Huang, Sicheng Zuo, Jie Zhou, Jiwen Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end autonomous driving has received increasing attention due to its
potential to learn from large amounts of data. However, most existing methods
are still open-loop and suffer from weak scalability, lack of high-order
interactions, and inefficient decision-making. In this paper, we explore a
closed-loop framework for autonomous driving and propose a large Driving wOrld
modEl (Doe-1) for unified perception, prediction, and planning. We formulate
autonomous driving as a next-token generation problem and use multi-modal
tokens to accomplish different tasks. Specifically, we use free-form texts
(i.e., scene descriptions) for perception and generate future predictions
directly in the RGB space with image tokens. For planning, we employ a
position-aware tokenizer to effectively encode action into discrete tokens. We
train a multi-modal transformer to autoregressively generate perception,
prediction, and planning tokens in an end-to-end and unified manner.
Experiments on the widely used nuScenes dataset demonstrate the effectiveness
of Doe-1 in various tasks including visual question-answering,
action-conditioned video generation, and motion planning. Code:
https://github.com/wzzheng/Doe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at: https://github.com/wzzheng/Doe</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spectral Image Tokenizer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Esteves, Mohammed Suhail, Ameesh Makadia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image tokenizers map images to sequences of discrete tokens, and are a
crucial component of autoregressive transformer-based image generation. The
tokens are typically associated with spatial locations in the input image,
arranged in raster scan order, which is not ideal for autoregressive modeling.
In this paper, we propose to tokenize the image spectrum instead, obtained from
a discrete wavelet transform (DWT), such that the sequence of tokens represents
the image in a coarse-to-fine fashion. Our tokenizer brings several advantages:
1) it leverages that natural images are more compressible at high frequencies,
2) it can take and reconstruct images of different resolutions without
retraining, 3) it improves the conditioning for next-token prediction --
instead of conditioning on a partial line-by-line reconstruction of the image,
it takes a coarse reconstruction of the full image, 4) it enables partial
decoding where the first few generated tokens can reconstruct a coarse version
of the image, 5) it enables autoregressive models to be used for image
upsampling. We evaluate the tokenizer reconstruction metrics as well as
multiscale image generation, text-guided image upsampling and editing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hidden Biases of End-to-End Driving <span class="highlight-title">Dataset</span>s <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Zimmerlin, Jens Beißwenger, Bernhard Jaeger, Andreas Geiger, Kashyap Chitta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end driving systems have made rapid progress, but have so far not been
applied to the challenging new CARLA Leaderboard 2.0. Further, while there is a
large body of literature on end-to-end architectures and training strategies,
the impact of the training dataset is often overlooked. In this work, we make a
first attempt at end-to-end driving for Leaderboard 2.0. Instead of
investigating architectures, we systematically analyze the training dataset,
leading to new insights: (1) Expert style significantly affects downstream
policy performance. (2) In complex data sets, the frames should not be weighted
on the basis of simplistic criteria such as class frequencies. (3) Instead,
estimating whether a frame changes the target labels compared to previous
frames can reduce the size of the dataset without removing important
information. By incorporating these findings, our model ranks first and second
respectively on the map and sensors tracks of the 2024 CARLA Challenge, and
sets a new state-of-the-art on the Bench2Drive test routes. Finally, we uncover
a design flaw in the current evaluation metrics and propose a modification for
future challenges. Our dataset, code, and pre-trained models are publicly
available at https://github.com/autonomousvision/carla_garage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report for the CVPR 2024 Workshop on Foundation Models for
  Autonomous Systems. Runner-up of the track 'CARLA Autonomous Driving
  Challenge' in the 2024 Autonomous Grand Challenge
  (https://opendrivelab.com/challenge2024/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Owl-1: Omni World Model for Consistent Long Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanhui Huang, Wenzhao Zheng, Yuan Gao, Xin Tao, Pengfei Wan, Di Zhang, Jie Zhou, Jiwen Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video generation models (VGMs) have received extensive attention recently and
serve as promising candidates for general-purpose large vision models. While
they can only generate short videos each time, existing methods achieve long
video generation by iteratively calling the VGMs, using the last-frame output
as the condition for the next-round generation. However, the last frame only
contains short-term fine-grained information about the scene, resulting in
inconsistency in the long horizon. To address this, we propose an Omni World
modeL (Owl-1) to produce long-term coherent and comprehensive conditions for
consistent long video generation. As videos are observations of the underlying
evolving world, we propose to model the long-term developments in a latent
space and use VGMs to film them into videos. Specifically, we represent the
world with a latent state variable which can be decoded into explicit video
observations. These observations serve as a basis for anticipating temporal
dynamics which in turn update the state variable. The interaction between
evolving dynamics and persistent state enhances the diversity and consistency
of the long videos. Extensive experiments show that Owl-1 achieves comparable
performance with SOTA methods on VBench-I2V and VBench-Long, validating its
ability to generate high-quality video observations. Code:
https://github.com/huang-yh/Owl.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at: https://github.com/huang-yh/Owl</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wait-Less Offline Tuning and Re-solving for Online Decision Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingruo Sun, Wenzhi Gao, Ellen Vitercik, Yinyu Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online linear programming (OLP) has found broad applications in revenue
management and resource allocation. State-of-the-art OLP algorithms achieve low
regret by repeatedly solving linear programming (LP) subproblems that
incorporate updated resource information. However, LP-based methods are
computationally expensive and often inefficient for large-scale applications.
In contrast, recent first-order OLP algorithms are more computationally
efficient but typically suffer from worse regret guarantees. To address these
shortcomings, we propose a new algorithm that combines the strengths of
LP-based and first-order OLP methods. The algorithm re-solves the LP
subproblems periodically at a predefined frequency $f$ and uses the latest dual
prices to guide online decision-making. In addition, a first-order method runs
in parallel during each interval between LP re-solves, smoothing resource
consumption. Our algorithm achieves $\mathscr{O}(\log (T/f) + \sqrt{f})$
regret, delivering a "wait-less" online decision-making process that balances
the computational efficiency of first-order methods and the superior regret
guarantee of LP-based methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neptune: The Long Orbit to Benchmarking Long Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arsha Nagrani, Mingda Zhang, Ramin Mehran, Rachel Hornung, Nitesh Bharadwaj Gundavarapu, Nilpa Jha, Austin Myers, Xingyi Zhou, Boqing Gong, Cordelia Schmid, Mikhail Sirotenko, Yukun Zhu, Tobias Weyand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes a semi-automatic pipeline to generate challenging
question-answer-decoy sets for understanding long videos. Many existing video
datasets and models are focused on short clips (10s-30s). While some long video
datasets do exist, they can often be solved by powerful image models applied
per frame (and often to very few frames) in a video, and are usually manually
annotated at high cost. In order to mitigate both these problems, we propose a
scalable dataset creation pipeline which leverages large models (VLMs and
LLMs), to automatically generate dense, time-aligned video captions, as well as
tough question answer decoy sets for video segments (up to 15 minutes in
length). Our dataset Neptune covers a broad range of long video reasoning
abilities and consists of a subset that emphasizes multimodal reasoning. Since
existing metrics for open-ended question answering are either rule-based or may
rely on proprietary models, we provide a new open source model-based metric GEM
to score open-ended responses on Neptune. Benchmark evaluations reveal that
most current open-source long video models perform poorly on Neptune,
particularly on questions testing temporal ordering, counting and state
changes. Through Neptune, we aim to spur the development of more advanced
models capable of understanding long videos. The dataset is available at
https://github.com/google-deepmind/neptune
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Theoretical Analysis of Soft-Label vs Hard-Label Training in Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saptarshi Mandal, Xiaojun Lin, R. Srikant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation, where a small student model learns from a pre-trained
large teacher model, has achieved substantial empirical success since the
seminal work of \citep{hinton2015distilling}. Despite prior theoretical studies
exploring the benefits of knowledge distillation, an important question remains
unanswered: why does soft-label training from the teacher require significantly
fewer neurons than directly training a small neural network with hard labels?
To address this, we first present motivating experimental results using simple
neural network models on a binary classification problem. These results
demonstrate that soft-label training consistently outperforms hard-label
training in accuracy, with the performance gap becoming more pronounced as the
dataset becomes increasingly difficult to classify. We then substantiate these
observations with a theoretical contribution based on two-layer neural network
models. Specifically, we show that soft-label training using gradient descent
requires only $O\left(\frac{1}{\gamma^2 \epsilon}\right)$ neurons to achieve a
classification loss averaged over epochs smaller than some $\epsilon > 0$,
where $\gamma$ is the separation margin of the limiting kernel. In contrast,
hard-label training requires $O\left(\frac{1}{\gamma^4} \cdot
\ln\left(\frac{1}{\epsilon}\right)\right)$ neurons, as derived from an adapted
version of the gradient descent analysis in \citep{ji2020polylogarithmic}. This
implies that when $\gamma \leq \epsilon$, i.e., when the dataset is challenging
to classify, the neuron requirement for soft-label training can be
significantly lower than that for hard-label training. Finally, we present
experimental results on deep neural networks, further validating these
theoretical findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main Body of the Paper is under Review at L4DC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JuStRank: Benchmarking LLM Judges for System Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ariel Gera, Odellia Boni, Yotam Perlitz, Roy Bar-Haim, Lilach Eden, Asaf Yehudai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the rapid progress of generative AI, there is a pressing need to
systematically compare and choose between the numerous models and
configurations available. The scale and versatility of such evaluations make
the use of LLM-based judges a compelling solution for this challenge.
Crucially, this approach requires first to validate the quality of the LLM
judge itself. Previous work has focused on instance-based assessment of LLM
judges, where a judge is evaluated over a set of responses, or response pairs,
while being agnostic to their source systems. We argue that this setting
overlooks critical factors affecting system-level ranking, such as a judge's
positive or negative bias towards certain systems. To address this gap, we
conduct the first large-scale study of LLM judges as system rankers. System
scores are generated by aggregating judgment scores over multiple system
outputs, and the judge's quality is assessed by comparing the resulting system
ranking to a human-based ranking. Beyond overall judge assessment, our analysis
provides a fine-grained characterization of judge behavior, including their
decisiveness and bias.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Obfuscated Activations Bypass LLM Latent-Space Defenses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luke Bailey, Alex Serrano, Abhay Sheshadri, Mikhail Seleznyov, Jordan Taylor, Erik Jenner, Jacob Hilton, Stephen Casper, Carlos Guestrin, Scott Emmons
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent latent-space monitoring techniques have shown promise as defenses
against LLM attacks. These defenses act as scanners that seek to detect harmful
activations before they lead to undesirable actions. This prompts the question:
Can models execute harmful behavior via inconspicuous latent states? Here, we
study such obfuscated activations. We show that state-of-the-art latent-space
defenses -- including sparse autoencoders, representation probing, and latent
OOD detection -- are all vulnerable to obfuscated activations. For example,
against probes trained to classify harmfulness, our attacks can often reduce
recall from 100% to 0% while retaining a 90% jailbreaking rate. However,
obfuscation has limits: we find that on a complex task (writing SQL code),
obfuscation reduces model performance. Together, our results demonstrate that
neural activations are highly malleable: we can reshape activation patterns in
a variety of ways, often while preserving a network's behavior. This poses a
fundamental challenge to latent-space defenses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://obfuscated-activations.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving the Reliability of Cable Broadband Networks via Proactive
  Network Maintenance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiyao Hu, Zhenyu Zhou, Xiaowei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cable broadband networks are one of the few "last-mile" broadband
technologies widely available in the U.S. Unfortunately, they have poor
reliability after decades of deployment. The cable industry proposed a
framework called Proactive Network Maintenance (PNM) to diagnose the cable
networks. However, there is little public knowledge or systematic study on how
to use these data to detect and localize cable network problems. Existing tools
in the public domain have prohibitive high false-positive rates. In this paper,
we propose CableMon, the first public-domain system that applies machine
learning techniques to PNM data to improve the reliability of cable broadband
networks. CableMon tackles two key challenges faced by cable ISPs: accurately
detecting failures, and distinguishing whether a failure occurs within a
network or at a subscriber's premise. CableMon uses statistical models to
generate features from time series data and uses customer trouble tickets as
hints to infer abnormal/failure thresholds for these generated features.
Further, CableMon employs an unsupervised learning model to group cable devices
sharing similar anomalous patterns and effectively identify impairments that
occur inside a cable network and impairments occur at a subscriber's premise,
as these two different faults require different types of technical personnel to
repair them. We use eight months of PNM data and customer trouble tickets from
an ISP and experimental deployment to evaluate CableMon's performance. Our
evaluation results show that CableMon can effectively detect and distinguish
failures from PNM data and outperforms existing public-domain tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages including reference. Submitted to IEEE/ACM Transactions on
  Networking. Partly published in NSDI'20, this is the extended version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Does Representation Matter? Exploring Intermediate Layers in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oscar Skean, Md Rifat Arefin, <span class="highlight-author">Yann LeCun</span>, Ravid Shwartz-Ziv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding what defines a good representation in large language models
(LLMs) is fundamental to both theoretical understanding and practical
applications. In this paper, we investigate the quality of intermediate
representations in various LLM architectures, including Transformers and State
Space Models (SSMs). We find that intermediate layers often yield more
informative representations for downstream tasks than the final layers. To
measure the representation quality, we adapt and apply a suite of metrics -
such as prompt entropy, curvature, and augmentation-invariance - originally
proposed in other contexts. Our empirical study reveals significant
architectural differences, how representations evolve throughout training, and
how factors like input randomness and prompt length affect each layer. Notably,
we observe a bimodal pattern in the entropy of some intermediate layers and
consider potential explanations tied to training data. Overall, our results
illuminate the internal mechanics of LLMs and guide strategies for
architectural optimization and training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 2024 NeurIPs Workshop on Machine Learning and Compression</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Experimental Machine Learning with Classical and Quantum Data via NMR
  Quantum Kernels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivek Sabarad, T. S. Mahesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kernel methods map data into high-dimensional spaces, enabling linear
algorithms to learn nonlinear functions without explicitly storing the feature
vectors. Quantum kernel methods promise efficient learning by encoding feature
maps into exponentially large Hilbert spaces inherent in quantum systems. In
this work we implement quantum kernels on a 10-qubit star-topology register in
a nuclear magnetic resonance (NMR) platform. We experimentally encode classical
data in the evolution of multiple quantum coherence orders using data-dependent
unitary transformations and then demonstrate one-dimensional regression and
two-dimensional classification tasks. By extending the register to a
double-layered star configuration, we propose an extended quantum kernel to
handle non-parametrized operator inputs. By numerically simulating the extended
quantum kernel, we show classification of entangling and nonentangling
unitaries. These results confirm that quantum kernels exhibit strong
capabilities in classical as well as quantum machine learning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Convergence of Decentralized Gradient Tracking under the KL
  Property 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaokai Chen, Tianyu Cao, Gesualdo Scutari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study decentralized multiagent optimization over networks, modeled as
undirected graphs. The optimization problem consists of minimizing a nonconvex
smooth function plus a convex extended-value function, which enforces
constraints or extra structure on the solution (e.g., sparsity, low-rank). We
further assume that the objective function satisfies the Kurdyka-{\L}ojasiewicz
(KL) property, with given exponent $\theta\in [0,1)$. The KL property is
satisfied by several (nonconvex) functions of practical interest, e.g., arising
from machine learning applications; in the centralized setting, it permits to
achieve strong convergence guarantees. Here we establish convergence of the
same type for the notorious decentralized gradient-tracking-based algorithm
SONATA. Specifically, $\textbf{(i)}$ when $\theta\in (0,1/2]$, the sequence
generated by SONATA converges to a stationary solution of the problem at
R-linear rate;$ \textbf{(ii)} $when $\theta\in (1/2,1)$, sublinear rate is
certified; and finally $\textbf{(iii)}$ when $\theta=0$, the iterates will
either converge in a finite number of steps or converges at R-linear rate. This
matches the convergence behavior of centralized proximal-gradient algorithms
except when $\theta=0$. Numerical results validate our theoretical findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SimAvatar: Simulation-Ready Avatars with Layered Hair and Clothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueting Li, Ye Yuan, Shalini De Mello, Gilles Daviet, Jonathan Leaf, Miles Macklin, Jan Kautz, Umar Iqbal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SimAvatar, a framework designed to generate simulation-ready
clothed 3D human avatars from a text prompt. Current text-driven human avatar
generation methods either model hair, clothing, and the human body using a
unified geometry or produce hair and garments that are not easily adaptable for
simulation within existing simulation pipelines. The primary challenge lies in
representing the hair and garment geometry in a way that allows leveraging
established prior knowledge from foundational image diffusion models (e.g.,
Stable Diffusion) while being simulation-ready using either physics or neural
simulators. To address this task, we propose a two-stage framework that
combines the flexibility of 3D Gaussians with simulation-ready hair strands and
garment meshes. Specifically, we first employ three text-conditioned 3D
generative models to generate garment mesh, body shape and hair strands from
the given text prompt. To leverage prior knowledge from foundational diffusion
models, we attach 3D Gaussians to the body mesh, garment mesh, as well as hair
strands and learn the avatar appearance through optimization. To drive the
avatar given a pose sequence, we first apply physics simulators onto the
garment meshes and hair strands. We then transfer the motion onto 3D Gaussians
through carefully designed mechanisms for each body part. As a result, our
synthesized avatars have vivid texture and realistic dynamic motion. To the
best of our knowledge, our method is the first to produce highly realistic,
fully simulation-ready 3D avatars, surpassing the capabilities of current
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://nvlabs.github.io/SimAvatar/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sail into the Headwind: Alignment via Robust Rewards and Dynamic Labels
  against Reward Hacking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paria Rashidinejad, Yuandong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning AI systems with human preferences typically suffers from the
infamous reward hacking problem, where optimization of an imperfect reward
model leads to undesired behaviors. In this paper, we investigate reward
hacking in offline preference optimization, which aims to improve an initial
model using a preference dataset. We identify two types of reward hacking
stemming from statistical fluctuations in the dataset: Type I Reward Hacking
due to subpar choices appearing more favorable, and Type II Reward Hacking due
to decent choices appearing less favorable. We prove that many (mainstream or
theoretical) preference optimization methods suffer from both types of reward
hacking. To mitigate Type I Reward Hacking, we propose POWER, a new preference
optimization method that combines Guiasu's weighted entropy with a robust
reward maximization objective. POWER enjoys finite-sample guarantees under
general function approximation, competing with the best covered policy in the
data. To mitigate Type II Reward Hacking, we analyze the learning dynamics of
preference optimization and develop a novel technique that dynamically updates
preference labels toward certain "stationary labels", resulting in diminishing
gradients for untrustworthy samples. Empirically, POWER with dynamic labels
(POWER-DL) consistently outperforms state-of-the-art methods on alignment
benchmarks, achieving improvements of up to 13.0 points on AlpacaEval 2.0 and
11.5 points on Arena-Hard over DPO, while also improving or maintaining
performance on downstream tasks such as mathematical reasoning. Strong
theoretical guarantees and empirical results demonstrate the promise of
POWER-DL in mitigating reward hacking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>46 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Capturing the Temporal Dependence of Training Data Influence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiachen T. Wang, Dawn Song, James Zou, Prateek Mittal, Ruoxi Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional data influence estimation methods, like influence function,
assume that learning algorithms are permutation-invariant with respect to
training data. However, modern training paradigms, especially for foundation
models using stochastic algorithms and multi-stage curricula, are sensitive to
data ordering, thus violating this assumption. This mismatch renders influence
functions inadequate for answering a critical question in machine learning: How
can we capture the dependence of data influence on the optimization trajectory
during training? To address this gap, we formalize the concept of
trajectory-specific leave-one-out (LOO) influence, which quantifies the impact
of removing a data point from a specific iteration during training, accounting
for the exact sequence of data encountered and the model's optimization
trajectory. However, exactly evaluating the trajectory-specific LOO presents a
significant computational challenge. To address this, we propose data value
embedding, a novel technique enabling efficient approximation of
trajectory-specific LOO. Specifically, we compute a training data embedding
that encapsulates the cumulative interactions between data and the evolving
model parameters. The LOO can then be efficiently approximated through a simple
dot-product between the data value embedding and the gradient of the given test
data. As data value embedding captures training data ordering, it offers
valuable insights into model training dynamics. In particular, we uncover
distinct phases of data influence, revealing that data points in the early and
late stages of training exert a greater impact on the final model. These
insights translate into actionable strategies for managing the computational
overhead of data selection by strategically timing the selection process,
potentially opening new avenues in data curation research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Correspondence to Jiachen T. Wang and Ruoxi Jia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GainAdaptor: Learning Quadrupedal Locomotion with Dual Actors for
  Adaptable and Energy-Efficient Walking on Various Terrains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mincheol Kim, Nahyun Kwon, Jung-Yup Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep reinforcement learning (DRL) has emerged as an innovative solution for
controlling legged robots in challenging environments using minimalist
architectures. Traditional control methods for legged robots, such as inverse
dynamics, either directly manage joint torques or use proportional-derivative
(PD) controllers to regulate joint positions at a higher level. In case of DRL,
direct torque control presents significant challenges, leading to a preference
for joint position control. However, this approach necessitates careful
adjustment of joint PD gains, which can limit both adaptability and efficiency.
In this paper, we propose GainAdaptor, an adaptive gain control framework that
autonomously tunes joint PD gains to enhance terrain adaptability and energy
efficiency. The framework employs a dual-actor algorithm to dynamically adjust
the PD gains based on varying ground conditions. By utilizing a divided action
space, GainAdaptor efficiently learns stable and energy-efficient locomotion.
We validate the effectiveness of the proposed method through experiments
conducted on a Unitree Go1 robot, demonstrating improved locomotion performance
across diverse terrains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Loss function to optimise signal significance in particle physics <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jai Bardhan, Cyrin Neeraj, Subhadip Mitra, Tanumoy Mandal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We construct a surrogate loss to directly optimise the significance metric
used in particle physics. We evaluate our loss function for a simple event
classification task using a linear model and show that it produces decision
boundaries that change according to the cross sections of the processes
involved. We find that the models trained with the new loss have higher signal
efficiency for similar values of estimated signal significance compared to ones
trained with a cross-entropy loss, showing promise to improve sensitivity of
particle physics searches at colliders.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures. Appeared in the Machine Learning for Physical
  Sciences (ML4PS) workshop in NeurIPS 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A novel ML-fuzzy control system for optimizing PHEV fuel efficiency and
  extending electric range under diverse driving conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehrdad Raeesi, Saba Mansour, Sina Changizian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aiming for a greener transportation future, this study introduces an
innovative control system for plug-in hybrid electric vehicles (PHEVs) that
utilizes machine learning (ML) techniques to forecast energy usage in the pure
electric mode of the vehicle and optimize power allocation across different
operational modes, including pure electric, series hybrid, parallel hybrid, and
internal combustion operation. The fuzzy logic decision-making process governs
the vehicle control system. The performance was assessed under various driving
conditions. Key findings include a significant enhancement in pure electric
mode efficiency, achieving an extended full-electric range of approximately 84
kilometers on an 80% utilization of a 20-kWh battery pack. During the WLTC
driving cycle, the control system reduced fuel consumption to 2.86 L/100km,
representing a 20% reduction in gasoline-equivalent fuel consumption.
Evaluations of vehicle performance at discrete driving speeds, highlighted
effective energy management, with the vehicle battery charging at lower speeds
and discharging at higher speeds, showing optimized energy recovery and
consumption strategies. Initial battery charge levels notably influenced
vehicle performance. A 90% initial charge enabled prolonged all-electric
operation, minimizing fuel consumption to 2 L/100km less than that of the base
control system. Real-world driving pattern analysis revealed significant
variations, with shorter, slower cycles requiring lower fuel consumption due to
prioritized electric propulsion, while longer, faster cycles increased internal
combustion engine usage. The control system also adapted to different battery
state of health (SOH) conditions, with higher SOH facilitating extended
electric mode usage, reducing total fuel consumption by up to 2.87 L/100km.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Regression and Classification with Single-Qubit Quantum Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leandro C. Souza, Bruno C. Guingo, Gilson Giraldi, Renato Portugal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since classical machine learning has become a powerful tool for developing
data-driven algorithms, quantum machine learning is expected to similarly
impact the development of quantum algorithms. The literature reflects a
mutually beneficial relationship between machine learning and quantum
computing, where progress in one field frequently drives improvements in the
other. Motivated by the fertile connection between machine learning and quantum
computing enabled by parameterized quantum circuits, we use a
resource-efficient and scalable Single-Qubit Quantum Neural Network (SQQNN) for
both regression and classification tasks. The SQQNN leverages parameterized
single-qubit unitary operators and quantum measurements to achieve efficient
learning. To train the model, we use gradient descent for regression tasks. For
classification, we introduce a novel training method inspired by the Taylor
series, which can efficiently find a global minimum in a single step. This
approach significantly accelerates training compared to iterative methods.
Evaluated across various applications, the SQQNN exhibits virtually error-free
and strong performance in regression and classification tasks, including the
MNIST dataset. These results demonstrate the versatility, scalability, and
suitability of the SQQNN for deployment on near-term quantum devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 7 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Early Detection of At-Risk Students Using Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azucena L. Jimenez Martinez, Kanika Sood, Rakeshkumar Mahto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research presents preliminary work to address the challenge of
identifying at-risk students using supervised machine learning and three unique
data categories: engagement, demographics, and performance data collected from
Fall 2023 using Canvas and the California State University, Fullerton
dashboard. We aim to tackle the persistent challenges of higher education
retention and student dropout rates by screening for at-risk students and
building a high-risk identification system. By focusing on previously
overlooked behavioral factors alongside traditional metrics, this work aims to
address educational gaps, enhance student outcomes, and significantly boost
student success across disciplines at the University. Pre-processing steps take
place to establish a target variable, anonymize student information, manage
missing data, and identify the most significant features. Given the mixed data
types in the datasets and the binary classification nature of this study, this
work considers several machine learning models, including Support Vector
Machines (SVM), Naive Bayes, K-nearest neighbors (KNN), Decision Trees,
Logistic Regression, and Random Forest. These models predict at-risk students
and identify critical periods of the semester when student performance is most
vulnerable. We will use validation techniques such as train test split and
k-fold cross-validation to ensure the reliability of the models. Our analysis
indicates that all algorithms generate an acceptable outcome for at-risk
student predictions, while Naive Bayes performs best overall.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Optimization via Continual Variational Last Layer Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Brunzema, Mikkel Jordahn, John Willes, Sebastian Trimpe, Jasper Snoek, James Harrison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian Processes (GPs) are widely seen as the state-of-the-art surrogate
models for Bayesian optimization (BO) due to their ability to model uncertainty
and their performance on tasks where correlations are easily captured (such as
those defined by Euclidean metrics) and their ability to be efficiently updated
online. However, the performance of GPs depends on the choice of kernel, and
kernel selection for complex correlation structures is often difficult or must
be made bespoke. While Bayesian neural networks (BNNs) are a promising
direction for higher capacity surrogate models, they have so far seen limited
use due to poor performance on some problem types. In this paper, we propose an
approach which shows competitive performance on many problem types, including
some that BNNs typically struggle with. We build on variational Bayesian last
layers (VBLLs), and connect training of these models to exact conditioning in
GPs. We exploit this connection to develop an efficient online training
algorithm that interleaves conditioning and optimization. Our findings suggest
that VBLL networks significantly outperform GPs and other BNN architectures on
tasks with complex input correlations, and match the performance of well-tuned
GPs on established benchmark tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Ensemble-Based Deep Learning Model with Explainable AI for
  Accurate Kidney Disease Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md. Arifuzzaman, Iftekhar Ahmed, Md. Jalal Uddin Chowdhury, Shadman Sakib, Mohammad Shoaib Rahman, Md. Ebrahim Hossain, Shakib Absar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chronic Kidney Disease (CKD) represents a significant global health
challenge, characterized by the progressive decline in renal function, leading
to the accumulation of waste products and disruptions in fluid balance within
the body. Given its pervasive impact on public health, there is a pressing need
for effective diagnostic tools to enable timely intervention. Our study delves
into the application of cutting-edge transfer learning models for the early
detection of CKD. Leveraging a comprehensive and publicly available dataset, we
meticulously evaluate the performance of several state-of-the-art models,
including EfficientNetV2, InceptionNetV2, MobileNetV2, and the Vision
Transformer (ViT) technique. Remarkably, our analysis demonstrates superior
accuracy rates, surpassing the 90% threshold with MobileNetV2 and achieving
91.5% accuracy with ViT. Moreover, to enhance predictive capabilities further,
we integrate these individual methodologies through ensemble modeling,
resulting in our ensemble model exhibiting a remarkable 96% accuracy in the
early detection of CKD. This significant advancement holds immense promise for
improving clinical outcomes and underscores the critical role of machine
learning in addressing complex medical challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Network Symmetrisation in Concrete Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rob Cornish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cornish (2024) recently gave a general theory of neural network
symmetrisation in the abstract context of Markov categories. We give a
high-level overview of these results, and their concrete implications for the
symmetrisation of deterministic functions and of Markov kernels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STORM: A Spatio-Temporal Factor Model Based on Dual Vector Quantized
  Variational Autoencoders for Financial Trading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilei Zhao, Wentao Zhang, Tingran Yang, Yong Jiang, Fei Huang, Wei Yang Bryan Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In financial trading, factor models are widely used to price assets and
capture excess returns from mispricing. Recently, we have witnessed the rise of
variational autoencoder-based latent factor models, which learn latent factors
self-adaptively. While these models focus on modeling overall market
conditions, they often fail to effectively capture the temporal patterns of
individual stocks. Additionally, representing multiple factors as single values
simplifies the model but limits its ability to capture complex relationships
and dependencies. As a result, the learned factors are of low quality and lack
diversity, reducing their effectiveness and robustness across different trading
periods. To address these issues, we propose a Spatio-Temporal factOR Model
based on dual vector quantized variational autoencoders, named STORM, which
extracts features of stocks from temporal and spatial perspectives, then fuses
and aligns these features at the fine-grained and semantic level, and
represents the factors as multi-dimensional embeddings. The discrete codebooks
cluster similar factor embeddings, ensuring orthogonality and diversity, which
helps distinguish between different factors and enables factor selection in
financial trading. To show the performance of the proposed factor model, we
apply it to two downstream experiments: portfolio management on two stock
datasets and individual trading tasks on six specific stocks. The extensive
experiments demonstrate STORM's flexibility in adapting to downstream tasks and
superior performance over baseline models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Finite-PINN: A Physics-Informed Neural Network Architecture for Solving
  Solid Mechanics Problems with General Geometries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolin Li, Yuyang Miao, Zahra Sharif Khodaei, M. H. Aliabadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PINN models have demonstrated impressive capabilities in addressing fluid PDE
problems, and their potential in solid mechanics is beginning to emerge. This
study identifies two key challenges when using PINN to solve general solid
mechanics problems. These challenges become evident when comparing the
limitations of PINN with the well-established numerical methods commonly used
in solid mechanics, such as the finite element method (FEM). Specifically: a)
PINN models generate solutions over an infinite domain, which conflicts with
the finite boundaries typical of most solid structures; and b) the solution
space utilised by PINN is Euclidean, which is inadequate for addressing the
complex geometries often present in solid structures.
  This work proposes a PINN architecture used for general solid mechanics
problems, termed the Finite-PINN model. The proposed model aims to effectively
address these two challenges while preserving as much of the original
implementation of PINN as possible. The unique architecture of the Finite-PINN
model addresses these challenges by separating the approximation of stress and
displacement fields, and by transforming the solution space from the
traditional Euclidean space to a Euclidean-topological joint space. Several
case studies presented in this paper demonstrate that the Finite-PINN model
provides satisfactory results for a variety of problem types, including both
forward and inverse problems, in both 2D and 3D contexts. The developed
Finite-PINN model offers a promising tool for addressing general solid
mechanics problems, particularly those not yet well-explored in current
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Search Strategy Generation for Branch and Bound Using Genetic
  Programming <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gwen Maudet, Grégoire Danoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Branch-and-Bound (B\&B) is an exact method in integer programming that
recursively divides the search space into a tree. During the resolution
process, determining the next subproblem to explore within the tree-known as
the search strategy-is crucial. Hand-crafted heuristics are commonly used, but
none are effective over all problem classes. Recent approaches utilizing neural
networks claim to make more intelligent decisions but are computationally
expensive. In this paper, we introduce GP2S (Genetic Programming for Search
Strategy), a novel machine learning approach that automatically generates a
B\&B search strategy heuristic, aiming to make intelligent decisions while
being computationally lightweight. We define a policy as a function that
evaluates the quality of a B\&B node by combining features from the node and
the problem; the search strategy policy is then defined by a best-first search
based on this node ranking. The policy space is explored using a genetic
programming algorithm, and the policy that achieves the best performance on a
training set is selected. We compare our approach with the standard method of
the SCIP solver, a recent graph neural network-based method, and handcrafted
heuristics. Our first evaluation includes three types of primal hard problems,
tested on instances similar to the training set and on larger instances. Our
method is at most 2\% slower than the best baseline and consistently
outperforms SCIP, achieving an average speedup of 11.3\%. Additionally, GP2S is
tested on the MIPLIB 2017 dataset, generating multiple heuristics from
different subsets of instances. It exceeds SCIP's average performance in 7 out
of 10 cases across 15 times more instances and under a time limit 15 times
longer, with some GP2S methods leading on most experiments in terms of the
number of feasible solutions or optimality gap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MOS: Model Surgery for <span class="highlight-title">Pre-Train</span>ed Model-Based Class-Incremental
  Learning <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai-Long Sun, Da-Wei Zhou, Hanbin Zhao, Le Gan, De-Chuan Zhan, Han-Jia Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-Incremental Learning (CIL) requires models to continually acquire
knowledge of new classes without forgetting old ones. Despite Pre-trained
Models (PTMs) have shown excellent performance in CIL, catastrophic forgetting
still occurs as the model learns new concepts. Existing work seeks to utilize
lightweight components to adjust the PTM, while the forgetting phenomenon still
comes from {\em parameter and retrieval} levels. Specifically, iterative
updates of the model result in parameter drift, while mistakenly retrieving
irrelevant modules leads to the mismatch during inference. To this end, we
propose MOdel Surgery (MOS) to rescue the model from forgetting previous
knowledge. By training task-specific adapters, we continually adjust the PTM to
downstream tasks. To mitigate parameter-level forgetting, we present an adapter
merging approach to learn task-specific adapters, which aims to bridge the gap
between different components while reserve task-specific information. Besides,
to address retrieval-level forgetting, we introduce a training-free
self-refined adapter retrieval mechanism during inference, which leverages the
model's inherent ability for better adapter retrieval. By jointly rectifying
the model with those steps, MOS can robustly resist catastrophic forgetting in
the learning process. Extensive experiments on seven benchmark datasets
validate MOS's state-of-the-art performance. Code is available at:
https://github.com/sun-hailong/AAAI25-MOS
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2025. Code is available at:
  https://github.com/sun-hailong/AAAI25-MOS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Efficient Prediction of excited-state properties using Quantum
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Hagelüken, Marco F. Huber, Marco Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the properties of excited states of complex molecules is
crucial for many chemical and physical processes. Calculating these properties
is often significantly more resource-intensive than calculating their ground
state counterparts. We present a quantum machine learning model that predicts
excited-state properties from the molecular ground state for different
geometric configurations. The model comprises a symmetry-invariant quantum
neural network and a conventional neural network and is able to provide
accurate predictions with only a few training data points. The proposed
procedure is fully NISQ compatible. This is achieved by using a quantum circuit
that requires a number of parameters linearly proportional to the number of
molecular orbitals, along with a parameterized measurement observable, thereby
reducing the number of necessary measurements. We benchmark the algorithm on
three different molecules by evaluating its performance in predicting excited
state transition energies and transition dipole moments. We show that, in many
instances, the procedure is able to outperform various classical models that
rely solely on classical features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 + 4 pages, 7 + 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixture of neural fields for heterogeneous reconstruction in cryo-EM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Axel Levy, Rishwanth Raghu, David Shustin, Adele Rui-Yang Peng, Huan Li, Oliver Biggs Clarke, Gordon Wetzstein, Ellen D. Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cryo-electron microscopy (cryo-EM) is an experimental technique for protein
structure determination that images an ensemble of macromolecules in
near-physiological contexts. While recent advances enable the reconstruction of
dynamic conformations of a single biomolecular complex, current methods do not
adequately model samples with mixed conformational and compositional
heterogeneity. In particular, datasets containing mixtures of multiple proteins
require the joint inference of structure, pose, compositional class, and
conformational states for 3D reconstruction. Here, we present Hydra, an
approach that models both conformational and compositional heterogeneity fully
ab initio by parameterizing structures as arising from one of K neural fields.
We employ a new likelihood-based loss function and demonstrate the
effectiveness of our approach on synthetic datasets composed of mixtures of
proteins with large degrees of conformational variability. We additionally
demonstrate Hydra on an experimental dataset of a cellular lysate containing a
mixture of different protein complexes. Hydra expands the expressivity of
heterogeneous reconstruction methods and thus broadens the scope of cryo-EM to
increasingly complex samples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning Within the Classical Robotics Stack: A Case Study
  in Robot Soccer <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Labiosa, Zhihan Wang, Siddhant Agarwal, William Cong, Geethika Hemkumar, Abhinav Narayan Harish, Benjamin Hong, Josh Kelle, Chen Li, Yuhao Li, Zisen Shao, Peter Stone, Josiah P. Hanna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot decision-making in partially observable, real-time, dynamic, and
multi-agent environments remains a difficult and unsolved challenge. Model-free
reinforcement learning (RL) is a promising approach to learning decision-making
in such domains, however, end-to-end RL in complex environments is often
intractable. To address this challenge in the RoboCup Standard Platform League
(SPL) domain, we developed a novel architecture integrating RL within a
classical robotics stack, while employing a multi-fidelity sim2real approach
and decomposing behavior into learned sub-behaviors with heuristic selection.
Our architecture led to victory in the 2024 RoboCup SPL Challenge Shield
Division. In this work, we fully describe our system's architecture and
empirically analyze key design decisions that contributed to its success. Our
approach demonstrates how RL-based behaviors can be integrated into complete
robot behavior architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learned Compression for Compressed Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dan Jacobellis, Neeraja J. Yadwadkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern sensors produce increasingly rich streams of high-resolution data. Due
to resource constraints, machine learning systems discard the vast majority of
this information via resolution reduction. Compressed-domain learning allows
models to operate on compact latent representations, allowing higher effective
resolution for the same budget. However, existing compression systems are not
ideal for compressed learning. Linear transform coding and end-to-end learned
compression systems reduce bitrate, but do not uniformly reduce dimensionality;
thus, they do not meaningfully increase efficiency. Generative autoencoders
reduce dimensionality, but their adversarial or perceptual objectives lead to
significant information loss. To address these limitations, we introduce WaLLoC
(Wavelet Learned Lossy Compression), a neural codec architecture that combines
linear transform coding with nonlinear dimensionality-reducing autoencoders.
WaLLoC sandwiches a shallow, asymmetric autoencoder and entropy bottleneck
between an invertible wavelet packet transform. Across several key metrics,
WaLLoC outperforms the autoencoders used in state-of-the-art latent diffusion
models. WaLLoC does not require perceptual or adversarial losses to represent
high-frequency detail, providing compatibility with modalities beyond RGB
images and stereo audio. WaLLoC's encoder consists almost entirely of linear
operations, making it exceptionally efficient and suitable for mobile
computing, remote sensing, and learning directly from compressed data. We
demonstrate WaLLoC's capability for compressed-domain learning across several
tasks, including image classification, colorization, document understanding,
and music source separation. Our code, experiments, and pre-trained audio and
image codecs are available at https://ut-sysml.org/walloc
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as paper to 2025 IEEE Data Compression Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Opinion de-polarization of social networks with GNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantinos Mylonas, Thrasyvoulos Spyropoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, social media is the ground for political debate and exchange of
opinions. There is a significant amount of research that suggests that social
media are highly polarized. A phenomenon that is commonly observed is the echo
chamber structure, where users are organized in polarized communities and form
connections only with similar-minded individuals, limiting themselves to
consume specific content. In this paper we explore a way to decrease the
polarization of networks with two echo chambers. Particularly, we observe that
if some users adopt a moderate opinion about a topic, the polarization of the
network decreases. Based on this observation, we propose an efficient algorithm
to identify a good set of K users, such that if they adopt a moderate stance
around a topic, the polarization is minimized. Our algorithm employs a Graph
Neural Network and thus it can handle large graphs more effectively than other
approaches
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Geometry-Aware Message Passing Neural Network for Modeling
  Aerodynamics over Airfoils 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Helwig, Xuan Zhang, Haiyang Yu, Shuiwang Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational modeling of aerodynamics is a key problem in aerospace
engineering, often involving flows interacting with solid objects such as
airfoils. Deep surrogate models have emerged as purely data-driven approaches
that learn direct mappings from simulation conditions to solutions based on
either simulation or experimental data. Here, we consider modeling of
incompressible flows over solid objects, wherein geometric structures are a key
factor in determining aerodynamics. To effectively incorporate geometries, we
propose a message passing scheme that efficiently and expressively integrates
the airfoil shape with the mesh representation. Under this framework, we first
obtain a representation of the geometry in the form of a latent graph on the
airfoil surface. We subsequently propagate this representation to all
collocation points through message passing on a directed, bipartite graph. We
demonstrate that this framework supports efficient training by downsampling the
solution mesh while avoiding distribution shifts at test time when evaluated on
the full mesh. To enable our model to be able to distinguish between distinct
spatial regimes of dynamics relative to the airfoil, we represent mesh points
in both a leading edge and trailing edge coordinate system. We further enhance
the expressiveness of our coordinate system representations by embedding our
hybrid Polar-Cartesian coordinates using sinusoidal and spherical harmonics
bases. We additionally find that a change of basis to canonicalize input
representations with respect to inlet velocity substantially improves
generalization. Altogether, these design choices lead to a purely data-driven
machine learning framework known as GeoMPNN, which won the Best Student
Submission award at the NeurIPS 2024 ML4CFD Competition, placing 4th overall.
Our code is publicly available as part of the AIRS library
(https://github.com/divelab/AIRS).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Stage Segmentation and Cascade Classification Methods for
  Improving Cardiac MRI Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09386v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09386v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vitalii Slobodzian, Pavlo Radiuk, Oleksander Barmak, Iurii Krak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The segmentation and classification of cardiac magnetic resonance imaging are
critical for diagnosing heart conditions, yet current approaches face
challenges in accuracy and generalizability. In this study, we aim to further
advance the segmentation and classification of cardiac magnetic resonance
images by introducing a novel deep learning-based approach. Using a multi-stage
process with U-Net and ResNet models for segmentation, followed by Gaussian
smoothing, the method improved segmentation accuracy, achieving a Dice
coefficient of 0.974 for the left ventricle and 0.947 for the right ventricle.
For classification, a cascade of deep learning classifiers was employed to
distinguish heart conditions, including hypertrophic cardiomyopathy, myocardial
infarction, and dilated cardiomyopathy, achieving an average accuracy of 97.2%.
The proposed approach outperformed existing models, enhancing segmentation
accuracy and classification precision. These advancements show promise for
clinical applications, though further validation and interpretation across
diverse imaging protocols is necessary.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Cardiac MRI, heart pathology, deep learning, segmentation, Gaussian
  smoothing, classification, cascade</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Model with Representation Alignment for Protein Inverse
  Folding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09380v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09380v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenglin Wang, Yucheng Zhou, Zijie Zhai, Jianbing Shen, Kai Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protein inverse folding is a fundamental problem in bioinformatics, aiming to
recover the amino acid sequences from a given protein backbone structure.
Despite the success of existing methods, they struggle to fully capture the
intricate inter-residue relationships critical for accurate sequence
prediction. We propose a novel method that leverages diffusion models with
representation alignment (DMRA), which enhances diffusion-based inverse folding
by (1) proposing a shared center that aggregates contextual information from
the entire protein structure and selectively distributes it to each residue;
and (2) aligning noisy hidden representations with clean semantic
representations during the denoising process. This is achieved by predefined
semantic representations for amino acid types and a representation alignment
method that utilizes type embeddings as semantic feedback to normalize each
residue. In experiments, we conduct extensive evaluations on the CATH4.2
dataset to demonstrate that DMRA outperforms leading methods, achieving
state-of-the-art performance and exhibiting strong generalization capabilities
on the TS50 and TS500 datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid variable spiking graph neural networks for energy-efficient
  scientific machine learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isha Jain, Shailesh Garg, Shaurya Shriyam, Souvik Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph-based representations for samples of computational mechanics-related
datasets can prove instrumental when dealing with problems like irregular
domains or molecular structures of materials, etc. To effectively analyze and
process such datasets, deep learning offers Graph Neural Networks (GNNs) that
utilize techniques like message-passing within their architecture. The issue,
however, is that as the individual graph scales and/ or GNN architecture
becomes increasingly complex, the increased energy budget of the overall deep
learning model makes it unsustainable and restricts its applications in
applications like edge computing. To overcome this, we propose in this paper
Hybrid Variable Spiking Graph Neural Networks (HVS-GNNs) that utilize Variable
Spiking Neurons (VSNs) within their architecture to promote sparse
communication and hence reduce the overall energy budget. VSNs, while promoting
sparse event-driven computations, also perform well for regression tasks, which
are often encountered in computational mechanics applications and are the main
target of this paper. Three examples dealing with prediction of mechanical
properties of material based on microscale/ mesoscale structures are shown to
test the performance of the proposed HVS-GNNs in regression tasks. We have also
compared the performance of HVS-GNN architectures with the performance of
vanilla GNNs and GNNs utilizing leaky integrate and fire neurons. The results
produced show that HVS-GNNs perform well for regression tasks, all while
promoting sparse communication and, hence, energy efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A comprehensive interpretable machine learning framework for Mild
  Cognitive Impairment and Alzheimer's disease diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Eleftheria Vlontzou, Maria Athanasiou, Kalliopi Dalakleidi, Ioanna Skampardoni, Christos Davatzikos, Konstantina Nikita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An interpretable machine learning (ML) framework is introduced to enhance the
diagnosis of Mild Cognitive Impairment (MCI) and Alzheimer's disease (AD) by
ensuring robustness of the ML models' interpretations. The dataset used
comprises volumetric measurements from brain MRI and genetic data from healthy
individuals and patients with MCI/AD, obtained through the Alzheimer's Disease
Neuroimaging Initiative. The existing class imbalance is addressed by an
ensemble learning approach, while various attribution-based and
counterfactual-based interpretability methods are leveraged towards producing
diverse explanations related to the pathophysiology of MCI/AD. A unification
method combining SHAP with counterfactual explanations assesses the
interpretability techniques' robustness. The best performing model yielded
87.5% balanced accuracy and 90.8% F1-score. The attribution-based
interpretability methods highlighted significant volumetric and genetic
features related to MCI/AD risk. The unification method provided useful
insights regarding those features' necessity and sufficiency, further
showcasing their significance in MCI/AD diagnosis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint has not been peer-reviewed yet but has been submitted
  to a journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distribution free uncertainty quantification in neuroscience-inspired
  deep operators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shailesh Garg, Souvik Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Energy-efficient deep learning algorithms are essential for a sustainable
future and feasible edge computing setups. Spiking neural networks (SNNs),
inspired from neuroscience, are a positive step in the direction of achieving
the required energy efficiency. However, in a bid to lower the energy
requirements, accuracy is marginally sacrificed. Hence, predictions of such
deep learning algorithms require an uncertainty measure that can inform users
regarding the bounds of a certain output. In this paper, we introduce the
Conformalized Randomized Prior Operator (CRP-O) framework that leverages
Randomized Prior (RP) networks and Split Conformal Prediction (SCP) to quantify
uncertainty in both conventional and spiking neural operators. To further
enable zero-shot super-resolution in UQ, we propose an extension incorporating
Gaussian Process Regression. This enhanced super-resolution-enabled CRP-O
framework is integrated with the recently developed Variable Spiking Wavelet
Neural Operator (VSWNO). To test the performance of the obtained calibrated
uncertainty bounds, we discuss four different examples covering both
one-dimensional and two-dimensional partial differential equations. Results
demonstrate that the uncertainty bounds produced by the conformalized RP-VSWNO
significantly enhance UQ estimates compared to vanilla RP-VSWNO, Quantile WNO
(Q-WNO), and Conformalized Quantile WNO (CQ-WNO). These findings underscore the
potential of the proposed approach for practical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantitative Evaluation of Motif Sets in Time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daan Van Wesenbeeck, Aras Yurtman, Wannes Meert, Hendrik Blockeel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time Series Motif Discovery (TSMD), which aims at finding recurring patterns
in time series, is an important task in numerous application domains, and many
methods for this task exist. These methods are usually evaluated qualitatively.
A few metrics for quantitative evaluation, where discovered motifs are compared
to some ground truth, have been proposed, but they typically make implicit
assumptions that limit their applicability. This paper introduces PROM, a
broadly applicable metric that overcomes those limitations, and TSMD-Bench, a
benchmark for quantitative evaluation of time series motif discovery.
Experiments with PROM and TSMD-Bench show that PROM provides a more
comprehensive evaluation than existing metrics, that TSMD-Bench is a more
challenging benchmark than earlier ones, and that the combination can help
understand the relative performance of TSMD methods. More generally, the
proposed approach enables large-scale, systematic performance comparisons in
this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Predictive Control with Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09342v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09342v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ralf Römer, Alexander von Rohr, Angela P. Schoellig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently gained popularity for policy learning in
robotics due to their ability to capture high-dimensional and multimodal
distributions. However, diffusion policies are inherently stochastic and
typically trained offline, limiting their ability to handle unseen and dynamic
conditions where novel constraints not represented in the training data must be
satisfied. To overcome this limitation, we propose diffusion predictive control
with constraints (DPCC), an algorithm for diffusion-based control with explicit
state and action constraints that can deviate from those in the training data.
DPCC uses constraint tightening and incorporates model-based projections into
the denoising process of a trained trajectory diffusion model. This allows us
to generate constraint-satisfying, dynamically feasible, and goal-reaching
trajectories for predictive control. We show through simulations of a robot
manipulator that DPCC outperforms existing methods in satisfying novel
test-time constraints while maintaining performance on the learned control
task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/ralfroemer99/dpcc. 14 pages, 3 figures, 3
  tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Auto-Regressive Moving Diffusion Models for Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Gao, Qinglong Cao, Yuntian Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series forecasting (TSF) is essential in various domains, and recent
advancements in diffusion-based TSF models have shown considerable promise.
However, these models typically adopt traditional diffusion patterns, treating
TSF as a noise-based conditional generation task. This approach neglects the
inherent continuous sequential nature of time series, leading to a fundamental
misalignment between diffusion mechanisms and the TSF objective, thereby
severely impairing performance. To bridge this misalignment, and inspired by
the classic Auto-Regressive Moving Average (ARMA) theory, which views time
series as continuous sequential progressions evolving from previous data
points, we propose a novel Auto-Regressive Moving Diffusion (ARMD) model to
first achieve the continuous sequential diffusion-based TSF. Unlike previous
methods that start from white Gaussian noise, our model employs chain-based
diffusion with priors, accurately modeling the evolution of time series and
leveraging intermediate state information to improve forecasting accuracy and
stability. Specifically, our approach reinterprets the diffusion process by
considering future series as the initial state and historical series as the
final state, with intermediate series generated using a sliding-based technique
during the forward process. This design aligns the diffusion model's sampling
procedure with the forecasting objective, resulting in an unconditional,
continuous sequential diffusion TSF model. Extensive experiments conducted on
seven widely used datasets demonstrate that our model achieves state-of-the-art
performance, significantly outperforming existing diffusion-based TSF models.
Our code is available on GitHub: https://github.com/daxin007/ARMD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>no comment</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic <span class="highlight-title">Prompt</span> Allocation and Tuning for Continual Test-Time Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoran Cui, Yongrui Zhen, Shuai Gong, Chunyun Zhang, Hui Liu, Yilong Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual test-time adaptation (CTTA) has recently emerged to adapt a
pre-trained source model to continuously evolving target distributions, which
accommodates the dynamic nature of real-world environments. To mitigate the
risk of catastrophic forgetting in CTTA, existing methods typically incorporate
explicit regularization terms to constrain the variation of model parameters.
However, they cannot fundamentally resolve catastrophic forgetting because they
rely on a single shared model to adapt across all target domains, which
inevitably leads to severe inter-domain interference. In this paper, we
introduce learnable domain-specific prompts that guide the model to adapt to
corresponding target domains, thereby partially disentangling the parameter
space of different domains. In the absence of domain identity for target
samples, we propose a novel dynamic Prompt AllocatIon aNd Tuning (PAINT)
method, which utilizes a query mechanism to dynamically determine whether the
current samples come from a known domain or an unexplored one. For known
domains, the corresponding domain-specific prompt is directly selected, while
for previously unseen domains, a new prompt is allocated. Prompt tuning is
subsequently performed using mutual information maximization along with
structural regularization. Extensive experiments on three benchmark datasets
demonstrate the effectiveness of our PAINT method for CTTA. We have released
our code at https://github.com/Cadezzyr/PAINT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 5 figures, and 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transfer Learning of RSSI to Improve Indoor Localisation Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thanaphon Suwannaphong, Ryan McConville, Ian Craddock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing demand for health monitoring systems, in-home localisation
is essential for tracking patient conditions. The unique spatial
characteristics of each house required annotated data for Bluetooth Low Energy
(BLE) Received Signal Strength Indicator (RSSI)-based monitoring system.
However, collecting annotated training data is time-consuming, particularly for
patients with limited health conditions. To address this, we propose
Conditional Generative Adversarial Networks (ConGAN)-based augmentation,
combined with our transfer learning framework (T-ConGAN), to enable the
transfer of generic RSSI information between different homes, even when data is
collected using different experimental protocols. This enhances the performance
and scalability of such intelligent systems by reducing the need for annotation
in each home. We are the first to demonstrate that BLE RSSI data can be shared
across different homes, and that shared information can improve the indoor
localisation performance. Our T-ConGAN enhances the macro F1 score of
room-level indoor localisation by up to 12.2%, with a remarkable 51%
improvement in challenging areas such as stairways or outside spaces. This
state-of-the-art RSSI augmentation model significantly enhances the robustness
of in-home health monitoring systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimising TinyML with Quantization and Distillation of <span class="highlight-title">Transformer</span> and
  Mamba Models for Indoor Localisation on Edge Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09289v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09289v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thanaphon Suwannaphong, Ferdian Jovan, Ian Craddock, Ryan McConville
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes small and efficient machine learning models (TinyML) for
resource-constrained edge devices, specifically for on-device indoor
localisation. Typical approaches for indoor localisation rely on centralised
remote processing of data transmitted from lower powered devices such as
wearables. However, there are several benefits for moving this to the edge
device itself, including increased battery life, enhanced privacy, reduced
latency and lowered operational costs, all of which are key for common
applications such as health monitoring. The work focuses on model compression
techniques, including quantization and knowledge distillation, to significantly
reduce the model size while maintaining high predictive performance. We base
our work on a large state-of-the-art transformer-based model and seek to deploy
it within low-power MCUs. We also propose a state-space-based architecture
using Mamba as a more compact alternative to the transformer. Our results show
that the quantized transformer model performs well within a 64 KB RAM
constraint, achieving an effective balance between model size and localisation
precision. Additionally, the compact Mamba model has strong performance under
even tighter constraints, such as a 32 KB of RAM, without the need for model
compression, making it a viable option for more resource-limited environments.
We demonstrate that, through our framework, it is feasible to deploy advanced
indoor localisation models onto low-power MCUs with restricted memory
limitations. The application of these TinyML models in healthcare has the
potential to revolutionize patient monitoring by providing accurate, real-time
location data while minimizing power consumption, increasing data privacy,
improving latency and reducing infrastructure costs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Novel Skills from Language-Generated Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ao-Qun Jin, Tian-Yu Xiang, Xiao-Hu Zhou, Mei-Jiang Gui, Xiao-Liang Xie, Shi-Qi Liu, Shuang-Yi Wang, Yue Cao, Sheng-Bin Duan, Fu-Chao Xie, Zeng-Guang Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current robot learning algorithms for acquiring novel skills often rely on
demonstration datasets or environment interactions, resulting in high labor
costs and potential safety risks. To address these challenges, this study
proposes a skill-learning framework that enables robots to acquire novel skills
from natural language instructions. The proposed pipeline leverages
vision-language models to generate demonstration videos of novel skills, which
are processed by an inverse dynamics model to extract actions from the
unlabeled demonstrations. These actions are subsequently mapped to
environmental contexts via imitation learning, enabling robots to learn new
skills effectively. Experimental evaluations in the MetaWorld simulation
environments demonstrate the pipeline's capability to generate high-fidelity
and reliable demonstrations. Using the generated demonstrations, various skill
learning algorithms achieve an accomplishment rate three times the original on
novel tasks. These results highlight a novel approach to robot learning,
offering a foundation for the intuitive and intelligent acquisition of novel
robotic skills.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CRVQ: Channel-relaxed Vector Quantization for Extreme Compression of
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhuang Xu, Shiyu Ji, Qingfu Zhu, Wanxiang Che
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Powerful large language models (LLMs) are increasingly expected to be
deployed with lower computational costs, enabling their capabilities on
resource-constrained devices. Post-training quantization (PTQ) has emerged as a
star approach to achieve this ambition, with best methods compressing weights
to less than 2 bit on average. In this paper, we propose Channel-Relaxed Vector
Quantization (CRVQ), a novel technique that significantly improves the
performance of PTQ baselines at the cost of only minimal additional bits. This
state-of-the-art extreme compression method achieves its results through two
key innovations: (1) carefully selecting and reordering a very small subset of
critical weight channels, and (2) leveraging multiple codebooks to relax the
constraint of critical channels. With our method, we demonstrate a 38.9%
improvement over the current strongest sub-2-bit PTQ baseline, enabling nearer
lossless 1-bit compression. Furthermore, our approach offers flexible
customization of quantization bit-width and performance, providing a wider
range of deployment options for diverse hardware platforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Score and Distribution Matching Policy: Advanced Accelerated Visuomotor
  Policies via Matched Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bofang Jia, Pengxiang Ding, Can Cui, Mingyang Sun, Pengfang Qian, Zhaoxin Fan, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual-motor policy learning has advanced with architectures like
diffusion-based policies, known for modeling complex robotic trajectories.
However, their prolonged inference times hinder high-frequency control tasks
requiring real-time feedback. While consistency distillation (CD) accelerates
inference, it introduces errors that compromise action quality. To address
these limitations, we propose the Score and Distribution Matching Policy (SDM
Policy), which transforms diffusion-based policies into single-step generators
through a two-stage optimization process: score matching ensures alignment with
true action distributions, and distribution matching minimizes KL divergence
for consistency. A dual-teacher mechanism integrates a frozen teacher for
stability and an unfrozen teacher for adversarial training, enhancing
robustness and alignment with target distributions. Evaluated on a 57-task
simulation benchmark, SDM Policy achieves a 6x inference speedup while having
state-of-the-art action quality, providing an efficient and reliable framework
for high-frequency robotic tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single-View Graph Contrastive Learning with Soft Neighborhood Awareness <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingqiang Sun, Chaoqi Chen, Ziyue Qiao, Xubin Zheng, Kai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most graph contrastive learning (GCL) methods heavily rely on cross-view
contrast, thus facing several concomitant challenges, such as the complexity of
designing effective augmentations, the potential for information loss between
views, and increased computational costs. To mitigate reliance on cross-view
contrasts, we propose \ttt{SIGNA}, a novel single-view graph contrastive
learning framework. Regarding the inconsistency between structural connection
and semantic similarity of neighborhoods, we resort to soft neighborhood
awareness for GCL. Specifically, we leverage dropout to obtain
structurally-related yet randomly-noised embedding pairs for neighbors, which
serve as potential positive samples. At each epoch, the role of partial
neighbors is switched from positive to negative, leading to probabilistic
neighborhood contrastive learning effect. Furthermore, we propose a normalized
Jensen-Shannon divergence estimator for a better effect of contrastive
learning. Surprisingly, experiments on diverse node-level tasks demonstrate
that our simple single-view GCL framework consistently outperforms existing
methods by margins of up to 21.74% (PPI). In particular, with soft neighborhood
awareness, SIGNA can adopt MLPs instead of complicated GCNs as the encoder to
generate representations in transductive learning tasks, thus speeding up its
inference process by 109 times to 331 times. The source code is available at
https://github.com/sunisfighting/SIGNA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025; full version including appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Can Memorization Improve Fairness? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bob Pepin, Christian Igel, Raghavendra Selvan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study to which extent additive fairness metrics (statistical parity, equal
opportunity and equalized odds) can be influenced in a multi-class
classification problem by memorizing a subset of the population. We give
explicit expressions for the bias resulting from memorization in terms of the
label and group membership distribution of the memorized dataset and the
classifier bias on the unmemorized dataset. We also characterize the memorized
datasets that eliminate the bias for all three metrics considered. Finally we
provide upper and lower bounds on the total probability mass in the memorized
dataset that is necessary for the complete elimination of these biases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeLoRA: Geometric Adaptive Ranks For Efficient LoRA Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdessalam Ed-dib, Zhanibek Datbayev, Amine Mohamed Aboussalah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large language models (LLMs) is computationally intensive because
it requires updating all parameters. Low-Rank Adaptation (LoRA) improves
efficiency by modifying only a subset of weights but introduces a trade-off
between expressivity and computational cost: lower ranks reduce resources but
limit expressiveness, while higher ranks enhance expressivity at increased
cost. Despite recent advances in adaptive LoRA techniques, existing methods
fail to provide a theoretical basis for optimizing the trade-off between model
performance and efficiency. We propose Geometric Low-Rank Adaptation (GeLoRA),
a novel framework that computes the intrinsic dimensionality of hidden state
representations to adaptively select LoRA ranks. We demonstrate that the
intrinsic dimension provides a lower bound for the optimal rank of LoRA
matrices, allowing for a principled selection that balances efficiency and
expressivity. GeLoRA dynamically adjusts the rank for each layer based on the
intrinsic dimensionality of its input and output representations, recognizing
that not all model parameters equally impact fine-tuning. Empirical validation
on multiple tasks shows that GeLoRA consistently outperforms recent baselines
within the same parameter budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uplift modeling with continuous treatments: A predict-then-optimize
  approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon De Vos, Christopher Bockel-Rickermann, Stefan Lessmann, Wouter Verbeke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of uplift modeling is to recommend actions that optimize specific
outcomes by determining which entities should receive treatment. One common
approach involves two steps: first, an inference step that estimates
conditional average treatment effects (CATEs), and second, an optimization step
that ranks entities based on their CATE values and assigns treatment to the top
k within a given budget. While uplift modeling typically focuses on binary
treatments, many real-world applications are characterized by continuous-valued
treatments, i.e., a treatment dose. This paper presents a predict-then-optimize
framework to allow for continuous treatments in uplift modeling. First, in the
inference step, conditional average dose responses (CADRs) are estimated from
data using causal machine learning techniques. Second, in the optimization
step, we frame the assignment task of continuous treatments as a
dose-allocation problem and solve it using integer linear programming (ILP).
This approach allows decision-makers to efficiently and effectively allocate
treatment doses while balancing resource availability, with the possibility of
adding extra constraints like fairness considerations or adapting the objective
function to take into account instance-dependent costs and benefits to maximize
utility. The experiments compare several CADR estimators and illustrate the
trade-offs between policy value and fairness, as well as the impact of an
adapted objective function. This showcases the framework's advantages and
flexibility across diverse applications in healthcare, lending, and human
resource management. All code is available on github.com/SimonDeVos/UMCT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Generation and Removal of Speaker Adversarial Perturbation for
  Voice-Privacy Protection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Guo, Liping Chen, Zhuhai Li, Kong Aik Lee, Zhen-Hua Ling, Wu Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks are commonly known to be vulnerable to adversarial attacks
mounted through subtle perturbation on the input data. Recent development in
voice-privacy protection has shown the positive use cases of the same technique
to conceal speaker's voice attribute with additive perturbation signal
generated by an adversarial network. This paper examines the reversibility
property where an entity generating the adversarial perturbations is authorized
to remove them and restore original speech (e.g., the speaker him/herself). A
similar technique could also be used by an investigator to deanonymize a
voice-protected speech to restore criminals' identities in security and
forensic analysis. In this setting, the perturbation generative module is
assumed to be known in the removal process. To this end, a joint training of
perturbation generation and removal modules is proposed. Experimental results
on the LibriSpeech dataset demonstrated that the subtle perturbations added to
the original speech can be predicted from the anonymized speech while achieving
the goal of privacy protection. By removing these perturbations from the
anonymized sample, the original speech can be restored. Audio samples can be
found in \url{https://voiceprivacy.github.io/Perturbation-Generation-Removal/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, published to IEEE SLT Workshop 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dimensionality Reduction Techniques for Global Bayesian Optimisation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luo Long, Coralia Cartis, Paz Fink Shustin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian Optimisation (BO) is a state-of-the-art global optimisation
technique for black-box problems where derivative information is unavailable,
and sample efficiency is crucial. However, improving the general scalability of
BO has proved challenging. Here, we explore Latent Space Bayesian Optimisation
(LSBO), that applies dimensionality reduction to perform BO in a
reduced-dimensional subspace. While early LSBO methods used (linear) random
projections (Wang et al., 2013), we employ Variational Autoencoders (VAEs) to
manage more complex data structures and general DR tasks. Building on Grosnit
et. al. (2021), we analyse the VAE-based LSBO framework, focusing on VAE
retraining and deep metric loss. We suggest a few key corrections in their
implementation, originally designed for tasks such as molecule generation, and
reformulate the algorithm for broader optimisation purposes. Our numerical
results show that structured latent manifolds improve BO performance.
Additionally, we examine the use of the Mat\'{e}rn-$\frac{5}{2}$ kernel for
Gaussian Processes in this LSBO context. We also integrate Sequential Domain
Reduction (SDR), a standard global optimization efficiency strategy, into BO.
SDR is included in a GPU-based environment using \textit{BoTorch}, both in the
original and VAE-generated latent spaces, marking the first application of SDR
within LSBO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024 Workshop OPT for ML: Optimization for
  Machine Learning (Submission Number:67)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $(ε, δ)$-Differentially Private Partial Least Squares
  Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ramin Nikzad-Langerodi, Mohit Kumar, Du Nguyen Duy, Mahtab Alghasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As data-privacy requirements are becoming increasingly stringent and
statistical models based on sensitive data are being deployed and used more
routinely, protecting data-privacy becomes pivotal. Partial Least Squares (PLS)
regression is the premier tool for building such models in analytical
chemistry, yet it does not inherently provide privacy guarantees, leaving
sensitive (training) data vulnerable to privacy attacks. To address this gap,
we propose an $(\epsilon, \delta)$-differentially private PLS (edPLS)
algorithm, which integrates well-studied and theoretically motivated Gaussian
noise-adding mechanisms into the PLS algorithm to ensure the privacy of the
data underlying the model. Our approach involves adding carefully calibrated
Gaussian noise to the outputs of four key functions in the PLS algorithm: the
weights, scores, $X$-loadings, and $Y$-loadings. The noise variance is
determined based on the global sensitivity of each function, ensuring that the
privacy loss is controlled according to the $(\epsilon, \delta)$-differential
privacy framework. Specifically, we derive the sensitivity bounds for each
function and use these bounds to calibrate the noise added to the model
components. Experimental results demonstrate that edPLS effectively renders
privacy attacks, aimed at recovering unique sources of variability in the
training data, ineffective. Application of edPLS to the NIR corn benchmark
dataset shows that the root mean squared error of prediction (RMSEP) remains
competitive even at strong privacy levels (i.e., $\epsilon=1$), given proper
pre-processing of the corresponding spectra. These findings highlight the
practical utility of edPLS in creating privacy-preserving multivariate
calibrations and for the analysis of their privacy-utility trade-offs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Adversarial Attacks on Traffic Sign Classifiers beyond
  Standard Baselines <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Svetlana Pavlitska, Leopold Müller, J. Marius Zöllner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks on traffic sign classification models were among the
first successfully tried in the real world. Since then, the research in this
area has been mainly restricted to repeating baseline models, such as LISA-CNN
or GTSRB-CNN, and similar experiment settings, including white and black
patches on traffic signs. In this work, we decouple model architectures from
the datasets and evaluate on further generic models to make a fair comparison.
Furthermore, we compare two attack settings, inconspicuous and visible, which
are usually regarded without direct comparison. Our results show that standard
baselines like LISA-CNN or GTSRB-CNN are significantly more susceptible than
the generic ones. We, therefore, suggest evaluating new attacks on a broader
spectrum of baselines in the future. Our code is available at
\url{https://github.com/KASTEL-MobilityLab/attacks-on-traffic-sign-recognition/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at ICMLA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Student-Informed Teacher Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nico Messikommer, Jiaxu Xing, Elie Aljalbout, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning with a privileged teacher has proven effective for
learning complex control behaviors from high-dimensional inputs, such as
images. In this framework, a teacher is trained with privileged task
information, while a student tries to predict the actions of the teacher with
more limited observations, e.g., in a robot navigation task, the teacher might
have access to distances to nearby obstacles, while the student only receives
visual observations of the scene. However, privileged imitation learning faces
a key challenge: the student might be unable to imitate the teacher's behavior
due to partial observability. This problem arises because the teacher is
trained without considering if the student is capable of imitating the learned
behavior. To address this teacher-student asymmetry, we propose a framework for
joint training of the teacher and student policies, encouraging the teacher to
learn behaviors that can be imitated by the student despite the latters'
limited access to information and its partial observability. Based on the
performance bound in imitation learning, we add (i) the approximated action
difference between teacher and student as a penalty term to the reward function
of the teacher, and (ii) a supervised teacher-student alignment step. We
motivate our method with a maze navigation task and demonstrate its
effectiveness on complex vision-based quadrotor flight and manipulation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Brief Discussion on KPI Development in Public Administration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simona Fioretto, Elio Masciari, Enea Vincenzo Napolitano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient and effective service delivery in Public Administration (PA) relies
on the development and utilization of key performance indicators (KPIs) for
evaluating and measuring performance. This paper presents an innovative
framework for KPI construction within performance evaluation systems,
leveraging Random Forest algorithms and variable importance analysis. The
proposed approach identifies key variables that significantly influence PA
performance, offering valuable insights into the critical factors driving
organizational success. By integrating variable importance analysis with expert
consultation, relevant KPIs can be systematically developed, ensuring that
improvement strategies address performance-critical areas. The framework
incorporates continuous monitoring mechanisms and adaptive phases to refine
KPIs in response to evolving administrative needs. This study aims to enhance
PA performance through the application of machine learning techniques,
fostering a more agile and results-driven approach to public administration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Modality Representation and Alignment for Multimodal
  Cold-start Active Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Shen, Yake Wei, Jianxiong Yin, Deepu Rajan, Di Hu, Simon See
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training multimodal models requires a large amount of labeled data. Active
learning (AL) aim to reduce labeling costs. Most AL methods employ warm-start
approaches, which rely on sufficient labeled data to train a well-calibrated
model that can assess the uncertainty and diversity of unlabeled data. However,
when assembling a dataset, labeled data are often scarce initially, leading to
a cold-start problem. Additionally, most AL methods seldom address multimodal
data, highlighting a research gap in this field. Our research addresses these
issues by developing a two-stage method for Multi-Modal Cold-Start Active
Learning (MMCSAL).
  Firstly, we observe the modality gap, a significant distance between the
centroids of representations from different modalities, when only using
cross-modal pairing information as self-supervision signals. This modality gap
affects data selection process, as we calculate both uni-modal and cross-modal
distances. To address this, we introduce uni-modal prototypes to bridge the
modality gap. Secondly, conventional AL methods often falter in multimodal
scenarios where alignment between modalities is overlooked. Therefore, we
propose enhancing cross-modal alignment through regularization, thereby
improving the quality of selected multimodal data pairs in AL. Finally, our
experiments demonstrate MMCSAL's efficacy in selecting multimodal data pairs
across three multimodal datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, ACMMM Asia 2024, Oral Presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMD-OPT : Maximum Mean Discrepancy Based Sample Efficient Collision Risk
  Minimization for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09121v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09121v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Basant Sharma, Arun Kumar Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose MMD-OPT: a sample-efficient approach for minimizing the risk of
collision under arbitrary prediction distribution of the dynamic obstacles.
MMD-OPT is based on embedding distribution in Reproducing Kernel Hilbert Space
(RKHS) and the associated Maximum Mean Discrepancy (MMD). We show how these two
concepts can be used to define a sample efficient surrogate for collision risk
estimate. We perform extensive simulations to validate the effectiveness of
MMD-OPT on both synthetic and real-world datasets. Importantly, we show that
trajectory optimization with our MMD-based collision risk surrogate leads to
safer trajectories at low sample regimes than popular alternatives based on
Conditional Value at Risk (CVaR).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Utility and Complexity of In- and Out-of-Distribution Machine
  Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youssef Allouah, Joshua Kazdan, Rachid Guerraoui, Sanmi Koyejo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine unlearning, the process of selectively removing data from trained
models, is increasingly crucial for addressing privacy concerns and knowledge
gaps post-deployment. Despite this importance, existing approaches are often
heuristic and lack formal guarantees. In this paper, we analyze the fundamental
utility, time, and space complexity trade-offs of approximate unlearning,
providing rigorous certification analogous to differential privacy. For
in-distribution forget data -- data similar to the retain set -- we show that a
surprisingly simple and general procedure, empirical risk minimization with
output perturbation, achieves tight unlearning-utility-complexity trade-offs,
addressing a previous theoretical gap on the separation from unlearning "for
free" via differential privacy, which inherently facilitates the removal of
such data. However, such techniques fail with out-of-distribution forget data
-- data significantly different from the retain set -- where unlearning time
complexity can exceed that of retraining, even for a single sample. To address
this, we propose a new robust and noisy gradient descent variant that provably
amortizes unlearning time complexity without compromising utility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Algorithm-Centered Approach To Model Streaming Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Hinder, Valerie Vaquet, David Komnick, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Besides the classical offline setup of machine learning, stream learning
constitutes a well-established setup where data arrives over time in
potentially non-stationary environments. Concept drift, the phenomenon that the
underlying distribution changes over time poses a significant challenge. Yet,
despite high practical relevance, there is little to no foundational theory for
learning in the drifting setup comparable to classical statistical learning
theory in the offline setting. This can be attributed to the lack of an
underlying object comparable to a probability distribution as in the classical
setup. While there exist approaches to transfer ideas to the streaming setup,
these start from a data perspective rather than an algorithmic one. In this
work, we suggest a new model of data over time that is aimed at the algorithm's
perspective. Instead of defining the setup using time points, we utilize a
window-based approach that resembles the inner workings of most stream learning
algorithms. We compare our framework to others from the literature on a
theoretical basis, showing that in many cases both model the same situation.
Furthermore, we perform a numerical evaluation and showcase an application in
the domain of critical infrastructure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript is currently under review at the Symposium on
  Intelligent Data Analysis (IDA 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How to Re-enable PDE Loss for Physical Systems Modeling Under Partial
  Observation <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haodong Feng, Yue Wang, Dixia Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In science and engineering, machine learning techniques are increasingly
successful in physical systems modeling (predicting future states of physical
systems). Effectively integrating PDE loss as a constraint of system transition
can improve the model's prediction by overcoming generalization issues due to
data scarcity, especially when data acquisition is costly. However, in many
real-world scenarios, due to sensor limitations, the data we can obtain is
often only partial observation, making the calculation of PDE loss seem to be
infeasible, as the PDE loss heavily relies on high-resolution states. We
carefully study this problem and propose a novel framework named Re-enable PDE
Loss under Partial Observation (RPLPO). The key idea is that although enabling
PDE loss to constrain system transition solely is infeasible, we can re-enable
PDE loss by reconstructing the learnable high-resolution state and constraining
system transition simultaneously. Specifically, RPLPO combines an encoding
module for reconstructing learnable high-resolution states with a transition
module for predicting future states. The two modules are jointly trained by
data and PDE loss. We conduct experiments in various physical systems to
demonstrate that RPLPO has significant improvement in generalization, even when
observation is sparse, irregular, noisy, and PDE is inaccurate. The code is
available on GitHub: RPLPO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision CNNs trained to estimate spatial latents learned similar
  ventral-stream-aligned representations <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09115v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09115v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yudi Xie, Weichen Huang, Esther Alter, Jeremy Schwartz, Joshua B. Tenenbaum, James J. DiCarlo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Studies of the functional role of the primate ventral visual stream have
traditionally focused on object categorization, often ignoring -- despite much
prior evidence -- its role in estimating "spatial" latents such as object
position and pose. Most leading ventral stream models are derived by optimizing
networks for object categorization, which seems to imply that the ventral
stream is also derived under such an objective. Here, we explore an alternative
hypothesis: Might the ventral stream be optimized for estimating spatial
latents? And a closely related question: How different -- if at all -- are
representations learned from spatial latent estimation compared to
categorization? To ask these questions, we leveraged synthetic image datasets
generated by a 3D graphic engine and trained convolutional neural networks
(CNNs) to estimate different combinations of spatial and category latents. We
found that models trained to estimate just a few spatial latents achieve neural
alignment scores comparable to those trained on hundreds of categories, and the
spatial latent performance of models strongly correlates with their neural
alignment. Spatial latent and category-trained models have very similar -- but
not identical -- internal representations, especially in their early and middle
layers. We provide evidence that this convergence is partly driven by
non-target latent variability in the training data, which facilitates the
implicit learning of representations of those non-target latents. Taken
together, these results suggest that many training objectives, such as spatial
latents, can lead to similar models aligned neurally with the ventral stream.
Thus, one should not assume that the ventral stream is optimized for object
categorization only. As a field, we need to continue to sharpen our measures of
comparing models to brains to better understand the functional roles of the
ventral stream.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 20 figures, ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-<span class="highlight-title">Dataset</span> Trajectory Return Regularization for Offline Preference-based
  Reinforcement Learning <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songjun Tu, Jingbo Sun, Qichao Zhang, Yaocheng Zhang, Jia Liu, Ke Chen, Dongbin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline preference-based reinforcement learning (PbRL) typically operates in
two phases: first, use human preferences to learn a reward model and annotate
rewards for a reward-free offline dataset; second, learn a policy by optimizing
the learned reward via offline RL. However, accurately modeling step-wise
rewards from trajectory-level preference feedback presents inherent challenges.
The reward bias introduced, particularly the overestimation of predicted
rewards, leads to optimistic trajectory stitching, which undermines the
pessimism mechanism critical to the offline RL phase. To address this
challenge, we propose In-Dataset Trajectory Return Regularization (DTR) for
offline PbRL, which leverages conditional sequence modeling to mitigate the
risk of learning inaccurate trajectory stitching under reward bias.
Specifically, DTR employs Decision Transformer and TD-Learning to strike a
balance between maintaining fidelity to the behavior policy with high
in-dataset trajectory returns and selecting optimal actions based on high
reward labels. Additionally, we introduce an ensemble normalization technique
that effectively integrates multiple reward models, balancing the tradeoff
between reward differentiation and accuracy. Empirical evaluations on various
benchmarks demonstrate the superiority of DTR over other state-of-the-art
baselines
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, Proceedings of the 39th AAAI Conference on Artificial
  Intelligence (AAAI-25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Filter-then-Generate: Large Language Models with Structure-Text Adapter
  for Knowledge Graph Completion <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Liu, Jihai Zhang, Fangquan Lin, Cheng Yang, Min Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) present massive inherent knowledge and superior
semantic comprehension capability, which have revolutionized various tasks in
natural language processing. Despite their success, a critical gap remains in
enabling LLMs to perform knowledge graph completion (KGC). Empirical evidence
suggests that LLMs consistently perform worse than conventional KGC approaches,
even through sophisticated prompt design or tailored instruction-tuning.
Fundamentally, applying LLMs on KGC introduces several critical challenges,
including a vast set of entity candidates, hallucination issue of LLMs, and
under-exploitation of the graph structure. To address these challenges, we
propose a novel instruction-tuning-based method, namely FtG. Specifically, we
present a \textit{filter-then-generate} paradigm and formulate the KGC task
into a multiple-choice question format. In this way, we can harness the
capability of LLMs while mitigating the issue casused by hallucinations.
Moreover, we devise a flexible ego-graph serialization prompt and employ a
structure-text adapter to couple structure and text information in a
contextualized manner. Experimental results demonstrate that FtG achieves
substantial performance gain compared to existing state-of-the-art methods. The
instruction dataset and code are available at
\url{https://github.com/LB0828/FtG}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2025 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrated trucks assignment and scheduling problem with mixed service
  mode docks: A Q-learning based adaptive large neighborhood search algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueyi Li, Mehrdad Mohammadi, Xiaodong Zhang, Yunxing Lan, Willem van Jaarsveld
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixed service mode docks enhance efficiency by flexibly handling both loading
and unloading trucks in warehouses. However, existing research often
predetermines the number and location of these docks prior to planning truck
assignment and sequencing. This paper proposes a new model integrating dock
mode decision, truck assignment, and scheduling, thus enabling adaptive dock
mode arrangements. Specifically, we introduce a Q-learning-based adaptive large
neighborhood search (Q-ALNS) algorithm to address the integrated problem. The
algorithm adjusts dock modes via perturbation operators, while truck assignment
and scheduling are solved using destroy and repair local search operators.
Q-learning adaptively selects these operators based on their performance
history and future gains, employing the epsilon-greedy strategy. Extensive
experimental results and statistical analysis indicate that the Q-ALNS benefits
from efficient operator combinations and its adaptive mechanism, consistently
outperforming benchmark algorithms in terms of optimality gap and Pareto front
discovery. In comparison to the predetermined service mode, our adaptive
strategy results in lower average tardiness and makespan, highlighting its
superior adaptability to varying demands.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 12 figures, 15 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Networks for Threshold Dynamics Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elisa Negrini, Almanzo Jiahe Gao, Abigail Bowering, Wei Zhu, Luca Capogna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce two convolutional neural network (CNN) architectures, inspired
by the Merriman-Bence-Osher (MBO) algorithm and by cellular automatons, to
model and learn threshold dynamics for front evolution from video data. The
first model, termed the (single-dynamics) MBO network, learns a specific kernel
and threshold for each input video without adapting to new dynamics, while the
second, a meta-learning MBO network, generalizes across diverse threshold
dynamics by adapting its parameters per input. Both models are evaluated on
synthetic and real-world videos (ice melting and fire front propagation), with
performance metrics indicating effective reconstruction and extrapolation of
evolving boundaries, even under noisy conditions. Empirical results highlight
the robustness of both networks across varied synthetic and real-world
dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Key words: threshold dynamics, cellular automaton, inverse problems,
  convolutional neural networks, deep learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SVasP: Self-Versatility Adversarial Style Perturbation for Cross-Domain
  Few-Shot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqian Li, Pengfei Fang, Hui Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-Domain Few-Shot Learning (CD-FSL) aims to transfer knowledge from seen
source domains to unseen target domains, which is crucial for evaluating the
generalization and robustness of models. Recent studies focus on utilizing
visual styles to bridge the domain gap between different domains. However, the
serious dilemma of gradient instability and local optimization problem occurs
in those style-based CD-FSL methods. This paper addresses these issues and
proposes a novel crop-global style perturbation method, called
\underline{\textbf{S}}elf-\underline{\textbf{V}}ersatility
\underline{\textbf{A}}dversarial \underline{\textbf{S}}tyle
\underline{\textbf{P}}erturbation (\textbf{SVasP}), which enhances the gradient
stability and escapes from poor sharp minima jointly. Specifically, SVasP
simulates more diverse potential target domain adversarial styles via
diversifying input patterns and aggregating localized crop style gradients, to
serve as global style perturbation stabilizers within one image, a concept we
refer to as self-versatility. Then a novel objective function is proposed to
maximize visual discrepancy while maintaining semantic consistency between
global, crop, and adversarial features. Having the stabilized global style
perturbation in the training phase, one can obtain a flattened minima in the
loss landscape, boosting the transferability of the model to the target
domains. Extensive experiments on multiple benchmark datasets demonstrate that
our method significantly outperforms existing state-of-the-art methods. Our
codes are available at https://github.com/liwenqianSEU/SVasP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-view Clustering via Unified Multi-kernel Learning and Matrix
  Factorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09065v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09065v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxing Jia, Mingjie Cai, Hamido Fujita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-view clustering has become increasingly important due to the
multi-source character of real-world data. Among existing multi-view clustering
methods, multi-kernel clustering and matrix factorization-based multi-view
clustering have gained widespread attention as mainstream approaches. However,
multi-kernel clustering tends to learn an optimal kernel and then perform
eigenvalue decomposition on it, which leads to high computational complexity.
Matrix factorization-based multi-view clustering methods impose orthogonal
constraints on individual views. This overly emphasizes the accuracy of
clustering structures within single views and restricts the learning of
individual views. Based on this analysis, we propose a multi-view clustering
method that integrates multi-kernel learning with matrix factorization. This
approach combines the advantages of both multi-kernel learning and matrix
factorization. It removes the orthogonal constraints on individual views and
imposes orthogonal constraints on the consensus matrix, resulting in an
accurate final clustering structure. Ultimately, the method is unified into a
simple form of multi-kernel clustering, but avoids learning an optimal kernel,
thus reducing the time complexity. Furthermore, we propose an efficient
three-step optimization algorithm to achieve a locally optimal solution.
Experiments on widely-used real-world datasets demonstrate the effectiveness of
our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Go With the Flow: Fast Diffusion for Gaussian Mixture Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Rapakoulias, Ali Reza Pedram, Panagiotis Tsiotras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Schr\"{o}dinger Bridges (SB) are diffusion processes that steer, in finite
time, a given initial distribution to another final one while minimizing a
suitable cost functional. Although various methods for computing SBs have
recently been proposed in the literature, most of these approaches require
computationally expensive training schemes, even for solving low-dimensional
problems. In this work, we propose an analytic parametrization of a set of
feasible policies for steering the distribution of a dynamical system from one
Gaussian Mixture Model (GMM) to another. Instead of relying on standard
non-convex optimization techniques, the optimal policy within the set can be
approximated as the solution of a low-dimensional linear program whose
dimension scales linearly with the number of components in each mixture.
Furthermore, our method generalizes naturally to more general classes of
dynamical systems such as controllable Linear Time-Varying systems that cannot
currently be solved using traditional neural SB approaches. We showcase the
potential of this approach in low-to-moderate dimensional problems such as
image-to-image translation in the latent space of an autoencoder, and various
other examples. We also benchmark our approach on an Entropic Optimal Transport
(EOT) problem and show that it outperforms state-of-the-art methods in cases
where the boundary distributions are mixture models while requiring virtually
no training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe Active Learning for Gaussian Differential Equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Glass, Katharina Ensinger, Christoph Zimmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian Process differential equations (GPODE) have recently gained momentum
due to their ability to capture dynamics behavior of systems and also represent
uncertainty in predictions. Prior work has described the process of training
the hyperparameters and, thereby, calibrating GPODE to data. How to design
efficient algorithms to collect data for training GPODE models is still an open
field of research. Nevertheless high-quality training data is key for model
performance. Furthermore, data collection leads to time-cost and financial-cost
and might in some areas even be safety critical to the system under test.
Therefore, algorithms for safe and efficient data collection are central for
building high quality GPODE models. Our novel Safe Active Learning (SAL) for
GPODE algorithm addresses this challenge by suggesting a mechanism to propose
efficient and non-safety-critical data to collect. SAL GPODE does so by
sequentially suggesting new data, measuring it and updating the GPODE model
with the new data. In this way, subsequent data points are iteratively
suggested. The core of our SAL GPODE algorithm is a constrained optimization
problem maximizing information of new data for GPODE model training constrained
by the safety of the underlying system. We demonstrate our novel SAL GPODE's
superiority compared to a standard, non-active way of measuring new data on two
relevant examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dial-In LLM: Human-Aligned Dialogue Intent Clustering with
  LLM-in-the-loop 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengze Hong, Yuanfeng Song, Di Jiang, Wailing Ng, Yanjie Sun, Chen Jason Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The discovery of customer intention from dialogue plays an important role in
automated support system. However, traditional text clustering methods are
poorly aligned with human perceptions due to the shift from embedding distance
to semantic distance, and existing quantitative metrics for text clustering may
not accurately reflect the true quality of intent clusters. In this paper, we
leverage the superior language understanding capabilities of Large Language
Models (LLMs) for designing better-calibrated intent clustering algorithms. We
first establish the foundation by verifying the robustness of fine-tuned LLM
utility in semantic coherence evaluation and cluster naming, resulting in an
accuracy of 97.50% and 94.40%, respectively, when compared to the human-labeled
ground truth. Then, we propose an iterative clustering algorithm that
facilitates cluster-level refinement and the continuous discovery of
high-quality intent clusters. Furthermore, we present several LLM-in-the-loop
semi-supervised clustering techniques tailored for intent discovery from
customer service dialogue. Experiments on a large-scale industrial dataset
comprising 1,507 intent clusters demonstrate the effectiveness of the proposed
techniques. The methods outperformed existing counterparts, achieving 6.25%
improvement in quantitative metrics and 12% enhancement in application-level
performance when constructing an intent classifier.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Confusion: A Fine-grained Dialectical Examination of Human
  Activity Recognition Benchmark <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Geissler, Dominique Nshimyimana, Vitor Fortes Rey, Sungho Suh, Bo Zhou, Paul Lukowicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The research of machine learning (ML) algorithms for human activity
recognition (HAR) has made significant progress with publicly available
datasets. However, most research prioritizes statistical metrics over examining
negative sample details. While recent models like transformers have been
applied to HAR datasets with limited success from the benchmark metrics, their
counterparts have effectively solved problems on similar levels with near 100%
accuracy. This raises questions about the limitations of current approaches.
This paper aims to address these open questions by conducting a fine-grained
inspection of six popular HAR benchmark datasets. We identified for some parts
of the data, none of the six chosen state-of-the-art ML methods can correctly
classify, denoted as the intersect of false classifications (IFC). Analysis of
the IFC reveals several underlying problems, including ambiguous annotations,
irregularities during recording execution, and misaligned transition periods.
We contribute to the field by quantifying and characterizing annotated data
ambiguities, providing a trinary categorization mask for dataset patching, and
stressing potential improvements for future data collections.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pulling the Carpet Below the Learner's Feet: Genetic Algorithm To Learn
  Ensemble Machine Learning Model During Concept Drift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teddy Lazebnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven models, in general, and machine learning (ML) models, in
particular, have gained popularity over recent years with an increased usage of
such models across the scientific and engineering domains. When using ML models
in realistic and dynamic environments, users need to often handle the challenge
of concept drift (CD). In this study, we explore the application of genetic
algorithms (GAs) to address the challenges posed by CD in such settings. We
propose a novel two-level ensemble ML model, which combines a global ML model
with a CD detector, operating as an aggregator for a population of ML pipeline
models, each one with an adjusted CD detector by itself responsible for
re-training its ML model. In addition, we show one can further improve the
proposed model by utilizing off-the-shelf automatic ML methods. Through
extensive synthetic dataset analysis, we show that the proposed model
outperforms a single ML pipeline with a CD algorithm, particularly in scenarios
with unknown CD characteristics. Overall, this study highlights the potential
of ensemble ML and CD models obtained through a heuristic and adaptive
optimization process such as the GA one to handle complex CD events.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RingFormer: A Ring-Enhanced Graph <span class="highlight-title">Transformer</span> for Organic Solar Cell
  Property Prediction <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Ding, Ting Zhang, Yiran Li, Jieming Shi, Chen Jason Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Organic Solar Cells (OSCs) are a promising technology for sustainable energy
production. However, the identification of molecules with desired OSC
properties typically involves laborious experimental research. To accelerate
progress in the field, it is crucial to develop machine learning models capable
of accurately predicting the properties of OSC molecules. While graph
representation learning has demonstrated success in molecular property
prediction, it remains underexplored for OSC-specific tasks. Existing methods
fail to capture the unique structural features of OSC molecules, particularly
the intricate ring systems that critically influence OSC properties, leading to
suboptimal performance. To fill the gap, we present RingFormer, a novel graph
transformer framework specially designed to capture both atom and ring level
structural patterns in OSC molecules. RingFormer constructs a hierarchical
graph that integrates atomic and ring structures and employs a combination of
local message passing and global attention mechanisms to generate expressive
graph representations for accurate OSC property prediction. We evaluate
RingFormer's effectiveness on five curated OSC molecule datasets through
extensive experiments. The results demonstrate that RingFormer consistently
outperforms existing methods, achieving a 22.77% relative improvement over the
nearest competitor on the CEPDB dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures. This is the extended version of the paper
  accepted at AAAI 2025, which includes all technical appendices and additional
  experimental details</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning and Current Prediction of PMSM Drive via Differential Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09028v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09028v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjie Mei, Xiaorui Wang, Yanrong Lu, Ke Yu, Shihua Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning models for dynamical systems in continuous time is significant for
understanding complex phenomena and making accurate predictions. This study
presents a novel approach utilizing differential neural networks (DNNs) to
model nonlinear systems, specifically permanent magnet synchronous motors
(PMSMs), and to predict their current trajectories. The efficacy of our
approach is validated through experiments conducted under various load
disturbances and no-load conditions. The results demonstrate that our method
effectively and accurately reconstructs the original systems, showcasing strong
short-term and long-term prediction capabilities and robustness. This study
provides valuable insights into learning the inherent dynamics of complex
dynamical data and holds potential for further applications in fields such as
weather forecasting, robotics, and collective behavior analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Physical Neural Networks for Analog In-Memory Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Sakemi, Yuji Okamoto, Takashi Morie, Sou Nobukawa, Takeo Hosomi, Kazuyuki Aihara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-memory computing (IMC) architectures mitigate the von Neumann bottleneck
encountered in traditional deep learning accelerators. Its energy efficiency
can realize deep learning-based edge applications. However, because IMC is
implemented using analog circuits, inherent non-idealities in the hardware pose
significant challenges. This paper presents physical neural networks (PNNs) for
constructing physical models of IMC. PNNs can address the synaptic current's
dependence on membrane potential, a challenge in charge-domain IMC systems. The
proposed model is mathematically equivalent to spiking neural networks with
reversal potentials. With a novel technique called differentiable spike-time
discretization, the PNNs are efficiently trained. We show that hardware
non-idealities traditionally viewed as detrimental can enhance the model's
learning performance. This bottom-up methodology was validated by designing an
IMC circuit with non-ideal characteristics using the sky130 process. When
employing this bottom-up approach, the modeling error reduced by an order of
magnitude compared to conventional top-down methods in post-layout simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>53 pages, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A physics-informed <span class="highlight-title">transformer</span> neural operator for learning generalized
  solutions of initial boundary value problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumanth Kumar Boya, Deepak Subramani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Initial boundary value problems arise commonly in applications with
engineering and natural systems governed by nonlinear partial differential
equations (PDEs). Operator learning is an emerging field for solving these
equations by using a neural network to learn a map between infinite dimensional
input and output function spaces. These neural operators are trained using a
combination of data (observations or simulations) and PDE-residuals
(physics-loss). A major drawback of existing neural approaches is the
requirement to retrain with new initial/boundary conditions, and the necessity
for a large amount of simulation data for training. We develop a
physics-informed transformer neural operator (named PINTO) that efficiently
generalizes to unseen initial and boundary conditions, trained in a
simulation-free setting using only physics loss. The main innovation lies in
our new iterative kernel integral operator units, implemented using
cross-attention, to transform the PDE solution's domain points into an
initial/boundary condition-aware representation vector, enabling efficient
learning of the solution function for new scenarios. The PINTO architecture is
applied to simulate the solutions of important equations used in engineering
applications: advection, Burgers, and steady and unsteady Navier-Stokes
equations (three flow scenarios). For these five test cases, we show that the
relative errors during testing under challenging conditions of unseen
initial/boundary conditions are only one-fifth to one-third of other leading
physics informed operator learning methods. Moreover, our PINTO model is able
to accurately solve the advection and Burgers equations at time steps that are
not included in the training collocation points. The code is available at
$\texttt{https://github.com/quest-lab-iisc/PINTO}$
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 11 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Motor Imagery Classification for Asynchronous EEG-Based Brain-Computer
  Interfaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanyu Wu, Siyang Li, Dongrui Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motor imagery (MI) based brain-computer interfaces (BCIs) enable the direct
control of external devices through the imagined movements of various body
parts. Unlike previous systems that used fixed-length EEG trials for MI
decoding, asynchronous BCIs aim to detect the user's MI without explicit
triggers. They are challenging to implement, because the algorithm needs to
first distinguish between resting-states and MI trials, and then classify the
MI trials into the correct task, all without any triggers. This paper proposes
a sliding window prescreening and classification (SWPC) approach for MI-based
asynchronous BCIs, which consists of two modules: a prescreening module to
screen MI trials out of the resting-state, and a classification module for MI
classification. Both modules are trained with supervised learning followed by
self-supervised learning, which refines the feature extractors. Within-subject
and cross-subject asynchronous MI classifications on four different EEG
datasets validated the effectiveness of SWPC, i.e., it always achieved the
highest average classification accuracy, and outperformed the best
state-of-the-art baseline on each dataset by about 2%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stellar parameter prediction and spectral simulation using machine
  learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vojtěch Cvrček, Martino Romaniello, Radim Šára, Wolfram Freudling, Pascal Ballester
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We applied machine learning to the entire data history of ESO's High Accuracy
Radial Velocity Planet Searcher (HARPS) instrument. Our primary goal was to
recover the physical properties of the observed objects, with a secondary
emphasis on simulating spectra. We systematically investigated the impact of
various factors on the accuracy and fidelity of the results, including the use
of simulated data, the effect of varying amounts of real training data, network
architectures, and learning paradigms. Our approach integrates supervised and
unsupervised learning techniques within autoencoder frameworks. Our methodology
leverages an existing simulation model that utilizes a library of existing
stellar spectra in which the emerging flux is computed from first principles
rooted in physics and a HARPS instrument model to generate simulated spectra
comparable to observational data. We trained standard and variational
autoencoders on HARPS data to predict spectral parameters and generate spectra.
Our models excel at predicting spectral parameters and compressing real
spectra, and they achieved a mean prediction error of approximately 50 K for
effective temperatures, making them relevant for most astrophysical
applications. Furthermore, the models predict metallicity ([M/H]) and surface
gravity (log g) with an accuracy of approximately 0.03 dex and 0.04 dex,
respectively, underscoring their broad applicability in astrophysical research.
The models' computational efficiency, with processing times of 779.6 ms on CPU
and 3.97 ms on GPU, makes them valuable for high-throughput applications like
massive spectroscopic surveys and large archival studies. By achieving accuracy
comparable to classical methods with significantly reduced computation time,
our methodology enhances the scope and efficiency of spectroscopic analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Astronomy & Astrophysics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Emergency Department Visits for Patients with Type II
  Diabetes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08984v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08984v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javad M Alizadeh, Jay S Patel, Gabriel Tajeu, Yuzhou Chen, Ilene L Hollin. Mukesh K Patel, Junchao Fei, Huanmei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over 30 million Americans are affected by Type II diabetes (T2D), a treatable
condition with significant health risks. This study aims to develop and
validate predictive models using machine learning (ML) techniques to estimate
emergency department (ED) visits among patients with T2D. Data for these
patients was obtained from the HealthShare Exchange (HSX), focusing on
demographic details, diagnoses, and vital signs. Our sample contained 34,151
patients diagnosed with T2D which resulted in 703,065 visits overall between
2017 and 2021. A workflow integrated EMR data with SDoH for ML predictions. A
total of 87 out of 2,555 features were selected for model construction. Various
machine learning algorithms, including CatBoost, Ensemble Learning, K-nearest
Neighbors (KNN), Support Vector Classification (SVC), Random Forest, and
Extreme Gradient Boosting (XGBoost), were employed with tenfold
cross-validation to predict whether a patient is at risk of an ED visit. The
ROC curves for Random Forest, XGBoost, Ensemble Learning, CatBoost, KNN, and
SVC, were 0.82, 0.82, 0.82, 0.81, 0.72, 0.68, respectively. Ensemble Learning
and Random Forest models demonstrated superior predictive performance in terms
of discrimination, calibration, and clinical applicability. These models are
reliable tools for predicting risk of ED visits among patients with T2D. They
can estimate future ED demand and assist clinicians in identifying critical
factors associated with ED utilization, enabling early interventions to reduce
such visits. The top five important features were age, the difference between
visitation gaps, visitation gaps, R10 or abdominal and pelvic pain, and the
Index of Concentration at the Extremes (ICE) for income.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript has been accepted and presented at AI-PHSS 2024: The
  2024 International Workshop on AI Applications in Public Health and Social
  Services in conjunction with the 22nd International Conference of Artificial
  Intelligence in Medicine (AIME 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Wander Through the Multimodal Landscape: Efficient Transfer Learning
  via Low-rank Sequence Multimodal Adapter <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08979v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08979v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zirun Guo, Xize Cheng, Yangyang Wu, Tao Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient transfer learning methods such as adapter-based methods have shown
great success in unimodal models and vision-language models. However, existing
methods have two main challenges in fine-tuning multimodal models. Firstly,
they are designed for vision-language tasks and fail to extend to situations
where there are more than two modalities. Secondly, they exhibit limited
exploitation of interactions between modalities and lack efficiency. To address
these issues, in this paper, we propose the loW-rank sequence multimodal
adapter (Wander). We first use the outer product to fuse the information from
different modalities in an element-wise way effectively. For efficiency, we use
CP decomposition to factorize tensors into rank-one components and achieve
substantial parameter reduction. Furthermore, we implement a token-level
low-rank decomposition to extract more fine-grained features and sequence
relationships between modalities. With these designs, Wander enables
token-level interactions between sequences of different modalities in a
parameter-efficient way. We conduct extensive experiments on datasets with
different numbers of modalities, where Wander outperforms state-of-the-art
efficient transfer learning methods consistently. The results fully demonstrate
the effectiveness, efficiency and universality of Wander.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Facial Consistency in Conditional Video Generation via Facial
  Landmark Transformation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08976v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08976v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lianrui Mu, Xingze Zhou, Wenjie Zheng, Jiangnan Ye, Xiaoyu Liang, Yuchen Yang, Jianhong Bai, Jiedong Zhuang, Haoji Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Landmark-guided character animation generation is an important field.
Generating character animations with facial features consistent with a
reference image remains a significant challenge in conditional video
generation, especially involving complex motions like dancing. Existing methods
often fail to maintain facial feature consistency due to mismatches between the
facial landmarks extracted from source videos and the target facial features in
the reference image. To address this problem, we propose a facial landmark
transformation method based on the 3D Morphable Model (3DMM). We obtain
transformed landmarks that align with the target facial features by
reconstructing 3D faces from the source landmarks and adjusting the 3DMM
parameters to match the reference image. Our method improves the facial
consistency between the generated videos and the reference images, effectively
improving the facial feature mismatch problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning Model Security: Threats and Defenses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyang Wang, Ziqian Bi, Yichao Zhang, Ming Liu, Weiche Hsieh, Pohsun Feng, Lawrence K. Q. Yan, Yizhu Wen, Benji Peng, Junyu Liu, Keyu Chen, Sen Zhang, Ming Li, Chuanqi Jiang, Xinyuan Song, Junjie Yang, Bowen Jing, Jintao Ren, Junhao Song, Hong-Ming Tseng, Silin Chen, Yunze Wang, Chia Xin Liang, Jiawei Xu, Xuanhe Pan, Jinlang Wang, Qian Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has transformed AI applications but faces critical security
challenges, including adversarial attacks, data poisoning, model theft, and
privacy leakage. This survey examines these vulnerabilities, detailing their
mechanisms and impact on model integrity and confidentiality. Practical
implementations, including adversarial examples, label flipping, and backdoor
attacks, are explored alongside defenses such as adversarial training,
differential privacy, and federated learning, highlighting their strengths and
limitations.
  Advanced methods like contrastive and self-supervised learning are presented
for enhancing robustness. The survey concludes with future directions,
emphasizing automated defenses, zero-trust architectures, and the security
challenges of large AI models. A balanced approach to performance and security
is essential for developing reliable deep learning systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Belted and Ensembled Neural Network for Linear and Nonlinear Sufficient
  Dimension Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yin Tang, Bing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a unified, flexible, and easy-to-implement framework of
sufficient dimension reduction that can accommodate both linear and nonlinear
dimension reduction, and both the conditional distribution and the conditional
mean as the targets of estimation. This unified framework is achieved by a
specially structured neural network -- the Belted and Ensembled Neural Network
(BENN) -- that consists of a narrow latent layer, which we call the belt, and a
family of transformations of the response, which we call the ensemble. By
strategically placing the belt at different layers of the neural network, we
can achieve linear or nonlinear sufficient dimension reduction, and by choosing
the appropriate transformation families, we can achieve dimension reduction for
the conditional distribution or the conditional mean. Moreover, thanks to the
advantage of the neural network, the method is very fast to compute, overcoming
a computation bottleneck of the traditional sufficient dimension reduction
estimators, which involves the inversion of a matrix of dimension either p or
n. We develop the algorithm and convergence rate of our method, compare it with
existing sufficient dimension reduction methods, and apply it to two data
examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 5 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stochastic Learning of Non-Conjugate Variational Posterior for Image
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kart-Leong Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large scale Bayesian nonparametrics (BNP) learner such as stochastic
variational inference (SVI) can handle datasets with large class number and
large training size at fractional cost. Like its predecessor, SVI rely on the
assumption of conjugate variational posterior to approximate the true
posterior. A more challenging problem is to consider large scale learning on
non-conjugate posterior. Recent works in this direction are mostly associated
with using Monte Carlo methods for approximating the learner. However, these
works are usually demonstrated on non-BNP related task and less complex models
such as logistic regression, due to higher computational complexity. In order
to overcome the issue faced by SVI, we develop a novel approach based on the
recently proposed variational maximization-maximization (VMM) learner to allow
large scale learning on non-conjugate posterior. Unlike SVI, our VMM learner
does not require closed-form expression for the variational posterior
expectatations. Our only requirement is that the variational posterior is
differentiable. In order to ensure convergence in stochastic settings, SVI rely
on decaying step-sizes to slow its learning. Inspired by SVI and Adam, we
propose the novel use of decaying step-sizes on both gradient and ascent
direction in our VMM to significantly improve its learning. We show that our
proposed methods is compatible with ResNet features when applied to large class
number datasets such as MIT67 and SUN397. Finally, we compare our proposed
learner with several recent works such as deep clustering algorithms and showed
we were able to produce on par or outperform the state-of-the-art methods in
terms of clustering measures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoSLD: An Extremely Parameter-Efficient Mixture-of-Shared LoRAs for
  Multi-Task Learning <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lulu Zhao, Weihao Zeng, Xiaofeng Shi, Hua Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, LoRA has emerged as a crucial technique for fine-tuning large
pre-trained models, yet its performance in multi-task learning scenarios often
falls short. In contrast, the MoE architecture presents a natural solution to
this issue. However, it introduces challenges such as mutual interference of
data across multiple domains and knowledge forgetting of various tasks.
Additionally, MoE significantly increases the number of parameters, posing a
computational cost challenge. Therefore, in this paper, we propose MoSLD, a
mixture-of-shared-LoRAs model with a dropout strategy. MoSLD addresses these
challenges by sharing the upper projection matrix in LoRA among different
experts, encouraging the model to learn general knowledge across tasks, while
still allowing the lower projection matrix to focus on the unique features of
each task. The application of dropout alleviates the imbalanced update of
parameter matrix and mitigates parameter overfitting in LoRA. Extensive
experiments demonstrate that our model exhibits excellent performance in both
single-task and multi-task scenarios, with robust out-of-domain generalization
capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept by COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLAVIDAL: A Large LAnguage VIsion Model for Daily Activities of Living 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09390v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09390v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominick Reilly, Rajatsubhra Chakraborty, Arkaprava Sinha, Manish Kumar Govind, Pu Wang, Francois Bremond, Le Xue, Srijan Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current Large Language Vision Models (LLVMs) trained on web videos perform
well in general video understanding but struggle with fine-grained details,
complex human-object interactions (HOI), and view-invariant representation
learning essential for Activities of Daily Living (ADL). This limitation stems
from a lack of specialized ADL video instruction-tuning datasets and
insufficient modality integration to capture discriminative action
representations. To address this, we propose a semi-automated framework for
curating ADL datasets, creating ADL-X, a multiview, multimodal RGBS
instruction-tuning dataset. Additionally, we introduce LLAVIDAL, an LLVM
integrating videos, 3D skeletons, and HOIs to model ADL's complex
spatiotemporal relationships. For training LLAVIDAL a simple joint alignment of
all modalities yields suboptimal results; thus, we propose a Multimodal
Progressive (MMPro) training strategy, incorporating modalities in stages
following a curriculum. We also establish ADL MCQ and video description
benchmarks to assess LLVM performance in ADL tasks. Trained on ADL-X, LLAVIDAL
achieves state-of-the-art performance across ADL benchmarks. Code and data will
be made publicly available at: https://adl-x.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Localizing Memorization in SSL Vision Encoders <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19069v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19069v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Wang, Adam Dziedzic, Michael Backes, Franziska Boenisch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work on studying memorization in self-supervised learning (SSL)
suggests that even though SSL encoders are trained on millions of images, they
still memorize individual data points. While effort has been put into
characterizing the memorized data and linking encoder memorization to
downstream utility, little is known about where the memorization happens inside
SSL encoders. To close this gap, we propose two metrics for localizing
memorization in SSL encoders on a per-layer (layermem) and per-unit basis
(unitmem). Our localization methods are independent of the downstream task, do
not require any label information, and can be performed in a forward pass. By
localizing memorization in various encoder architectures (convolutional and
transformer-based) trained on diverse datasets with contrastive and
non-contrastive SSL frameworks, we find that (1) while SSL memorization
increases with layer depth, highly memorizing units are distributed across the
entire encoder, (2) a significant fraction of units in SSL encoders experiences
surprisingly high memorization of individual data points, which is in contrast
to models trained under supervision, (3) atypical (or outlier) data points
cause much higher layer and unit memorization than standard data points, and
(4) in vision transformers, most memorization happens in the fully-connected
layers. Finally, we show that localizing memorization in SSL has the potential
to improve fine-tuning and to inform pruning strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Imitation to Refinement -- Residual RL for Precise Assembly 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16677v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16677v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lars Ankile, Anthony Simeonov, Idan Shenfeld, Marcel Torne, Pulkit Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Behavior Cloning (BC) have made it easy to teach robots
new tasks. However, we find that the ease of teaching comes at the cost of
unreliable performance that saturates with increasing data for tasks requiring
precision. The performance saturation can be attributed to two critical
factors: (a) distribution shift resulting from the use of offline data and (b)
the lack of closed-loop corrective control caused by action chucking
(predicting a set of future actions executed open-loop) critical for BC
performance. Our key insight is that by predicting action chunks, BC policies
function more like trajectory "planners" than closed-loop controllers necessary
for reliable execution. To address these challenges, we devise a simple yet
effective method, ResiP (Residual for Precise Manipulation), that overcomes the
reliability problem while retaining BC's ease of teaching and long-horizon
capabilities. ResiP augments a frozen, chunked BC model with a fully
closed-loop residual policy trained with reinforcement learning (RL) that
addresses distribution shifts and introduces closed-loop corrections over
open-loop execution of action chunks predicted by the BC trajectory planner.
Videos, code, and data: https://residual-assembly.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://residual-assembly.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disentangling Mean Embeddings for Better Diagnostics of Image Generators <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01314v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01314v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian G. Gruber, Pascal Tobias Ziegler, Florian Buettner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of image generators remains a challenge due to the limitations
of traditional metrics in providing nuanced insights into specific image
regions. This is a critical problem as not all regions of an image may be
learned with similar ease. In this work, we propose a novel approach to
disentangle the cosine similarity of mean embeddings into the product of cosine
similarities for individual pixel clusters via central kernel alignment.
Consequently, we can quantify the contribution of the cluster-wise performance
to the overall image generation performance. We demonstrate how this enhances
the explainability and the likelihood of identifying pixel regions of model
misbehavior across various real-world use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at Interpretable AI: Past, Present and Future Workshop at
  NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Addressing common misinterpretations of KART and UAT in neural network
  literature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16389v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16389v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vugar Ismailov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This note addresses the Kolmogorov-Arnold Representation Theorem (KART) and
the Universal Approximation Theorem (UAT), focusing on their common
misinterpretations in some papers related to neural network approximation. Our
remarks aim to support a more accurate understanding of KART and UAT among
neural network specialists.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages; a section, two theorems and several references added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-IID data in Federated Learning: A <span class="highlight-title">Survey</span> with Taxonomy, Metrics,
  Methods, Frameworks and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12377v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12377v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel M. Jimenez G., David Solans, Mikko Heikkila, Andrea Vitaletti, Nicolas Kourtellis, Aris Anagnostopoulos, Ioannis Chatzigiannakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in machine learning have highlighted Federated Learning (FL)
as a promising approach that enables multiple distributed users (so-called
clients) to collectively train ML models without sharing their private data.
While this privacy-preserving method shows potential, it struggles when data
across clients is not independent and identically distributed (non-IID) data.
The latter remains an unsolved challenge that can result in poorer model
performance and slower training times. Despite the significance of non-IID data
in FL, there is a lack of consensus among researchers about its classification
and quantification. This technical survey aims to fill that gap by providing a
detailed taxonomy for non-IID data, partition protocols, and metrics to
quantify data heterogeneity. Additionally, we describe popular solutions to
address non-IID data and standardized frameworks employed in FL with
heterogeneous data. Based on our state-of-the-art survey, we present key
lessons learned and suggest promising future research directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BEACON: Benchmark for Comprehensive RNA Tasks and Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10391v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10391v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Ren, Zhiyuan Chen, Lifeng Qiao, Hongtai Jing, Yuchen Cai, Sheng Xu, Peng Ye, Xinzhu Ma, Siqi Sun, Hongliang Yan, Dong Yuan, Wanli Ouyang, Xihui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  RNA plays a pivotal role in translating genetic instructions into functional
outcomes, underscoring its importance in biological processes and disease
mechanisms. Despite the emergence of numerous deep learning approaches for RNA,
particularly universal RNA language models, there remains a significant lack of
standardized benchmarks to assess the effectiveness of these methods. In this
study, we introduce the first comprehensive RNA benchmark BEACON
(\textbf{BE}nchm\textbf{A}rk for \textbf{CO}mprehensive R\textbf{N}A Task and
Language Models). First, BEACON comprises 13 distinct tasks derived from
extensive previous work covering structural analysis, functional studies, and
engineering applications, enabling a comprehensive assessment of the
performance of methods on various RNA understanding tasks. Second, we examine a
range of models, including traditional approaches like CNNs, as well as
advanced RNA foundation models based on language models, offering valuable
insights into the task-specific performances of these models. Third, we
investigate the vital RNA language model components from the tokenizer and
positional encoding aspects. Notably, our findings emphasize the superiority of
single nucleotide tokenization and the effectiveness of Attention with Linear
Biases (ALiBi) over traditional positional encoding methods. Based on these
insights, a simple yet strong baseline called BEACON-B is proposed, which can
achieve outstanding performance with limited data and computational resources.
The datasets and source code of our benchmark are available at
https://github.com/terry-r123/RNABenchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024 Dataset and Benchmark Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Achieving Constant Regret in Linear Markov Decision Processes <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10745v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10745v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weitong Zhang, Zhiyuan Fan, Jiafan He, Quanquan Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the constant regret guarantees in reinforcement learning (RL). Our
objective is to design an algorithm that incurs only finite regret over
infinite episodes with high probability. We introduce an algorithm,
Cert-LSVI-UCB, for misspecified linear Markov decision processes (MDPs) where
both the transition kernel and the reward function can be approximated by some
linear function up to misspecification level $\zeta$. At the core of
Cert-LSVI-UCB is an innovative \method, which facilitates a fine-grained
concentration analysis for multi-phase value-targeted regression, enabling us
to establish an instance-dependent regret bound that is constant w.r.t. the
number of episodes. Specifically, we demonstrate that for a linear MDP
characterized by a minimal suboptimality gap $\Delta$, Cert-LSVI-UCB has a
cumulative regret of $\tilde{\mathcal{O}}(d^3H^5/\Delta)$ with high
probability, provided that the misspecification level $\zeta$ is below
$\tilde{\mathcal{O}}(\Delta / (\sqrt{d}H^2))$. Here $d$ is the dimension of the
feature space and $H$ is the horizon. Remarkably, this regret bound is
independent of the number of episodes $K$. To the best of our knowledge,
Cert-LSVI-UCB is the first algorithm to achieve a constant, instance-dependent,
high-probability regret bound in RL with linear function approximation without
relying on prior distribution assumptions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, 3 tables, 2 figures, in 38th Conference on Neural
  Information Processing Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The rate of convergence of Bregman proximal methods: Local geometry vs.
  regularity vs. sharpness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08043v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08043v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Waïss Azizian, Franck Iutzeler, Jérôme Malick, Panayotis Mertikopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We examine the last-iterate convergence rate of Bregman proximal methods -
from mirror descent to mirror-prox and its optimistic variants - as a function
of the local geometry induced by the prox-mapping defining the method. For
generality, we focus on local solutions of constrained, non-monotone
variational inequalities, and we show that the convergence rate of a given
method depends sharply on its associated Legendre exponent, a notion that
measures the growth rate of the underlying Bregman function (Euclidean,
entropic, or other) near a solution. In particular, we show that boundary
solutions exhibit a stark separation of regimes between methods with a zero and
non-zero Legendre exponent: the former converge at a linear rate, while the
latter converge, in general, sublinearly. This dichotomy becomes even more
pronounced in linearly constrained problems where methods with entropic
regularization achieve a linear convergence rate along sharp directions,
compared to convergence in a finite number of steps under Euclidean
regularization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nearly Minimax Optimal Submodular Maximization with Bandit Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18465v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18465v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artin Tajdini, Lalit Jain, Kevin Jamieson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider maximizing an unknown monotonic, submodular set function $f:
2^{[n]} \rightarrow [0,1]$ with cardinality constraint under stochastic bandit
feedback. At each time $t=1,\dots,T$ the learner chooses a set $S_t \subset
[n]$ with $|S_t| \leq k$ and receives reward $f(S_t) + \eta_t$ where $\eta_t$
is mean-zero sub-Gaussian noise. The objective is to minimize the learner's
regret with respect to an approximation of the maximum $f(S_*)$ with $|S_*| =
k$, obtained through robust greedy maximization of $f$. To date, the best
regret bound in the literature scales as $k n^{1/3} T^{2/3}$. And by trivially
treating every set as a unique arm one deduces that $\sqrt{ {n \choose k} T }$
is also achievable using standard multi-armed bandit algorithms. In this work,
we establish the first minimax lower bound for this setting that scales like
$\tilde{\Omega}(\min_{L \le k}(L^{1/3}n^{1/3}T^{2/3} + \sqrt{{n \choose k -
L}T}))$. For a slightly restricted algorithm class, we prove a stronger regret
lower bound of $\tilde{\Omega}(\min_{L \le k}(Ln^{1/3}T^{2/3} + \sqrt{{n
\choose k - L}T}))$. Moreover, we propose an algorithm Sub-UCB that achieves
regret $\tilde{\mathcal{O}}(\min_{L \le k}(Ln^{1/3}T^{2/3} + \sqrt{{n \choose k
- L}T}))$ capable of matching the lower bound on regret for the restricted
class up to logarithmic factors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Free Guided Flow Matching with Optimal Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18070v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18070v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luran Wang, Chaoran Cheng, Yizhen Liao, Yanru Qu, Ge Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controlled generation with pre-trained Diffusion and Flow Matching models has
vast applications. One strategy for guiding ODE-based generative models is
through optimizing a target loss $R(x_1)$ while staying close to the prior
distribution. Along this line, some recent work showed the effectiveness of
guiding flow model by differentiating through its ODE sampling process. Despite
the superior performance, the theoretical understanding of this line of methods
is still preliminary, leaving space for algorithm improvement. Moreover,
existing methods predominately focus on Euclidean data manifold, and there is a
compelling need for guided flow methods on complex geometries such as SO(3),
which prevails in high-stake scientific applications like protein design. We
present OC-Flow, a general and theoretically grounded training-free framework
for guided flow matching using optimal control. Building upon advances in
optimal control theory, we develop effective and practical algorithms for
solving optimal control in guided ODE-based generation and provide a systematic
theoretical analysis of the convergence guarantee in both Euclidean and SO(3).
We show that existing backprop-through-ODE methods can be interpreted as
special cases of Euclidean OC-Flow. OC-Flow achieved superior performance in
extensive experiments on text-guided image manipulation, conditional molecule
generation, and all-atom peptide design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autonomous Goal Detection and Cessation in Reinforcement Learning: A
  Case Study on Source Term Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09541v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09541v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Shi, Muning Wen, Qi Zhang, Weinan Zhang, Cunjia Liu, Weiru Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning has revolutionized decision-making processes in
dynamic environments, yet it often struggles with autonomously detecting and
achieving goals without clear feedback signals. For example, in a Source Term
Estimation problem, the lack of precise environmental information makes it
challenging to provide clear feedback signals and to define and evaluate how
the source's location is determined. To address this challenge, the Autonomous
Goal Detection and Cessation (AGDC) module was developed, enhancing various RL
algorithms by incorporating a self-feedback mechanism for autonomous goal
detection and cessation upon task completion. Our method effectively identifies
and ceases undefined goals by approximating the agent's belief, significantly
enhancing the capabilities of RL algorithms in environments with limited
feedback. To validate effectiveness of our approach, we integrated AGDC with
deep Q-Network, proximal policy optimization, and deep deterministic policy
gradient algorithms, and evaluated its performance on the Source Term
Estimation problem. The experimental results showed that AGDC-enhanced RL
algorithms significantly outperformed traditional statistical methods such as
infotaxis, entrotaxis, and dual control for exploitation and exploration, as
well as a non-statistical random action selection method. These improvements
were evident in terms of success rate, mean traveled distance, and search time,
highlighting AGDC's effectiveness and efficiency in complex, real-world
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating <span class="highlight-title">GPT</span>-4 at Grading Handwritten Solutions in Math Exams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05231v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05231v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adriana Caraeni, Alexander Scarlatos, Andrew Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in generative artificial intelligence (AI) have shown promise
in accurately grading open-ended student responses. However, few prior works
have explored grading handwritten responses due to a lack of data and the
challenge of combining visual and textual information. In this work, we
leverage state-of-the-art multi-modal AI models, in particular GPT-4o, to
automatically grade handwritten responses to college-level math exams. Using
real student responses to questions in a probability theory exam, we evaluate
GPT-4o's alignment with ground-truth scores from human graders using various
prompting techniques. We find that while providing rubrics improves alignment,
the model's overall accuracy is still too low for real-world settings, showing
there is significant room for growth in this task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in LAK 2025: The 15th International Learning Analytics and
  Knowledge Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differential learning kinetics govern the transition from memorization
  to generalization during in-context learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00104v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00104v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Nguyen, Gautam Reddy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers exhibit in-context learning (ICL): the ability to use novel
information presented in the context without additional weight updates. Recent
work shows that ICL emerges when models are trained on a sufficiently diverse
set of tasks and the transition from memorization to generalization is sharp
with increasing task diversity. One interpretation is that a network's limited
capacity to memorize favors generalization. Here, we examine the mechanistic
underpinnings of this transition using a small transformer applied to a
synthetic ICL task. Using theory and experiment, we show that the sub-circuits
that memorize and generalize can be viewed as largely independent. The relative
rates at which these sub-circuits learn explains the transition from
memorization to generalization, rather than capacity constraints. We uncover a
memorization scaling law, which determines the task diversity threshold at
which the network generalizes. The theory quantitatively explains a variety of
other ICL-related phenomena, including the long-tailed distribution of when ICL
is acquired, the bimodal behavior of solutions close to the task diversity
threshold, the influence of contextual and data distributional statistics on
ICL, and the transient nature of ICL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs are Highly-Constrained Biophysical Sequence Optimizers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22296v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22296v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angelica Chen, Samuel D. Stanton, Robert G. Alberstein, Andrew M. Watkins, Richard Bonneau, Vladimir Gligorijević, Kyunghyun Cho, Nathan C. Frey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have recently shown significant potential in
various biological tasks such as protein engineering and molecule design. These
tasks typically involve black-box discrete sequence optimization, where the
challenge lies in generating sequences that are not only biologically feasible
but also adhere to hard fine-grained constraints. However, LLMs often struggle
with such constraints, especially in biological contexts where verifying
candidate solutions is costly and time-consuming. In this study, we explore the
possibility of employing LLMs as highly-constrained bilevel optimizers through
a methodology we refer to as Language Model Optimization with Margin
Expectation (LLOME). This approach combines both offline and online
optimization, utilizing limited oracle evaluations to iteratively enhance the
sequences generated by the LLM. We additionally propose a novel training
objective -- Margin-Aligned Expectation (MargE) -- that trains the LLM to
smoothly interpolate between the reward and reference distributions. Lastly, we
introduce a synthetic test suite that bears strong geometric similarity to real
biophysical problems and enables rapid evaluation of LLM optimizers without
time-consuming lab validation. Our findings reveal that, in comparison to
genetic algorithm baselines, LLMs achieve significantly lower regret solutions
while requiring fewer test function evaluations. However, we also observe that
LLMs exhibit moderate miscalibration, are susceptible to generator collapse,
and have difficulty finding the optimal solution when no explicit ground truth
rewards are available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Supercedes arXiv:2407.00236v1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model Developmental Safety: A Retention-Centric Method and Applications
  in Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03955v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03955v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gang Li, Wendi Yu, Yao Yao, Wei Tong, Yingbin Liang, Qihang Lin, Tianbao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the real world, a learning-enabled system usually undergoes multiple
cycles of model development to enhance the system's ability to handle difficult
or emerging tasks. This continual model development process raises a
significant issue that the model development for acquiring new or improving
existing capabilities may inadvertently lose capabilities of the old model,
also known as catastrophic forgetting. Existing continual learning studies
focus on mitigating catastrophic forgetting by trading off performance on
previous tasks and new tasks to ensure good average performance. However, they
are inadequate for many applications especially in safety-critical domains, as
failure to strictly preserve the good performance of the old model not only
introduces safety risks and uncertainties but also imposes substantial expenses
in the re-improving and re-validation of existing properties. To address this
issue, we introduce model developmental safety as a guarantee of a learning
system such that in the model development process the new model should strictly
preserve the existing protected capabilities of the old model while improving
its performance on target tasks. To ensure the model developmental safety, we
present a retention-centric framework by formulating the model developmental
safety as data-dependent constraints. Under this framework, we study how to
develop a pretrained vision-language model, specifically the CLIP model, for
acquiring new capabilities or improving existing capabilities of image
classification. We propose an efficient constrained optimization algorithm with
theoretical guarantee and use its insights to finetune a CLIP model with
task-dependent heads for promoting the model developmental safety. Our
experiments on improving vision perception capabilities on autonomous driving
and scene recognition datasets demonstrate the efficacy of the proposed
approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STARC: A General Framework For Quantifying Differences Between Reward
  Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15257v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15257v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joar Skalse, Lucy Farnik, Sumeet Ramesh Motwani, Erik Jenner, Adam Gleave, Alessandro Abate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to solve a task using reinforcement learning, it is necessary to
first formalise the goal of that task as a reward function. However, for many
real-world tasks, it is very difficult to manually specify a reward function
that never incentivises undesirable behaviour. As a result, it is increasingly
popular to use reward learning algorithms, which attempt to learn a reward
function from data. However, the theoretical foundations of reward learning are
not yet well-developed. In particular, it is typically not known when a given
reward learning algorithm with high probability will learn a reward function
that is safe to optimise. This means that reward learning algorithms generally
must be evaluated empirically, which is expensive, and that their failure modes
are difficult to anticipate in advance. One of the roadblocks to deriving
better theoretical guarantees is the lack of good methods for quantifying the
difference between reward functions. In this paper we provide a solution to
this problem, in the form of a class of pseudometrics on the space of all
reward functions that we call STARC (STAndardised Reward Comparison) metrics.
We show that STARC metrics induce both an upper and a lower bound on worst-case
regret, which implies that our metrics are tight, and that any metric with the
same properties must be bilipschitz equivalent to ours. Moreover, we also
identify a number of issues with reward metrics proposed by earlier works.
Finally, we evaluate our metrics empirically, to demonstrate their practical
efficacy. STARC metrics can be used to make both theoretical and empirical
analysis of reward learning algorithms both easier and more principled.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unlocking FedNL: Self-Contained Compute-Optimized Implementation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08760v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08760v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantin Burlachenko, Peter Richtárik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is an emerging paradigm that enables intelligent
agents to collaboratively train Machine Learning (ML) models in a distributed
manner, eliminating the need for sharing their local data. The recent work
(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)
algorithms, marking a significant step towards applying second-order methods to
FL and large-scale optimization. However, the reference FedNL prototype
exhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch
a single experiment in a sever-grade workstation; (ii) The prototype only
simulates multi-node setting; (iii) Prototype integration into
resource-constrained applications is challenging. To bridge the gap between
theory and practice, we present a self-contained implementation of FedNL,
FedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves
the aforementioned issues and reduces the wall clock time by x1000. With this
FedNL outperforms alternatives for training logistic regression in a
single-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark
(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose
two practical-orientated compressors for FedNL - adaptive TopLEK and
cache-aware RandSeqK, which fulfill the theory of FedNL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>55 pages, 12 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perturb and Recover: Fine-tuning for Effective Backdoor Removal from
  CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naman Deep Singh, Francesco Croce, Matthias Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language models like CLIP have been shown to be highly effective at
linking visual perception and natural language understanding, enabling
sophisticated image-text capabilities, including strong retrieval and zero-shot
classification performance. Their widespread use, as well as the fact that CLIP
models are trained on image-text pairs from the web, make them both a
worthwhile and relatively easy target for backdoor attacks. As training
foundational models, such as CLIP, from scratch is very expensive, this paper
focuses on cleaning potentially poisoned models via fine-tuning. We first show
that existing cleaning techniques are not effective against simple structured
triggers used in Blended or BadNet backdoor attacks, exposing a critical
vulnerability for potential real-world deployment of these models. Then, we
introduce PAR, Perturb and Recover, a surprisingly simple yet effective
mechanism to remove backdoors from CLIP models. Through extensive experiments
across different encoders and types of backdoor attacks, we show that PAR
achieves high backdoor removal rate while preserving good standard performance.
Finally, we illustrate that our approach is effective even only with synthetic
text-image pairs, i.e. without access to real training data. The code and
models are available at https://github.com/nmndeep/PerturbAndRecover.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parallel simulation for sampling under isoperimetry and score-based
  diffusion models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07435v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07435v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanjian Zhou, Masashi Sugiyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been a surge of interest in proving discretization
bounds for sampling under isoperimetry and for diffusion models. As data size
grows, reducing the iteration cost becomes an important goal. Inspired by the
great success of the parallel simulation of the initial value problem in
scientific computation, we propose parallel Picard methods for sampling tasks.
Rigorous theoretical analysis reveals that our algorithm achieves better
dependence on dimension $d$ than prior works in iteration complexity (i.e.,
reduced from $\widetilde{O}(\log^2 d)$ to $\widetilde{O}(\log d)$), which is
even optimal for sampling under isoperimetry with specific iteration
complexity. Our work highlights the potential advantages of simulation methods
in scientific computation for dynamics-based sampling and diffusion models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedAA: A Reinforcement Learning Perspective on Adaptive Aggregation for
  Fair and Robust Federated Learning <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05541v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05541v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialuo He, Wei Chen, Xiaojin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) has emerged as a promising approach for
privacy-preserving model training across decentralized devices. However, it
faces challenges such as statistical heterogeneity and susceptibility to
adversarial attacks, which can impact model robustness and fairness.
Personalized FL attempts to provide some relief by customizing models for
individual clients. However, it falls short in addressing server-side
aggregation vulnerabilities. We introduce a novel method called \textbf{FedAA},
which optimizes client contributions via \textbf{A}daptive \textbf{A}ggregation
to enhance model robustness against malicious clients and ensure fairness
across participants in non-identically distributed settings. To achieve this
goal, we propose an approach involving a Deep Deterministic Policy
Gradient-based algorithm for continuous control of aggregation weights, an
innovative client selection method based on model parameter distances, and a
reward mechanism guided by validation set performance. Empirically, extensive
experiments demonstrate that, in terms of robustness, \textbf{FedAA}
outperforms the state-of-the-art methods, while maintaining comparable levels
of fairness, offering a promising solution to build resilient and fair
federated systems. Our code is available at https://github.com/Gp1g/FedAA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scikit-fingerprints: easy and efficient computation of molecular
  fingerprints in Python 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13291v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13291v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Adamczyk, Piotr Ludynia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present scikit-fingerprints, a Python package for
computation of molecular fingerprints for applications in chemoinformatics. Our
library offers an industry-standard scikit-learn interface, allowing intuitive
usage and easy integration with machine learning pipelines. It is also highly
optimized, featuring parallel computation that enables efficient processing of
large molecular datasets. Currently, scikit-fingerprints stands as the most
feature-rich library in the open source Python ecosystem, offering over 30
molecular fingerprints. Our library simplifies chemoinformatics tasks based on
molecular fingerprints, including molecular property prediction and virtual
screening. It is also flexible, highly efficient, and fully open source.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Newton-CG methods for nonconvex unconstrained optimization with Hölder
  continuous Hessian 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13094v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13094v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuan He, Heng Huang, Zhaosong Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we consider a nonconvex unconstrained optimization problem
minimizing a twice differentiable objective function with H\"older continuous
Hessian. Specifically, we first propose a Newton-conjugate gradient (Newton-CG)
method for finding an approximate first- and second-order stationary point of
this problem, assuming the associated the H\"older parameters are explicitly
known. Then we develop a parameter-free Newton-CG method without requiring any
prior knowledge of these parameters. To the best of our knowledge, this method
is the first parameter-free second-order method achieving the best-known
iteration and operation complexity for finding an approximate first- and
second-order stationary point of this problem. Finally, we present preliminary
numerical results to demonstrate the superior practical performance of our
parameter-free Newton-CG method over a well-known regularized Newton method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2301.03139</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Injectivity of ReLU networks: perspectives from statistical physics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14112v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14112v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Maillard, Afonso S. Bandeira, David Belius, Ivan Dokmanić, Shuta Nakajima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When can the input of a ReLU neural network be inferred from its output? In
other words, when is the network injective? We consider a single layer, $x
\mapsto \mathrm{ReLU}(Wx)$, with a random Gaussian $m \times n$ matrix $W$, in
a high-dimensional setting where $n, m \to \infty$. Recent work connects this
problem to spherical integral geometry giving rise to a conjectured sharp
injectivity threshold for $\alpha = \frac{m}{n}$ by studying the expected Euler
characteristic of a certain random set. We adopt a different perspective and
show that injectivity is equivalent to a property of the ground state of the
spherical perceptron, an important spin glass model in statistical physics. By
leveraging the (non-rigorous) replica symmetry-breaking theory, we derive
analytical equations for the threshold whose solution is at odds with that from
the Euler characteristic. Furthermore, we use Gordon's min--max theorem to
prove that a replica-symmetric upper bound refutes the Euler characteristic
prediction. Along the way we aim to give a tutorial-style introduction to key
ideas from statistical physics in an effort to make the exposition accessible
to a broad audience. Our analysis establishes a connection between spin glasses
and integral geometry but leaves open the problem of explaining the
discrepancies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>62 pages ; Changes to match the published version (v2), in particular
  Appendix A.7 was added, and Appendix G was re-worked as an alternative proof
  of Theorem 1.8</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Personalized Coupled Tensor Decomposition for Multimodal Data Fusion:
  Uniqueness and Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01102v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01102v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ricardo Augusto Borsoi, Konstantin Usevich, David Brie, Tülay Adali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coupled tensor decompositions (CTDs) perform data fusion by linking factors
from different datasets. Although many CTDs have been already proposed, current
works do not address important challenges of data fusion, where: 1) the
datasets are often heterogeneous, constituting different "views" of a given
phenomena (multimodality); and 2) each dataset can contain personalized or
dataset-specific information, constituting distinct factors that are not
coupled with other datasets. In this work, we introduce a personalized CTD
framework tackling these challenges. A flexible model is proposed where each
dataset is represented as the sum of two components, one related to a common
tensor through a multilinear measurement model, and another specific to each
dataset. Both the common and distinct components are assumed to admit a
polyadic decomposition. This generalizes several existing CTD models. We
provide conditions for specific and generic uniqueness of the decomposition
that are easy to interpret. These conditions employ uni-mode uniqueness of
different individual datasets and properties of the measurement model. Two
algorithms are proposed to compute the common and distinct components: a
semi-algebraic one and a coordinate-descent optimization method. Experimental
results illustrate the advantage of the proposed framework compared with the
state of the art approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multi-Stage Framework for Joint Chest X-Ray Diagnosis and Visual
  Attention Prediction Using Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16970v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16970v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zirui Qiu, Hassan Rivaz, Yiming Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: As visual inspection is an inherent process during radiological
screening, the associated eye gaze data can provide valuable insights into
relevant clinical decisions. As deep learning has become the state-of-the-art
for computer-assisted diagnosis, integrating human behavior, such as eye gaze
data, into these systems is instrumental to help align machine predictions with
clinical diagnostic criteria, thus enhancing the quality of automatic
radiological diagnosis. Methods: We propose a novel deep learning framework for
joint disease diagnosis and prediction of corresponding clinical visual
attention maps for chest X-ray scans. Specifically, we introduce a new
dual-encoder multi-task UNet, which leverages both a DenseNet201 backbone and a
Residual and Squeeze-and-Excitation block-based encoder to extract diverse
features for visual attention map prediction, and a multi-scale feature-fusion
classifier to perform disease classification. To tackle the issue of
asynchronous training schedules of individual tasks in multi-task learning, we
proposed a multi-stage cooperative learning strategy, with contrastive learning
for feature encoder pretraining to boost performance. Results: Our proposed
method is shown to significantly outperform existing techniques for chest X-ray
diagnosis (AUC=0.93) and the quality of visual attention map prediction
(Correlation coefficient=0.58). Conclusion: Benefiting from the proposed
multi-task multi-stage cooperative learning, our technique demonstrates the
benefit of integrating clinicians' eye gaze into clinical AI systems to boost
performance and potentially explainability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distance-Adaptive Quaternion Knowledge Graph Embedding with
  Bidirectional Rotation <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04076v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04076v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihua Wang, Qiuyu Liang, Feilong Bao, Guanglai Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quaternion contains one real part and three imaginary parts, which provided a
more expressive hypercomplex space for learning knowledge graph. Existing
quaternion embedding models measure the plausibility of a triplet either
through semantic matching or geometric distance scoring functions. However, it
appears that semantic matching diminishes the separability of entities, while
the distance scoring function weakens the semantics of entities. To address
this issue, we propose a novel quaternion knowledge graph embedding model. Our
model combines semantic matching with entity's geometric distance to better
measure the plausibility of triplets. Specifically, in the quaternion space, we
perform a right rotation on head entity and a reverse rotation on tail entity
to learn rich semantic features. Then, we utilize distance adaptive
translations to learn geometric distance between entities. Furthermore, we
provide mathematical proofs to demonstrate our model can handle complex logical
relationships. Extensive experimental results and analyses show our model
significantly outperforms previous models on well-known knowledge graph
completion benchmark datasets. Our code is available at
https://github.com/llqy123/DaBR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of Artificial Intelligence in Gait-Based Neurodegenerative
  Disease Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13082v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13082v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haocong Rao, Minlin Zeng, Xuejiao Zhao, Chunyan Miao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed an increasing global population affected by
neurodegenerative diseases (NDs), which traditionally require extensive
healthcare resources and human effort for medical diagnosis and monitoring. As
a crucial disease-related motor symptom, human gait can be exploited to
characterize different NDs. The current advances in artificial intelligence
(AI) models enable automatic gait analysis for NDs identification and
classification, opening a new avenue to facilitate faster and more
cost-effective diagnosis of NDs. In this paper, we provide a comprehensive
survey on recent progress of machine learning and deep learning based AI
techniques applied to diagnosis of five typical NDs through gait. We provide an
overview of the process of AI-assisted NDs diagnosis, and present a systematic
taxonomy of existing gait data and AI models. Meanwhile, a novel quality
evaluation criterion is proposed to quantitatively assess the quality of
existing studies. Through an extensive review and analysis of 169 studies, we
present recent technical advancements, discuss existing challenges, potential
solutions, and future directions in this field. Finally, we envision the
prospective utilization of 3D skeleton data for human gait representation and
the development of more efficient AI models for NDs diagnosis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Article: 57 pages, citing 290 papers. Appendix: 30 pages. A
  up-to-date resource (papers, data, etc.) of this survey (AI4NDD) is provided
  at https://github.com/minlinzeng/AI4NDD-Survey</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Biology-inspired joint distribution neurons based on Hierarchical
  Correlation Reconstruction allowing for multidirectional neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05097v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05097v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jarek Duda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biological neural networks seem qualitatively superior (e.g. in learning,
flexibility, robustness) to current artificial like Multi-Layer Perceptron
(MLP) or Kolmogorov-Arnold Network (KAN). Simultaneously, in contrast to them:
biological have fundamentally multidirectional signal propagation \cite{axon},
also of probability distributions e.g. for uncertainty estimation, and are
believed not being able to use standard backpropagation training
\cite{backprop}. There are proposed novel artificial neurons based on HCR
(Hierarchical Correlation Reconstruction) allowing to remove the above low
level differences: with neurons containing local joint distribution model (of
its connections), representing joint density on normalized variables as just
linear combination of $(f_\mathbf{j})$ orthonormal polynomials:
$\rho(\mathbf{x})=\sum_{\mathbf{j}\in B} a_\mathbf{j} f_\mathbf{j}(\mathbf{x})$
for $\mathbf{x} \in [0,1]^d$ and $B\subset \mathbb{N}^d$ some chosen basis. By
various index summations of such $(a_\mathbf{j})_{\mathbf{j}\in B}$ tensor as
neuron parameters, we get simple formulas for e.g. conditional expected values
for propagation in any direction, like $E[x|y,z]$, $E[y|x]$, which degenerate
to KAN-like parametrization if restricting to pairwise dependencies. Such HCR
network can also propagate probability distributions (also joint) like
$\rho(y,z|x)$. It also allows for additional training approaches, like direct
$(a_\mathbf{j})$ estimation, through tensor decomposition, or more biologically
plausible information bottleneck training: layers directly influencing only
neighbors, optimizing content to maximize information about the next layer, and
minimizing about the previous to remove noise, extract crucial information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Avoiding strict saddle points of nonconvex regularized problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.09274v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.09274v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luwei Bai, Yaohua Hu, Hao Wang, Xiaoqi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider a class of non-convex and non-smooth sparse
optimization problems, which encompass most existing nonconvex
sparsity-inducing terms. We show the second-order optimality conditions only
depend on the nonzeros of the stationary points. We propose two damped
iterative reweighted algorithms including the iteratively reweighted $\ell_1$
algorithm (DIRL$_1$) and the iteratively reweighted $\ell_2$ (DIRL$_2$)
algorithm, to solve these problems. For DIRL$_1$, we show the reweighted
$\ell_1$ subproblem has support identification property so that DIRL$_1$
locally reverts to a gradient descent algorithm around a stationary point. For
DIRL$_2$, we show the solution map of the reweighted $\ell_2$ subproblem is
differentiable and Lipschitz continuous everywhere. Therefore, the map of
DIRL$_1$ and DIRL$_2$ and their inverse are Lipschitz continuous, and the
strict saddle points are their unstable fixed points. By applying the stable
manifold theorem, these algorithms are shown to converge only to local
minimizers with randomly initialization when the strictly saddle point property
is assumed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU <span class="chip">SOSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12456v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12456v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Song, Zeyu Mi, Haotong Xie, Haibo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces PowerInfer, a high-speed Large Language Model (LLM)
inference engine on a personal computer (PC) equipped with a single
consumer-grade GPU. The key principle underlying the design of PowerInfer is
exploiting the high locality inherent in LLM inference, characterized by a
power-law distribution in neuron activation. This distribution indicates that a
small subset of neurons, termed hot neurons, are consistently activated across
inputs, while the majority, cold neurons, vary based on specific inputs.
PowerInfer exploits such an insight to design a GPU-CPU hybrid inference
engine: hot-activated neurons are preloaded onto the GPU for fast access, while
cold-activated neurons are computed on the CPU, thus significantly reducing GPU
memory demands and CPU-GPU data transfers. PowerInfer further integrates
adaptive predictors and neuron-aware sparse operators, optimizing the
efficiency of neuron activation and computational sparsity. The evaluation
shows that PowerInfer significantly outperforms llama.cpp by up to 11.69x while
retaining model accuracy across various LLMs (including OPT-175B) on a single
NVIDIA RTX 4090 GPU. For the OPT-30B model, PowerInfer achieves performance
comparable to that of a high-end server-grade A100 GPU, reaching 82% of its
token generation rate on a single consumer-grade RTX 4090 GPU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SOSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CommonPower: A Framework for Safe Data-Driven Smart Grid Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03231v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03231v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Eichelbeck, Hannah Markgraf, Matthias Althoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing complexity of power system management has led to an increased
interest in reinforcement learning (RL). However, vanilla RL controllers cannot
themselves ensure satisfaction of system constraints. Therefore, combining them
with formally correct safeguarding mechanisms is an important aspect when
studying RL for power system management. Integrating safeguarding into complex
use cases requires tool support. To address this need, we introduce the Python
tool CommonPower. CommonPower's unique contribution lies in its symbolic
modeling approach, which enables flexible, model-based safeguarding of RL
controllers. Moreover, CommonPower offers a unified interface for single-agent
RL, multi-agent RL, and optimal control, with seamless integration of different
forecasting methods. This allows users to validate the effectiveness of safe RL
controllers across a large variety of case studies and investigate the
influence of specific aspects on overall performance. We demonstrate
CommonPower's versatility through a numerical case study that compares RL
agents featuring different safeguards with a model predictive controller in the
context of building energy management.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For the corresponding code repository, see
  https://github.com/TUMcps/commonpower</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PowerInfer-2: Fast Large Language Model Inference on a Smartphone 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06282v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06282v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenliang Xue, Yixin Song, Zeyu Mi, Xinrui Zheng, Yubin Xia, Haibo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) on smartphones enable real-time AI assistance
and privacy-preserving, offline operation. However, resource constraints of
smartphones limit current deployments to small language models (SLMs),
significantly compromising their capabilities. This paper introduces
PowerInfer-2, a smartphone-based framework that enables fast inference for LLMs
exceeding the memory capacity. The key insight is decomposing matrix operations
into neuron clusters as the basic processing unit, which enables flexible
scheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages
this neuron-cluster-based design in both computation and storage. For
computation, neuron clusters with dense activations are processed on NPU, while
sparse clusters use CPU. The storage engine provides a fine-grained pipeline
mechanism that coordinates cluster-level computation and I/O operations,
enhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2
achieves up to a 27.8x speed increase compared to state-of-the-art frameworks.
PowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving
11.68 tokens/s. Notably, these performance improvements preserve model quality
with negligible accuracy degradation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A second-order-like optimizer with adaptive gradient scaling for deep
  learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05871v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05871v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jérôme Bolte, Ryan Boustany, Edouard Pauwels, Andrei Purica
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this empirical article, we introduce INNAprop, an optimization algorithm
that combines the INNA method with the RMSprop adaptive gradient scaling. It
leverages second-order information and rescaling while keeping the memory
requirements of standard DL methods as AdamW or SGD with momentum. After giving
geometrical insights, we evaluate INNAprop on CIFAR-10, Food101, and ImageNet
with ResNets, VGG, DenseNet, and ViT, and on GPT-2 (OpenWebText) train from
scratch and with LoRA fine-tuning (E2E). INNAprop consistently matches or
outperforms AdamW both in training speed and accuracy, with minimal
hyperparameter tuning in large-scale settings. Our code is publicly available
at \url{https://github.com/innaprop/innaprop}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive Multi-scale Approach for Speech and Dynamics Synchrony
  in Talking Head Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Louis Airale, Dominique Vaufreydaz, Xavier Alameda-Pineda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Animating still face images with deep generative models using a speech input
signal is an active research topic and has seen important recent
progress.However, much of the effort has been put into lip syncing and
rendering quality while the generation of natural head motion, let alone the
audio-visual correlation between head motion and speech, has often been
neglected.In this work, we propose a multi-scale audio-visual synchrony loss
and a multi-scale autoregressive GAN to better handle short and long-term
correlation between speech and the dynamics of the head and lips.In particular,
we train a stack of syncer models on multimodal input pyramids and use these
models as guidance in a multi-scale generator network to produce audio-aligned
motion unfolding over diverse time scales.Both the pyramid of audio-visual
syncers and the generative models are trained in a low-dimensional space that
fully preserves dynamics cues.The experiments show significant improvements
over the state-of-the-art in head motion dynamics quality and especially in
multi-scale audio-visual synchrony on a collection of benchmark datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Likely Do LLMs with CoT Mimic Human Reasoning? <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16048v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16048v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangsheng Bao, Hongbo Zhang, Cunxiang Wang, Linyi Yang, Yue Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-thought emerges as a promising technique for eliciting reasoning
capabilities from Large Language Models (LLMs). However, it does not always
improve task performance or accurately represent reasoning processes, leaving
unresolved questions about its usage. In this paper, we diagnose the underlying
mechanism by comparing the reasoning process of LLMs with humans, using causal
analysis to understand the relationships between the problem instruction,
reasoning, and the answer in LLMs. Our empirical study reveals that LLMs often
deviate from the ideal causal chain, resulting in spurious correlations and
potential consistency errors (inconsistent reasoning and answers). We also
examine various factors influencing the causal structure, finding that
in-context learning with examples strengthens it, while post-training
techniques like supervised fine-tuning and reinforcement learning on human
feedback weaken it. To our surprise, the causal structure cannot be
strengthened by enlarging the model size only, urging research on new
techniques. We hope that this preliminary study will shed light on
understanding and improving the reasoning process in LLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2025 Camera Version (8 pages, 3 figures, 18 tables)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity
  within Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13516v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13516v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Song, Xu Han, Zhengyan Zhang, Shengding Hu, Xiyu Shi, Kuai Li, Chen Chen, Zhiyuan Liu, Guangli Li, Tao Yang, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Activation sparsity refers to the existence of considerable
weakly-contributed elements among activation outputs. As a prevalent property
of the models using the ReLU activation function, activation sparsity has been
proven a promising paradigm to boost model inference efficiency. Nevertheless,
most large language models (LLMs) adopt activation functions without intrinsic
activation sparsity (e.g., GELU and Swish). Some recent efforts have explored
introducing ReLU or its variants as the substitutive activation function to
help LLMs achieve activation sparsity and inference acceleration, but few can
simultaneously obtain high sparsity and comparable model performance. This
paper introduces a simple and effective sparsification method named "ProSparse"
to push LLMs for higher activation sparsity while maintaining comparable
performance. Specifically, after substituting the activation function of LLMs
with ReLU, ProSparse adopts progressive sparsity regularization with a factor
smoothly increasing along the multi-stage sine curves. This can enhance
activation sparsity and mitigate performance degradation by avoiding radical
shifts in activation distributions. With ProSparse, we obtain high sparsity of
89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for end-size
MiniCPM-1B, respectively, achieving comparable performance to their original
Swish-activated versions. These present the most sparsely activated models
among open-source LLaMA versions and competitive end-size models, considerably
surpassing ReluLLaMA-7B (66.98%) and ReluLLaMA-13B (71.56%). Our inference
acceleration experiments further demonstrate the significant practical
acceleration potential of LLMs with higher activation sparsity, obtaining up to
4.52$\times$ inference speedup.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 4 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Missing Melodies: AI Music Generation and its "Nearly" Complete Omission
  of the Global South 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atharva Mehta, Shivam Chauhan, Monojit Choudhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in generative AI have sparked renewed interest and expanded
possibilities for music generation. However, the performance and versatility of
these systems across musical genres are heavily influenced by the availability
of training data. We conducted an extensive analysis of over one million hours
of audio datasets used in AI music generation research and manually reviewed
more than 200 papers from eleven prominent AI and music conferences and
organizations (AAAI, ACM, EUSIPCO, EURASIP, ICASSP, ICML, IJCAI, ISMIR,
NeurIPS, NIME, SMC) to identify a critical gap in the fair representation and
inclusion of the musical genres of the Global South in AI research. Our
findings reveal a stark imbalance: approximately 86% of the total dataset hours
and over 93% of researchers focus primarily on music from the Global North.
However, around 40% of these datasets include some form of non-Western music,
genres from the Global South account for only 14.6% of the data. Furthermore,
approximately 51% of the papers surveyed concentrate on symbolic music
generation, a method that often fails to capture the cultural nuances inherent
in music from regions such as South Asia, the Middle East, and Africa. As AI
increasingly shapes the creation and dissemination of music, the significant
underrepresentation of music genres in datasets and research presents a serious
threat to global musical diversity. We also propose some important steps to
mitigate these risks and foster a more inclusive future for AI-driven music
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to CACM, 12 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large language models as oracles for instantiating ontologies with
  domain-specific knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.04108v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.04108v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni Ciatto, Andrea Agiollo, Matteo Magnini, Andrea Omicini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background. Endowing intelligent systems with semantic data commonly requires
designing and instantiating ontologies with domain-specific knowledge.
Especially in the early phases, those activities are typically performed
manually by human experts possibly leveraging on their own experience. The
resulting process is therefore time-consuming, error-prone, and often biased by
the personal background of the ontology designer. Objective. To mitigate that
issue, we propose a novel domain-independent approach to automatically
instantiate ontologies with domain-specific knowledge, by leveraging on large
language models (LLMs) as oracles. Method. Starting from (i) an initial schema
composed by inter-related classes and properties and (ii) a set of query
templates, our method queries the LLM multiple times, and generates instances
for both classes and properties from its replies. Thus, the ontology is
automatically filled with domain-specific knowledge, compliant to the initial
schema. As a result, the ontology is quickly and automatically enriched with
manifold instances, which experts may consider to keep, adjust, discard, or
complement according to their own needs and expertise. Contribution. We
formalise our method in general way and instantiate it over various LLMs, as
well as on a concrete case study. We report experiments rooted in the
nutritional domain where an ontology of food meals and their ingredients is
automatically instantiated from scratch, starting from a categorisation of
meals and their relationships. There, we analyse the quality of the generated
ontologies and compare ontologies attained by exploiting different LLMs.
Experimentally, our approach achieves a quality metric that is up to five times
higher than the state-of-the-art, while reducing erroneous entities and
relations by up to ten times. Finally, we provide a SWOT analysis of the
proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Watermarking Training Data of Music Generation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08549v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08549v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Epple, Igor Shilov, Bozhidar Stevanoski, Yves-Alexandre de Montjoye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Artificial Intelligence (Gen-AI) models are increasingly used to
produce content across domains, including text, images, and audio. While these
models represent a major technical breakthrough, they gain their generative
capabilities from being trained on enormous amounts of human-generated content,
which often includes copyrighted material. In this work, we investigate whether
audio watermarking techniques can be used to detect an unauthorized usage of
content to train a music generation model. We compare outputs generated by a
model trained on watermarked data to a model trained on non-watermarked data.
We study factors that impact the model's generation behaviour: the watermarking
technique, the proportion of watermarked samples in the training set, and the
robustness of the watermarking technique against the model's tokenizer. Our
results show that audio watermarking techniques, including some that are
imperceptible to humans, can lead to noticeable shifts in the model's outputs.
We also study the robustness of a state-of-the-art watermarking technique to
removal techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Golden Noise for Diffusion Models: A Learning Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09502v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09502v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zikai Zhou, Shitong Shao, Lichen Bai, Zhiqiang Xu, Bo Han, Zeke Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion model is a popular paradigm that synthesizes
personalized images by providing a text prompt and a random Gaussian noise.
While people observe that some noises are ``golden noises'' that can achieve
better text-image alignment and higher human preference than others, we still
lack a machine learning framework to obtain those golden noises. To learn
golden noises for diffusion sampling, we mainly make three contributions in
this paper. First, we identify a new concept termed the \textit{noise prompt},
which aims at turning a random Gaussian noise into a golden noise by adding a
small desirable perturbation derived from the text prompt. Following the
concept, we first formulate the \textit{noise prompt learning} framework that
systematically learns ``prompted'' golden noise associated with a text prompt
for diffusion models. Second, we design a noise prompt data collection pipeline
and collect a large-scale \textit{noise prompt dataset}~(NPD) that contains
100k pairs of random noises and golden noises with the associated text prompts.
With the prepared NPD as the training dataset, we trained a small \textit{noise
prompt network}~(NPNet) that can directly learn to transform a random noise
into a golden noise. The learned golden noise perturbation can be considered as
a kind of prompt for noise, as it is rich in semantic information and tailored
to the given text prompt. Third, our extensive experiments demonstrate the
impressive effectiveness and generalization of NPNet on improving the quality
of synthesized images across various diffusion models, including SDXL,
DreamShaper-xl-v2-turbo, and Hunyuan-DiT. Moreover, NPNet is a small and
efficient controller that acts as a plug-and-play module with very limited
additional inference and computational costs, as it just provides a golden
noise instead of a random noise without accessing the original pipeline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Action Influence Aware Counterfactual Data Augmentation <span class="chip">ICML
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18917v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18917v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Núria Armengol Urpí, Marco Bagatella, Marin Vlastelica, Georg Martius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline data are both valuable and practical resources for teaching robots
complex behaviors. Ideally, learning agents should not be constrained by the
scarcity of available demonstrations, but rather generalize beyond the training
distribution. However, the complexity of real-world scenarios typically
requires huge amounts of data to prevent neural network policies from picking
up on spurious correlations and learning non-causal relationships. We propose
CAIAC, a data augmentation method that can create feasible synthetic
transitions from a fixed dataset without having access to online environment
interactions. By utilizing principled methods for quantifying causal influence,
we are able to perform counterfactual reasoning by swapping
$\it{action}$-unaffected parts of the state-space between independent
trajectories in the dataset. We empirically show that this leads to a
substantial increase in robustness of offline learning algorithms against
distributional shift.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in 41st International Conference on Machine Learning (ICML
  2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced Low-Dose CT Image Reconstruction by Domain and Task Shifting
  Gaussian Denoisers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03551v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03551v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Selig, Thomas März, Martin Storath, Andreas Weinmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computed tomography from a low radiation dose (LDCT) is challenging due to
high noise in the projection data. Popular approaches for LDCT image
reconstruction are two-stage methods, typically consisting of the filtered
backprojection (FBP) algorithm followed by a neural network for LDCT image
enhancement. Two-stage methods are attractive for their simplicity and
potential for computational efficiency, typically requiring only a single FBP
and a neural network forward pass for inference. However, the best
reconstruction quality is currently achieved by unrolled iterative methods
(Learned Primal-Dual and ItNet), which are more complex and thus have a higher
computational cost for training and inference. We propose a method combining
the simplicity and efficiency of two-stage methods with state-of-the-art
reconstruction quality. Our strategy utilizes a neural network pretrained for
Gaussian noise removal from natural grayscale images, fine-tuned for LDCT image
enhancement. We call this method FBP-DTSGD (Domain and Task Shifted Gaussian
Denoisers) as the fine-tuning is a task shift from Gaussian denoising to
enhancing LDCT images and a domain shift from natural grayscale to LDCT images.
An ablation study with three different pretrained Gaussian denoisers indicates
that the performance of FBP-DTSGD does not depend on a specific denoising
architecture, suggesting future advancements in Gaussian denoising could
benefit the method. The study also shows that pretraining on natural images
enhances LDCT reconstruction quality, especially with limited training data.
Notably, pretraining involves no additional cost, as existing pretrained models
are used. The proposed method currently holds the top mean position in the
LoDoPaB-CT challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vanilla Bayesian Optimization Performs Great in High Dimensions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02229v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02229v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carl Hvarfner, Erik Orm Hellsten, Luigi Nardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-dimensional problems have long been considered the Achilles' heel of
Bayesian optimization algorithms. Spurred by the curse of dimensionality, a
large collection of algorithms aim to make it more performant in this setting,
commonly by imposing various simplifying assumptions on the objective. In this
paper, we identify the degeneracies that make vanilla Bayesian optimization
poorly suited to high-dimensional tasks, and further show how existing
algorithms address these degeneracies through the lens of lowering the model
complexity. Moreover, we propose an enhancement to the prior assumptions that
are typical to vanilla Bayesian optimization algorithms, which reduces the
complexity to manageable levels without imposing structural restrictions on the
objective. Our modification - a simple scaling of the Gaussian process
lengthscale prior with the dimensionality - reveals that standard Bayesian
optimization works drastically better than previously thought in high
dimensions, clearly outperforming existing state-of-the-art algorithms on
multiple commonly considered real-world high-dimensional tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Survey</span> on Test-Time Adaptation under Distribution Shifts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.15361v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.15361v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Liang, Ran He, Tieniu Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning methods strive to acquire a robust model during the training
process that can effectively generalize to test samples, even in the presence
of distribution shifts. However, these methods often suffer from performance
degradation due to unknown test distributions. Test-time adaptation (TTA), an
emerging paradigm, has the potential to adapt a pre-trained model to unlabeled
data during testing, before making predictions. Recent progress in this
paradigm has highlighted the significant benefits of using unlabeled data to
train self-adapted models prior to inference. In this survey, we categorize TTA
into several distinct groups based on the form of test data, namely, test-time
domain adaptation, test-time batch adaptation, and online test-time adaptation.
For each category, we provide a comprehensive taxonomy of advanced algorithms
and discuss various learning scenarios. Furthermore, we analyze relevant
applications of TTA and discuss open challenges and promising areas for future
research. For a comprehensive list of TTA methods, kindly refer to
\url{https://github.com/tim-learn/awesome-test-time-adaptation}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Discussions, comments, and questions are all welcomed in
  \url{https://github.com/tim-learn/awesome-test-time-adaptation}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AdaStop: adaptive statistical testing for sound comparisons of Deep RL
  agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.10882v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.10882v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothée Mathieu, Riccardo Della Vecchia, Alena Shilova, Matheus Medeiros Centa, Hector Kohler, Odalric-Ambrym Maillard, Philippe Preux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the scientific community has questioned the statistical
reproducibility of many empirical results, especially in the field of machine
learning. To contribute to the resolution of this reproducibility crisis, we
propose a theoretically sound methodology for comparing the performance of a
set of algorithms. We exemplify our methodology in Deep Reinforcement Learning
(Deep RL). The performance of one execution of a Deep RL algorithm is a random
variable. Therefore, several independent executions are needed to evaluate its
performance. When comparing algorithms with random performance, a major
question concerns the number of executions to perform to ensure that the result
of the comparison is theoretically sound. Researchers in Deep RL often use less
than 5 independent executions to compare algorithms: we claim that this is not
enough in general. Moreover, when comparing more than 2 algorithms at once, we
have to use a multiple tests procedure to preserve low error guarantees. We
introduce AdaStop, a new statistical test based on multiple group sequential
tests. When used to compare algorithms, AdaStop adapts the number of executions
to stop as early as possible while ensuring that enough information has been
collected to distinguish algorithms that have different score distributions. We
prove theoretically that AdaStop has a low probability of making a
(family-wise) error. We illustrate the effectiveness of AdaStop in various
use-cases, including toy examples and Deep RL algorithms on challenging Mujoco
environments. AdaStop is the first statistical test fitted to this sort of
comparisons: it is both a significant contribution to statistics, and an
important contribution to computational studies performed in reinforcement
learning and in other domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SimVPv2: Towards Simple yet Powerful Spatiotemporal Predictive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12509v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12509v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Tan, Zhangyang Gao, Siyuan Li, Stan Z. Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed remarkable advances in spatiotemporal predictive
learning, with methods incorporating auxiliary inputs, complex neural
architectures, and sophisticated training strategies. While SimVP has
introduced a simpler, CNN-based baseline for this task, it still relies on
heavy Unet-like architectures for spatial and temporal modeling, which still
suffers from high complexity and computational overhead. In this paper, we
propose SimVPv2, a streamlined model that eliminates the need for Unet
architectures and demonstrates that plain stacks of convolutional layers,
enhanced with an efficient Gated Spatiotemporal Attention mechanism, can
deliver state-of-the-art performance. SimVPv2 not only simplifies the model
architecture but also improves both performance and computational efficiency.
On the standard Moving MNIST benchmark, SimVPv2 achieves superior performance
compared to SimVP, with fewer FLOPs, about half the training time, and 60%
faster inference efficiency. Extensive experiments across eight diverse
datasets, including real-world tasks such as traffic forecasting and climate
prediction, further demonstrate that SimVPv2 offers a powerful yet
straightforward solution, achieving robust generalization across various
spatiotemporal learning scenarios. We believe the proposed SimVPv2 can serve as
a solid baseline to benefit the spatiotemporal predictive learning community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TMM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A simple thinking about the application of the attention mechanism in
  medical ultrasound image segmentation task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00919v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00919v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gongping Chen, Rui Wang, Xiaotao Yin, Liang Cui, Yu Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The AI-based assisted diagnosis programs have been widely investigated on
medical ultrasound images. Complex scenario of ultrasound image, in which the
coupled interference of internal and external factors is severe, brings a
unique challenge for localize the object region automatically and precisely in
ultrasound images. In this study, we seek to propose a more general and robust
Benchmark Attention Adaptive Framework (BAAF) to assist doctors segment or
diagnose lesions and tissues in ultrasound images more quickly and accurately.
Different from existing attention schemes, the BAAF consists of a parallel
hybrid attention module (PHAM) and an adaptive calibration mechanism (ACM).
Specifically, BAAF first coarsely calibrates the input features from the
channel and spatial dimensions, and then adaptively selects more robust lesion
or tissue characterizations from the coarse-calibrated feature maps. The design
of BAAF further optimizes the "what" and "where" focus and selection problems
in CNNs and seeks to improve the segmentation accuracy of lesions or tissues in
medical ultrasound images. The method is evaluated on four medical ultrasound
segmentation tasks, and the adequate experimental results demonstrate the
remarkable performance improvement over existing state-of-the-art methods. In
addition, the comparison with existing attention mechanisms also demonstrates
the superiority of BAAF. This work provides the possibility for automated
medical ultrasound assisted diagnosis and reduces reliance on human accuracy
and precision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transfer Learning with Partially Observable Offline Data via Causal
  Bounds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03572v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03572v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueping Gong, Wei You, Jiheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transfer learning has emerged as an effective approach to accelerate learning
by integrating knowledge from related source agents. However, challenges arise
due to data heterogeneity-such as differences in feature sets or incomplete
datasets-which often results in the nonidentifiability of causal effects. In
this paper, we investigate transfer learning in partially observable contextual
bandits, where agents operate with incomplete information and limited access to
hidden confounders. To address the challenges posed by unobserved confounders,
we formulate optimization problems to derive tight bounds on the
nonidentifiable causal effects. We then propose an efficient method that
discretizes the functional constraints of unknown distributions into linear
constraints, allowing us to sample compatible causal models through a
sequential process of solving linear programs. This method takes into account
estimation errors and exhibits strong convergence properties, ensuring robust
and reliable causal bounds. Leveraging these causal bounds, we improve
classical bandit algorithms, achieving tighter regret upper and lower bounds
relative to the sizes of action sets and function spaces. In tasks involving
function approximation, which are crucial for handling complex context spaces,
our method significantly improves the dependence on function space size
compared to previous work. We formally prove that our causally enhanced
algorithms outperform classical bandit algorithms, achieving notably faster
convergence rates. The applicability of our approach is further illustrated
through an example of offline pricing policy learning with censored
demand.Simulations confirm the superiority of our approach over
state-of-the-art methods, demonstrating its potential to enhance contextual
bandit agents in real-world applications, especially when data is scarce,
costly, or restricted due to privacy concerns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>57 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GARLIC: <span class="highlight-title">GPT</span>-Augmented Reinforcement Learning with Intelligent Control
  for Vehicle Dispatching <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10286v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10286v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Han, Zijian Zhang, Xiangyu Zhao, Guojiang Shen, Xiangjie Kong, Xuetao Wei, Liqiang Nie, Jieping Ye, Yuanshao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As urban residents demand higher travel quality, vehicle dispatch has become
a critical component of online ride-hailing services. However, current vehicle
dispatch systems struggle to navigate the complexities of urban traffic
dynamics, including unpredictable traffic conditions, diverse driver behaviors,
and fluctuating supply and demand patterns. These challenges have resulted in
travel difficulties for passengers in certain areas, while many drivers in
other areas are unable to secure orders, leading to a decline in the overall
quality of urban transportation services. To address these issues, this paper
introduces GARLIC: a framework of GPT-Augmented Reinforcement Learning with
Intelligent Control for vehicle dispatching. GARLIC utilizes multiview graphs
to capture hierarchical traffic states, and learns a dynamic reward function
that accounts for individual driving behaviors. The framework further
integrates a GPT model trained with a custom loss function to enable
high-precision predictions and optimize dispatching policies in real-world
scenarios. Experiments conducted on two real-world datasets demonstrate that
GARLIC effectively aligns with driver behaviors while reducing the empty load
rate of vehicles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Application of Neural Ordinary Differential Equations for ITER Burning
  Plasma Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14404v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14404v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zefang Liu, Weston M. Stacey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dynamics of burning plasmas in tokamaks are crucial for advancing
controlled thermonuclear fusion. This study applies the NeuralPlasmaODE, a
multi-region multi-timescale transport model, to simulate the complex energy
transfer processes in ITER deuterium-tritium (D-T) plasmas. Our model captures
the interactions between energetic alpha particles, electrons, and ions, which
are vital for understanding phenomena such as thermal runaway instability. We
employ neural ordinary differential equations (Neural ODEs) for the numerical
derivation of diffusivity parameters, enabling precise modeling of energy
interactions between different plasma regions. By leveraging transfer learning,
we utilize model parameters derived from DIII-D experimental data, enhancing
the efficiency and accuracy of our simulations without training from scratch.
Applying this model to ITER's inductive and non-inductive operational
scenarios, our results demonstrate that radiation and transport processes
effectively remove excess heat from the core plasma, preventing thermal runaway
instability. This study underscores the potential of machine learning in
advancing our understanding and control of burning plasma dynamics in fusion
reactors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training on the Test Task Confounds Evaluation and Emergence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07890v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07890v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ricardo Dominguez-Olmedo, Florian E. Dorner, Moritz Hardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a fundamental problem in the evaluation of large language models
that we call training on the test task. Unlike wrongful practices like training
on the test data, leakage, or data contamination, training on the test task is
not a malpractice. Rather, the term describes a growing set of practices that
utilize knowledge about evaluation tasks at training time. We demonstrate that
training on the test task confounds both relative model evaluations and claims
about emergent capabilities. We argue that the seeming superiority of one model
family over another may be explained by a different degree of training on the
test task. To this end, we propose an effective method to adjust for the effect
of training on the test task on benchmark evaluations. Put simply, to fine-tune
each model under comparison on the same task-relevant data before evaluation.
We then show that instances of emergent behavior disappear gradually as models
train on the test task. Our work promotes a new perspective on the evaluation
of large language models with broad implications for benchmarking and the study
of emergent capabilities
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning and Machine Learning, Advancing Big Data Analytics and
  Management: Unveiling AI's Potential Through Tools, Techniques, and
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01268v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01268v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pohsun Feng, Ziqian Bi, Yizhu Wen, Xuanhe Pan, Benji Peng, Ming Liu, Jiawei Xu, Keyu Chen, Junyu Liu, Caitlyn Heqi Yin, Sen Zhang, Jinlang Wang, Qian Niu, Ming Li, Tianyang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI), machine learning, and deep learning have become
transformative forces in big data analytics and management, enabling
groundbreaking advancements across diverse industries. This article delves into
the foundational concepts and cutting-edge developments in these fields, with a
particular focus on large language models (LLMs) and their role in natural
language processing, multimodal reasoning, and autonomous decision-making.
Highlighting tools such as ChatGPT, Claude, and Gemini, the discussion explores
their applications in data analysis, model design, and optimization.
  The integration of advanced algorithms like neural networks, reinforcement
learning, and generative models has enhanced the capabilities of AI systems to
process, visualize, and interpret complex datasets. Additionally, the emergence
of technologies like edge computing and automated machine learning (AutoML)
democratizes access to AI, empowering users across skill levels to engage with
intelligent systems. This work also underscores the importance of ethical
considerations, transparency, and fairness in the deployment of AI
technologies, paving the way for responsible innovation.
  Through practical insights into hardware configurations, software
environments, and real-world applications, this article serves as a
comprehensive resource for researchers and practitioners. By bridging
theoretical underpinnings with actionable strategies, it showcases the
potential of AI and LLMs to revolutionize big data management and drive
meaningful advancements across domains such as healthcare, finance, and
autonomous systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This book contains 155 pages and 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accurate Link Prediction for Edge-Incomplete Graphs via PU Learning <span class="chip">AAAI'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11911v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11911v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junghun Kim, Ka Hyun Park, Hoyoung Yoon, U Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given an edge-incomplete graph, how can we accurately find the missing links?
The link prediction in edge-incomplete graphs aims to discover the missing
relations between entities when their relationships are represented as a graph.
Edge-incomplete graphs are prevalent in real-world due to practical
limitations, such as not checking all users when adding friends in a social
network. Addressing the problem is crucial for various tasks, including
recommending friends in social networks and finding references in citation
networks. However, previous approaches rely heavily on the given
edge-incomplete (observed) graph, making it challenging to consider the missing
(unobserved) links during training. In this paper, we propose PULL
(PU-Learning-based Link predictor), an accurate link prediction method based on
the positive-unlabeled (PU) learning. PULL treats the observed edges in the
training graph as positive examples, and the unconnected node pairs as
unlabeled ones. PULL effectively prevents the link predictor from overfitting
to the observed graph by proposing latent variables for every edge, and
leveraging the expected graph structure with respect to the variables.
Extensive experiments on five real-world datasets show that PULL consistently
outperforms the baselines for predicting links in edge-incomplete graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unlearning or Concealment? A Critical Analysis and Evaluation Metrics
  for Unlearning in Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05668v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05668v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aakash Sen Sharma, Niladri Sarkar, Vikram Chundawat, Ankur A Mali, Murari Mandal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has seen significant interest in methods for concept removal
and targeted forgetting in text-to-image diffusion models. In this paper, we
conduct a comprehensive white-box analysis showing the vulnerabilities in
existing diffusion model unlearning methods. We show that existing unlearning
methods lead to decoupling of the targeted concepts (meant to be forgotten) for
the corresponding prompts. This is concealment and not actual forgetting, which
was the original goal. This paper presents a rigorous theoretical and empirical
examination of five commonly used techniques for unlearning in diffusion
models, while showing their potential weaknesses. We introduce two new
evaluation metrics: Concept Retrieval Score (\textbf{CRS}) and Concept
Confidence Score (\textbf{CCS}). These metrics are based on a successful
adversarial attack setup that can recover \textit{forgotten} concepts from
unlearned diffusion models. \textbf{CRS} measures the similarity between the
latent representations of the unlearned and fully trained models after
unlearning. It reports the extent of retrieval of the \textit{forgotten}
concepts with increasing amount of guidance. CCS quantifies the confidence of
the model in assigning the target concept to the manipulated data. It reports
the probability of the \textit{unlearned} model's generations to be aligned
with the original domain knowledge with increasing amount of guidance. The
\textbf{CCS} and \textbf{CRS} enable a more robust evaluation of concept
erasure methods. Evaluating existing five state-of-the-art methods with our
metrics, reveal significant shortcomings in their ability to truly
\textit{unlearn}. Source Code:
\color{blue}{https://respailab.github.io/unlearning-or-concealment}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synthetic Potential Outcomes and Causal Mixture Identifiability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19225v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19225v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bijan Mazaheri, Chandler Squires, Caroline Uhler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Heterogeneous data from multiple populations, sub-groups, or sources is often
represented as a ``mixture model'' with a single latent class influencing all
of the observed covariates. Heterogeneity can be resolved at multiple levels by
grouping populations according to different notions of similarity. This paper
proposes grouping with respect to the causal response of an intervention or
perturbation on the system. This definition is distinct from previous notions,
such as similar covariate values (e.g. clustering) or similar correlations
between covariates (e.g. Gaussian mixture models). To solve the problem, we
``synthetically sample'' from a counterfactual distribution using higher-order
multi-linear moments of the observable data. To understand how these ``causal
mixtures'' fit in with more classical notions, we develop a hierarchy of
mixture identifiability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Annotation-guided Protein Design with Multi-Level Domain Alignment <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16866v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16866v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaohao Yuan, Songyou Li, Geyan Ye, Yikun Zhang, Long-Kai Huang, Wenbing Huang, Wei Liu, Jianhua Yao, Yu Rong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The core challenge of de novo protein design lies in creating proteins with
specific functions or properties, guided by certain conditions. Current models
explore to generate protein using structural and evolutionary guidance, which
only provide indirect conditions concerning functions and properties. However,
textual annotations of proteins, especially the annotations for protein
domains, which directly describe the protein's high-level functionalities,
properties, and their correlation with target amino acid sequences, remain
unexplored in the context of protein design tasks. In this paper, we propose
Protein-Annotation Alignment Generation, PAAG, a multi-modality protein design
framework that integrates the textual annotations extracted from protein
database for controllable generation in sequence space. Specifically, within a
multi-level alignment module, PAAG can explicitly generate proteins containing
specific domains conditioned on the corresponding domain annotations, and can
even design novel proteins with flexible combinations of different kinds of
annotations. Our experimental results underscore the superiority of the aligned
protein representations from PAAG over 7 prediction tasks. Furthermore, PAAG
demonstrates a significant increase in generation success rate (24.7% vs 4.7%
in zinc finger, and 54.3% vs 22.0% in the immunoglobulin domain) in comparison
to the existing model. We anticipate that PAAG will broaden the horizons of
protein design by leveraging the knowledge from between textual annotation and
proteins.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CGGM: A conditional graph generation model with adaptive sparsity for
  node anomaly detection in IoT networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17363v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17363v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Munan Li, Xianshi Su, Runze Ma, Tongbang Jiang, Zijian Li, Tony Q. S. Quek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic graphs are extensively employed for detecting anomalous behavior in
nodes within the Internet of Things (IoT). Graph generative models are often
used to address the issue of imbalanced node categories in dynamic graphs.
Nevertheless, the constraints it faces include the monotonicity of adjacency
relationships, the difficulty in constructing multi-dimensional features for
nodes, and the lack of a method for end-to-end generation of multiple
categories of nodes. In this paper, we propose a novel graph generation model,
called CGGM, specifically for generating samples belonging to the minority
class. The framework consists two core module: a conditional graph generation
module and a graph-based anomaly detection module. The generative module adapts
to the sparsity of the matrix by downsampling a noise adjacency matrix, and
incorporates a multi-dimensional feature encoder based on multi-head
self-attention to capture latent dependencies among features. Additionally, a
latent space constraint is combined with the distribution distance to
approximate the latent distribution of real data. The graph-based anomaly
detection module utilizes the generated balanced dataset to predict the node
behaviors. Extensive experiments have shown that CGGM outperforms the
state-of-the-art methods in terms of accuracy and divergence. The results also
demonstrate CGGM can generated diverse data categories, that enhancing the
performance of multi-category classification task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guiding Vision-Language Model Selection for Visual Question-Answering
  Across Tasks, Domains, and Knowledge Types <span class="chip">COLING</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09269v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09269v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neelabh Sinha, Vinija Jain, Aman Chadha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Question-Answering (VQA) has become key to user experience,
particularly after improved generalization capabilities of Vision-Language
Models (VLMs). But evaluating VLMs for an application requirement using a
standardized framework in practical settings is still challenging. This paper
aims to solve that using an end-to-end framework. We present VQA360 - a novel
dataset derived from established VQA benchmarks, annotated with task types,
application domains, and knowledge types, for a comprehensive evaluation. We
also introduce GoEval, a multimodal evaluation metric developed using GPT-4o,
achieving a correlation factor of 56.71% with human judgments. Our experiments
with state-of-the-art VLMs reveal that no single model excels universally,
thus, making a right choice a key design decision. Proprietary models such as
Gemini-1.5-Pro and GPT-4o-mini generally outperform others, but open-source
models like InternVL-2-8B and CogVLM-2-Llama-3-19B also demonstrate competitive
strengths, while providing additional advantages. Our framework can also be
extended to other tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at The First Workshop of Evaluation of Multi-Modal
  Generation (EvalMG) in 31st International Conference on Computational
  Linguistics (COLING), 2025. 8 pages + references + 6 pages of Appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VickreyFeedback: Cost-efficient Data Construction for Reinforcement
  Learning from Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18417v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18417v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxi Zhang, Jiuding Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the cost-efficiency aspect of Reinforcement Learning
from Human Feedback (RLHF). RLHF leverages datasets of human preferences over
outputs of large language models (LLM)s to instill human expectations into
LLMs. Although preference annotation comes with a monetized cost, the economic
utility of a preference dataset has not been considered by far. What
exacerbates this situation is that, given complex intransitive or cyclic
relationships in preference datasets, existing algorithms for fine-tuning LLMs
are still far from capturing comprehensive preferences. This raises severe
cost-efficiency concerns in production environments, where preference data
accumulate over time. In this paper, we discuss the fine-tuning of LLMs as a
monetized economy and introduce an auction mechanism to improve the efficiency
of preference data collection in dollar terms. We show that introducing an
auction mechanism can play an essential role in enhancing the cost-efficiency
of RLHF, while maintaining satisfactory model performance. Experimental results
demonstrate that our proposed auction-based protocol is cost-effective for
fine-tuning LLMs concentrating on high-quality feedback.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learn To be Efficient: Build Structured Sparsity in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06126v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06126v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haizhong Zheng, Xiaoyan Bai, Xueshen Liu, Z. Morley Mao, Beidi Chen, Fan Lai, Atul Prakash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved remarkable success with their
billion-level parameters, yet they incur high inference overheads. The
emergence of activation sparsity in LLMs provides a natural approach to reduce
this cost by involving only parts of the parameters for inference. However,
existing methods only focus on utilizing this naturally formed activation
sparsity in a post-training setting, overlooking the potential for further
amplifying this inherent sparsity. In this paper, we hypothesize that LLMs can
learn to be efficient by achieving more structured activation sparsity. To
achieve this, we introduce a novel training algorithm, Learn-To-be-Efficient
(LTE), designed to train efficiency-aware LLMs to learn to activate fewer
neurons and achieve a better trade-off between sparsity and performance.
Furthermore, unlike SOTA MoEfication methods, which mainly focus on ReLU-based
models, LTE can also be applied to LLMs like LLaMA using non-ReLU activations.
Extensive evaluation on language understanding, language generation, and
instruction tuning tasks show that LTE consistently outperforms SOTA baselines.
Along with our hardware-aware custom kernel implementation, LTE reduces
LLaMA2-7B inference latency by 25% at 50% sparsity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TorchCP: A Python Library for Conformal Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12683v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12683v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianguo Huang, Jianqing Song, Xuanning Zhou, Bingyi Jing, Hongxin Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conformal Prediction (CP) has attracted great attention from the research
community due to its strict theoretical guarantees. However, researchers and
developers still face challenges of applicability and efficiency when applying
CP algorithms to deep learning models. In this paper, we introduce \torchcp, a
comprehensive PyTorch-based toolkit to strengthen the usability of CP for deep
learning models. \torchcp implements a wide range of post-hoc and training
methods of conformal prediction for various machine learning tasks, including
classification, regression, GNN, and LLM. Moreover, we provide user-friendly
interfaces and extensive evaluations to easily integrate CP algorithms into
specific tasks. Our \torchcp toolkit, built entirely with PyTorch, enables
high-performance GPU acceleration for deep learning models and mini-batch
computation on large-scale datasets. With the LGPL license, the code is
open-sourced at \url{https://github.com/ml-stat-Sustech/TorchCP} and will be
continuously updated.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">16</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representing Long Volumetric Video with Temporal Gaussian Hierarchy <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Xu, Yinghao Xu, Zhiyuan Yu, Sida Peng, Jiaming Sun, Hujun Bao, Xiaowei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper aims to address the challenge of reconstructing long volumetric
videos from multi-view RGB videos. Recent dynamic view synthesis methods
leverage powerful 4D representations, like feature grids or point cloud
sequences, to achieve high-quality rendering results. However, they are
typically limited to short (1~2s) video clips and often suffer from large
memory footprints when dealing with longer videos. To solve this issue, we
propose a novel 4D representation, named Temporal Gaussian Hierarchy, to
compactly model long volumetric videos. Our key observation is that there are
generally various degrees of temporal redundancy in dynamic scenes, which
consist of areas changing at different speeds. Motivated by this, our approach
builds a multi-level hierarchy of 4D Gaussian primitives, where each level
separately describes scene regions with different degrees of content change,
and adaptively shares Gaussian primitives to represent unchanged scene content
over different temporal segments, thus effectively reducing the number of
Gaussian primitives. In addition, the tree-like structure of the Gaussian
hierarchy allows us to efficiently represent the scene at a particular moment
with a subset of Gaussian primitives, leading to nearly constant GPU memory
usage during the training or rendering regardless of the video length.
Extensive experimental results demonstrate the superiority of our method over
alternative methods in terms of training cost, rendering speed, and storage
usage. To our knowledge, this work is the first approach capable of efficiently
handling minutes of volumetric video data while maintaining state-of-the-art
rendering quality. Our project page is available at:
https://zju3dv.github.io/longvolcap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGGRAPH Asia 2024 (TOG). Project page:
  https://zju3dv.github.io/longvolcap</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhisheng Zhong, Chengyao Wang, Yuqi Liu, Senqiao Yang, Longxiang Tang, Yuechen Zhang, Jingyao Li, Tianyuan Qu, Yanwei Li, Yukang Chen, Shaozuo Yu, Sitong Wu, Eric Lo, Shu Liu, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Multi-modal Large Language Models (MLLMs) evolve, expanding beyond
single-domain capabilities is essential to meet the demands for more versatile
and efficient AI. However, previous omni-models have insufficiently explored
speech, neglecting its integration with multi-modality. We introduce Lyra, an
efficient MLLM that enhances multimodal abilities, including advanced
long-speech comprehension, sound understanding, cross-modality efficiency, and
seamless speech interaction. To achieve efficiency and speech-centric
capabilities, Lyra employs three strategies: (1) leveraging existing
open-source large models and a proposed multi-modality LoRA to reduce training
costs and data requirements; (2) using a latent multi-modality regularizer and
extractor to strengthen the relationship between speech and other modalities,
thereby enhancing model performance; and (3) constructing a high-quality,
extensive dataset that includes 1.5M multi-modal (language, vision, audio) data
samples and 12K long speech samples, enabling Lyra to handle complex long
speech inputs and achieve more robust omni-cognition. Compared to other
omni-methods, Lyra achieves state-of-the-art performance on various
vision-language, vision-speech, and speech-language benchmarks, while also
using fewer computational resources and less training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video Seal: Open and Efficient Video Watermarking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Fernandez, Hady Elsahar, I. Zeki Yalniz, Alexandre Mourachko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of AI-generated content and sophisticated video editing
tools has made it both important and challenging to moderate digital platforms.
Video watermarking addresses these challenges by embedding imperceptible
signals into videos, allowing for identification. However, the rare open tools
and methods often fall short on efficiency, robustness, and flexibility. To
reduce these gaps, this paper introduces Video Seal, a comprehensive framework
for neural video watermarking and a competitive open-sourced model. Our
approach jointly trains an embedder and an extractor, while ensuring the
watermark robustness by applying transformations in-between, e.g., video
codecs. This training is multistage and includes image pre-training, hybrid
post-training and extractor fine-tuning. We also introduce temporal watermark
propagation, a technique to convert any image watermarking model to an
efficient video watermarking model without the need to watermark every
high-resolution frame. We present experimental results demonstrating the
effectiveness of the approach in terms of speed, imperceptibility, and
robustness. Video Seal achieves higher robustness compared to strong baselines
especially under challenging distortions combining geometric transformations
and video compression. Additionally, we provide new insights such as the impact
of video compression during training, and how to compare methods operating on
different payloads. Contributions in this work - including the codebase,
models, and a public demo - are open-sourced under permissive licenses to
foster further research and development in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at https://github.com/facebookresearch/videoseal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Music Generation with Explicit Bridges and Retrieval
  Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baisen Wang, Le Zhuo, Zhaokai Wang, Chenxi Bao, Wu Chengjing, Xuecheng Nie, Jiao Dai, Jizhong Han, Yue Liao, Si Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal music generation aims to produce music from diverse input
modalities, including text, videos, and images. Existing methods use a common
embedding space for multimodal fusion. Despite their effectiveness in other
modalities, their application in multimodal music generation faces challenges
of data scarcity, weak cross-modal alignment, and limited controllability. This
paper addresses these issues by using explicit bridges of text and music for
multimodal alignment. We introduce a novel method named Visuals Music Bridge
(VMB). Specifically, a Multimodal Music Description Model converts visual
inputs into detailed textual descriptions to provide the text bridge; a
Dual-track Music Retrieval module that combines broad and targeted retrieval
strategies to provide the music bridge and enable user control. Finally, we
design an Explicitly Conditioned Music Generation framework to generate music
based on the two bridges. We conduct experiments on video-to-music,
image-to-music, text-to-music, and controllable music generation tasks, along
with experiments on controllability. The results demonstrate that VMB
significantly enhances music quality, modality, and customization alignment
compared to previous methods. VMB sets a new standard for interpretable and
expressive multimodal music generation with applications in various multimedia
fields. Demos and code are available at https://github.com/wbs2788/VMB.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Graphical Models for Vision-Language Compositional Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fiorenzo Parascandolo, Nicholas Moratelli, Enver Sangineto, Lorenzo Baraldi, Rita Cucchiara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has empirically shown that Vision-Language Models (VLMs) struggle
to fully understand the compositional properties of the human language, usually
modeling an image caption as a "bag of words". As a result, they perform poorly
on compositional tasks, which require a deeper understanding of the different
entities of a sentence (subject, verb, etc.) jointly with their mutual
relationships in order to be solved. In this paper, we model the dependency
relations among textual and visual tokens using a Causal Graphical Model (CGM),
built using a dependency parser, and we train a decoder conditioned by the VLM
visual encoder. Differently from standard autoregressive or parallel
predictions, our decoder's generative process is partially-ordered following
the CGM structure. This structure encourages the decoder to learn only the main
causal dependencies in a sentence discarding spurious correlations. Using
extensive experiments on five compositional benchmarks, we show that our method
significantly outperforms all the state-of-the-art compositional approaches by
a large margin, and it also improves over methods trained using much larger
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Open-Vocabulary Video Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinhao Li, Yun Liu, Guolei Sun, Min Wu, Le Zhang, Ce Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation in videos has been a focal point of recent research.
However, existing models encounter challenges when faced with unfamiliar
categories. To address this, we introduce the Open Vocabulary Video Semantic
Segmentation (OV-VSS) task, designed to accurately segment every pixel across a
wide range of open-vocabulary categories, including those that are novel or
previously unexplored. To enhance OV-VSS performance, we propose a robust
baseline, OV2VSS, which integrates a spatial-temporal fusion module, allowing
the model to utilize temporal relationships across consecutive frames.
Additionally, we incorporate a random frame enhancement module, broadening the
model's understanding of semantic context throughout the entire video sequence.
Our approach also includes video text encoding, which strengthens the model's
capability to interpret textual information within the video context.
Comprehensive evaluations on benchmark datasets such as VSPW and Cityscapes
highlight OV-VSS's zero-shot generalization capabilities, especially in
handling novel categories. The results validate OV2VSS's effectiveness,
demonstrating improved performance in semantic segmentation tasks across
diverse video datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Sentiment Analysis based on Video and Audio Inputs <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Fernandez, Suzan Awinat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the abundance of current researches working on the sentiment analysis
from videos and audios, finding the best model that gives the highest accuracy
rate is still considered a challenge for researchers in this field. The main
objective of this paper is to prove the usability of emotion recognition models
that take video and audio inputs. The datasets used to train the models are the
CREMA-D dataset for audio and the RAVDESS dataset for video. The fine-tuned
models that been used are: Facebook/wav2vec2-large for audio and the
Google/vivit-b-16x2-kinetics400 for video. The avarage of the probabilities for
each emotion generated by the two previous models is utilized in the decision
making framework. After disparity in the results, if one of the models gets
much higher accuracy, another test framework is created. The methods used are
the Weighted Average method, the Confidence Level Threshold method, the Dynamic
Weighting Based on Confidence method, and the Rule-Based Logic method. This
limited approach gives encouraging results that make future research into these
methods viable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented as a full paper in the 15th International Conference on
  Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2024) October
  28-30, 2024, Leuven, Belgium</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ YingSound: Video-Guided Sound Effects Generation with Multi-modal
  Chain-of-Thought Controls 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09168v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09168v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Chen, Haomin Zhang, Xinhan Di, Haoyu Wang, Sizhe Shan, Junjie Zheng, Yunming Liang, Yihan Fan, Xinfa Zhu, Wenjie Tian, Yihua Wang, Chaofan Ding, Lei Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating sound effects for product-level videos, where only a small amount
of labeled data is available for diverse scenes, requires the production of
high-quality sounds in few-shot settings. To tackle the challenge of limited
labeled data in real-world scenes, we introduce YingSound, a foundation model
designed for video-guided sound generation that supports high-quality audio
generation in few-shot settings. Specifically, YingSound consists of two major
modules. The first module uses a conditional flow matching transformer to
achieve effective semantic alignment in sound generation across audio and
visual modalities. This module aims to build a learnable audio-visual
aggregator (AVA) that integrates high-resolution visual features with
corresponding audio features at multiple stages. The second module is developed
with a proposed multi-modal visual-audio chain-of-thought (CoT) approach to
generate finer sound effects in few-shot settings. Finally, an
industry-standard video-to-audio (V2A) dataset that encompasses various
real-world scenarios is presented. We show that YingSound effectively generates
high-quality synchronized sounds across diverse conditional inputs through
automated evaluations and human studies. Project Page:
\url{https://giantailab.github.io/yingsound/}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Modality Representation and Alignment for Multimodal
  Cold-start Active Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Shen, Yake Wei, Jianxiong Yin, Deepu Rajan, Di Hu, Simon See
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training multimodal models requires a large amount of labeled data. Active
learning (AL) aim to reduce labeling costs. Most AL methods employ warm-start
approaches, which rely on sufficient labeled data to train a well-calibrated
model that can assess the uncertainty and diversity of unlabeled data. However,
when assembling a dataset, labeled data are often scarce initially, leading to
a cold-start problem. Additionally, most AL methods seldom address multimodal
data, highlighting a research gap in this field. Our research addresses these
issues by developing a two-stage method for Multi-Modal Cold-Start Active
Learning (MMCSAL).
  Firstly, we observe the modality gap, a significant distance between the
centroids of representations from different modalities, when only using
cross-modal pairing information as self-supervision signals. This modality gap
affects data selection process, as we calculate both uni-modal and cross-modal
distances. To address this, we introduce uni-modal prototypes to bridge the
modality gap. Secondly, conventional AL methods often falter in multimodal
scenarios where alignment between modalities is overlooked. Therefore, we
propose enhancing cross-modal alignment through regularization, thereby
improving the quality of selected multimodal data pairs in AL. Finally, our
experiments demonstrate MMCSAL's efficacy in selecting multimodal data pairs
across three multimodal datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, ACMMM Asia 2024, Oral Presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MS2Mesh-XR: Multi-modal Sketch-to-Mesh Generation in XR Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09008v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09008v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Tong, Yue Qiu, Ruiyang Li, Shi Qiu, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present MS2Mesh-XR, a novel multi-modal sketch-to-mesh generation pipeline
that enables users to create realistic 3D objects in extended reality (XR)
environments using hand-drawn sketches assisted by voice inputs. In specific,
users can intuitively sketch objects using natural hand movements in mid-air
within a virtual environment. By integrating voice inputs, we devise ControlNet
to infer realistic images based on the drawn sketches and interpreted text
prompts. Users can then review and select their preferred image, which is
subsequently reconstructed into a detailed 3D mesh using the Convolutional
Reconstruction Model. In particular, our proposed pipeline can generate a
high-quality 3D mesh in less than 20 seconds, allowing for immersive
visualization and manipulation in run-time XR scenes. We demonstrate the
practicability of our pipeline through two use cases in XR settings. By
leveraging natural user inputs and cutting-edge generative AI capabilities, our
approach can significantly facilitate XR-based creative production and enhance
user experiences. Our code and demo will be available at:
https://yueqiu0911.github.io/MS2Mesh-XR/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE AIxVR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EmoDubber: Towards High Quality and Emotion Controllable Movie Dubbing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaoxiang Cong, Jiadong Pan, Liang Li, Yuankai Qi, Yuxin Peng, Anton van den Hengel, Jian Yang, Qingming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a piece of text, a video clip, and a reference audio, the movie dubbing
task aims to generate speech that aligns with the video while cloning the
desired voice. The existing methods have two primary deficiencies: (1) They
struggle to simultaneously hold audio-visual sync and achieve clear
pronunciation; (2) They lack the capacity to express user-defined emotions. To
address these problems, we propose EmoDubber, an emotion-controllable dubbing
architecture that allows users to specify emotion type and emotional intensity
while satisfying high-quality lip sync and pronunciation. Specifically, we
first design Lip-related Prosody Aligning (LPA), which focuses on learning the
inherent consistency between lip motion and prosody variation by duration level
contrastive learning to incorporate reasonable alignment. Then, we design
Pronunciation Enhancing (PE) strategy to fuse the video-level phoneme sequences
by efficient conformer to improve speech intelligibility. Next, the speaker
identity adapting module aims to decode acoustics prior and inject the speaker
style embedding. After that, the proposed Flow-based User Emotion Controlling
(FUEC) is used to synthesize waveform by flow matching prediction network
conditioned on acoustics prior. In this process, the FUEC determines the
gradient direction and guidance scale based on the user's emotion instructions
by the positive and negative guidance mechanism, which focuses on amplifying
the desired emotion while suppressing others. Extensive experimental results on
three benchmark datasets demonstrate favorable performance compared to several
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reversing the Damage: A QP-Aware <span class="highlight-title">Transformer</span>-Diffusion Approach for 8K
  Video Restoration under Codec Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Mollaahmadi Dehaghi, Reza Razavi, Mohammad Moshirpour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce DiQP; a novel Transformer-Diffusion model for
restoring 8K video quality degraded by codec compression. To the best of our
knowledge, our model is the first to consider restoring the artifacts
introduced by various codecs (AV1, HEVC) by Denoising Diffusion without
considering additional noise. This approach allows us to model the complex,
non-Gaussian nature of compression artifacts, effectively learning to reverse
the degradation. Our architecture combines the power of Transformers to capture
long-range dependencies with an enhanced windowed mechanism that preserves
spatiotemporal context within groups of pixels across frames. To further
enhance restoration, the model incorporates auxiliary "Look Ahead" and "Look
Around" modules, providing both future and surrounding frame information to aid
in reconstructing fine details and enhancing overall visual quality. Extensive
experiments on different datasets demonstrate that our model outperforms
state-of-the-art methods, particularly for high-resolution videos such as 4K
and 8K, showcasing its effectiveness in restoring perceptually pleasing videos
from highly compressed sources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Riemann-based Multi-scale Attention Reasoning Network for Text-3D
  Retrieval <span class="chip">AAAI25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13712v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13712v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenrui Li, Wei Han, Yandu Chen, Yeyu Chai, Yidan Lu, Xingtao Wang, Xiaopeng Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the challenges in acquiring paired Text-3D data and the inherent
irregularity of 3D data structures, combined representation learning of 3D
point clouds and text remains unexplored. In this paper, we propose a novel
Riemann-based Multi-scale Attention Reasoning Network (RMARN) for text-3D
retrieval. Specifically, the extracted text and point cloud features are
refined by their respective Adaptive Feature Refiner (AFR). Furthermore, we
introduce the innovative Riemann Local Similarity (RLS) module and the Global
Pooling Similarity (GPS) module. However, as 3D point cloud data and text data
often possess complex geometric structures in high-dimensional space, the
proposed RLS employs a novel Riemann Attention Mechanism to reflect the
intrinsic geometric relationships of the data. Without explicitly defining the
manifold, RMARN learns the manifold parameters to better represent the
distances between text-point cloud samples. To address the challenges of
lacking paired text-3D data, we have created the large-scale Text-3D Retrieval
dataset T3DR-HIT, which comprises over 3,380 pairs of text and point cloud
data. T3DR-HIT contains coarse-grained indoor 3D scenes and fine-grained
Chinese artifact scenes, consisting of 1,380 and over 2,000 text-3D pairs,
respectively. Experiments on our custom datasets demonstrate the superior
performance of the proposed method. Our code and proposed datasets are
available at \url{https://github.com/liwrui/RMARN}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OneAdapt: Fast Configuration Adaptation for Video Analytics Applications
  via Backpropagation <span class="chip">SoCC' 23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02422v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02422v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuntai Du, Yuhan Liu, Yitian Hao, Qizheng Zhang, Haodong Wang, Yuyang Huang, Ganesh Ananthanarayanan, Junchen Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning inference on streaming media data, such as object detection in
video or LiDAR feeds and text extraction from audio waves, is now ubiquitous.
To achieve high inference accuracy, these applications typically require
significant network bandwidth to gather high-fidelity data and extensive GPU
resources to run deep neural networks (DNNs). While the high demand for network
bandwidth and GPU resources could be substantially reduced by optimally
adapting the configuration knobs, such as video resolution and frame rate,
current adaptation techniques fail to meet three requirements simultaneously:
adapt configurations (i) with minimum extra GPU or bandwidth overhead; (ii) to
reach near-optimal decisions based on how the data affects the final DNN's
accuracy, and (iii) do so for a range of configuration knobs. This paper
presents OneAdapt, which meets these requirements by leveraging a
gradient-ascent strategy to adapt configuration knobs. The key idea is to
embrace DNNs' differentiability to quickly estimate the accuracy's gradient to
each configuration knob, called AccGrad. Specifically, OneAdapt estimates
AccGrad by multiplying two gradients: InputGrad (i.e. how each configuration
knob affects the input to the DNN) and DNNGrad (i.e. how the DNN input affects
the DNN inference output). We evaluate OneAdapt across five types of
configurations, four analytic tasks, and five types of input data. Compared to
state-of-the-art adaptation schemes, OneAdapt cuts bandwidth usage and GPU
usage by 15-59% while maintaining comparable accuracy or improves accuracy by
1-5% while using equal or fewer resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SoCC' 23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Agent Journey Beyond RGB: Unveiling Hybrid Semantic-Spatial
  Environmental Representations for Vision-and-Language Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06465v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06465v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuesong Zhang, Yunbo Xu, Jia Li, Zhenzhen Hu, Richnag Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigating unseen environments based on natural language instructions remains
difficult for egocentric agents in Vision-and-Language Navigation (VLN). While
recent advancements have yielded promising outcomes, they primarily rely on RGB
images for environmental representation, often overlooking the underlying
semantic knowledge and spatial cues. Intuitively, humans inherently ground
textual semantics within the spatial layout during indoor navigation. Inspired
by this, we propose a versatile Semantic Understanding and Spatial Awareness
(SUSA) architecture to facilitate navigation. SUSA includes a Textual Semantic
Understanding (TSU) module, which narrows the modality gap between instructions
and environments by generating and associating the descriptions of
environmental landmarks in the agent's immediate surroundings. Additionally, a
Depth-based Spatial Perception (DSP) module incrementally constructs a depth
exploration map, enabling a more nuanced comprehension of environmental
layouts. Experimental results demonstrate that SUSA hybrid semantic-spatial
representations effectively enhance navigation performance, setting new
state-of-the-art performance across three VLN benchmarks (REVERIE, R2R, and
SOON). The source code will be publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A technical report consisting of 16 pages, 12 figures, 10 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DriveMM: All-in-One Large Multimodal Model for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07689v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07689v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijian Huang, Chengjian Feng, Feng Yan, Baihui Xiao, Zequn Jie, Yujie Zhong, Xiaodan Liang, Lin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Multimodal Models (LMMs) have demonstrated exceptional comprehension
and interpretation capabilities in Autonomous Driving (AD) by incorporating
large language models. Despite the advancements, current data-driven AD
approaches tend to concentrate on a single dataset and specific tasks,
neglecting their overall capabilities and ability to generalize. To bridge
these gaps, we propose DriveMM, a general large multimodal model designed to
process diverse data inputs, such as images and multi-view videos, while
performing a broad spectrum of AD tasks, including perception, prediction, and
planning. Initially, the model undergoes curriculum pre-training to process
varied visual signals and perform basic visual comprehension and perception
tasks. Subsequently, we augment and standardize various AD-related datasets to
fine-tune the model, resulting in an all-in-one LMM for autonomous driving. To
assess the general capabilities and generalization ability, we conduct
evaluations on six public benchmarks and undertake zero-shot transfer on an
unseen dataset, where DriveMM achieves state-of-the-art performance across all
tasks. We hope DriveMM as a promising solution for future end-to-end autonomous
driving applications in the real world. Project page with code:
https://github.com/zhijian11/DriveMM.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-11T00:00:00Z">2024-12-11</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">46</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Concept Models: Language Modeling in a Sentence Representation
  Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         The LCM team, Loïc Barrault, Paul-Ambroise Duquenne, Maha Elbayad, Artyom Kozhevnikov, Belen Alastruey, Pierre Andrews, Mariano Coria, Guillaume Couairon, Marta R. Costa-jussà, David Dale, Hady Elsahar, Kevin Heffernan, João Maria Janeiro, Tuan Tran, Christophe Ropers, Eduardo Sánchez, Robin San Roman, Alexandre Mourachko, Safiyyah Saleem, Holger Schwenk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs have revolutionized the field of artificial intelligence and have
emerged as the de-facto tool for many tasks. The current established technology
of LLMs is to process input and generate output at the token level. This is in
sharp contrast to humans who operate at multiple levels of abstraction, well
beyond single words, to analyze information and to generate creative content.
In this paper, we present an attempt at an architecture which operates on an
explicit higher-level semantic representation, which we name a concept.
Concepts are language- and modality-agnostic and represent a higher level idea
or action in a flow. Hence, we build a "Large Concept Model". In this study, as
proof of feasibility, we assume that a concept corresponds to a sentence, and
use an existing sentence embedding space, SONAR, which supports up to 200
languages in both text and speech modalities.
  The Large Concept Model is trained to perform autoregressive sentence
prediction in an embedding space. We explore multiple approaches, namely MSE
regression, variants of diffusion-based generation, and models operating in a
quantized SONAR space. These explorations are performed using 1.6B parameter
models and training data in the order of 1.3T tokens. We then scale one
architecture to a model size of 7B parameters and training data of about 2.7T
tokens. We perform an experimental evaluation on several generative tasks,
namely summarization and a new task of summary expansion. Finally, we show that
our model exhibits impressive zero-shot generalization performance to many
languages, outperforming existing LLMs of the same size. The training code of
our models is freely available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>49 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ jina-clip-v2: Multilingual Multimodal Embeddings for Text and Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Koukounas, Georgios Mastrapas, Bo Wang, Mohammad Kalim Akram, Sedigheh Eslami, Michael Günther, Isabelle Mohr, Saba Sturua, Scott Martens, Nan Wang, Han Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language-Image Pretraining (CLIP) is a highly effective method
for aligning images and texts in a shared embedding space. These models are
widely used for tasks such as cross-modal information retrieval and multi-modal
understanding. However, CLIP models often struggle with text-only tasks,
underperforming compared to specialized text models. This performance disparity
forces retrieval systems to rely on separate models for text-only and
multi-modal tasks. In this work, we build upon our previous model,
jina-clip-v1, by introducing a refined framework that utilizes multi-task,
multi-stage contrastive learning across multiple languages, coupled with an
improved training recipe to enhance text-only retrieval. The resulting model,
jina-clip-v2, outperforms its predecessor on text-only and multimodal tasks,
while adding multilingual support, better understanding of complex visual
documents and efficiency gains thanks to Matryoshka Representation Learning and
vector truncation. The model performs comparably to the state-of-the-art in
both multilingual-multimodal and multilingual text retrieval benchmarks,
addressing the challenge of unifying text-only and multi-modal retrieval
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 1-10 main paper, 10-12 refs, 12-21 benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Coverage-based Fairness in Multi-document Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyuan Li, Yusen Zhang, Rui Zhang, Snigdha Chaturvedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fairness in multi-document summarization (MDS) measures whether a system can
generate a summary fairly representing information from documents with
different social attribute values. Fairness in MDS is crucial since a fair
summary can offer readers a comprehensive view. Previous works focus on
quantifying summary-level fairness using Proportional Representation, a
fairness measure based on Statistical Parity. However, Proportional
Representation does not consider redundancy in input documents and overlooks
corpus-level unfairness. In this work, we propose a new summary-level fairness
measure, Equal Coverage, which is based on coverage of documents with different
social attribute values and considers the redundancy within documents. To
detect the corpus-level unfairness, we propose a new corpus-level measure,
Coverage Parity. Our human evaluations show that our measures align more with
our definition of fairness. Using our measures, we evaluate the fairness of
thirteen different LLMs. We find that Claude3-sonnet is the fairest among all
evaluated LLMs. We also find that almost all LLMs overrepresent different
social attribute values.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BDA: Bangla Text Data Augmentation Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md. Tariquzzaman, Audwit Nafi Anam, Naimul Haque, Mohsinul Kabir, Hasan Mahmud, Md Kamrul Hasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data augmentation involves generating synthetic samples that resemble those
in a given dataset. In resource-limited fields where high-quality data is
scarce, augmentation plays a crucial role in increasing the volume of training
data. This paper introduces a Bangla Text Data Augmentation (BDA) Framework
that uses both pre-trained models and rule-based methods to create new variants
of the text. A filtering process is included to ensure that the new text keeps
the same meaning as the original while also adding variety in the words used.
We conduct a comprehensive evaluation of the framework's effectiveness in
Bangla text classification tasks. Our framework achieved significant
improvement in F1 scores across five distinct datasets, delivering performance
equivalent to models trained on 100\% of the data while utilizing only 50\% of
the training dataset. Additionally, we explore the impact of data scarcity by
progressively reducing the training data and augmenting it through BDA,
resulting in notable F1 score enhancements. The study offers a thorough
examination of BDA's performance, identifying key factors for optimal results
and addressing its limitations through detailed analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-Context Learning with Topological Information for Knowledge Graph
  Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Udari Madhushani Sehwag, Kassiani Papasotiriou, Jared Vann, Sumitra Ganesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graphs (KGs) are crucial for representing and reasoning over
structured information, supporting a wide range of applications such as
information retrieval, question answering, and decision-making. However, their
effectiveness is often hindered by incompleteness, limiting their potential for
real-world impact. While knowledge graph completion (KGC) has been extensively
studied in the literature, recent advances in generative AI models,
particularly large language models (LLMs), have introduced new opportunities
for innovation. In-context learning has recently emerged as a promising
approach for leveraging pretrained knowledge of LLMs across a range of natural
language processing tasks and has been widely adopted in both academia and
industry. However, how to utilize in-context learning for effective KGC remains
relatively underexplored. We develop a novel method that incorporates
topological information through in-context learning to enhance KGC performance.
By integrating ontological knowledge and graph structure into the context of
LLMs, our approach achieves strong performance in the transductive setting
i.e., nodes in the test graph dataset are present in the training graph
dataset. Furthermore, we apply our approach to KGC in the more challenging
inductive setting, i.e., nodes in the training graph dataset and test graph
dataset are disjoint, leveraging the ontology to infer useful information about
missing nodes which serve as contextual cues for the LLM during inference. Our
method demonstrates superior performance compared to baselines on the
ILPC-small and ILPC-large datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity
  Visual Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Zhang, Ollie Liu, Tianyu Yu, Jinyi Hu, Willie Neiswanger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have made rapid progress in recent
years, yet continue to struggle with low-level visual perception (LLVP) --
particularly the ability to accurately describe the geometric details of an
image. This capability is crucial for applications in areas such as robotics,
medical image analysis, and manufacturing. In this paper, we first introduce
Geoperception, a benchmark designed to evaluate an MLLM's ability to accurately
transcribe 2D geometric information from an image. Using this benchmark, we
demonstrate the limitations of leading MLLMs, and then conduct a comprehensive
empirical study to explore strategies for improving their performance on
geometric tasks. Our findings highlight the benefits of certain model
architectures, training techniques, and data strategies, including the use of
high-fidelity synthetic data and multi-stage training with a data curriculum.
Notably, we find that a data curriculum enables models to learn challenging
geometry understanding tasks which they fail to learn from scratch. Leveraging
these insights, we develop Euclid, a family of models specifically optimized
for strong low-level geometric perception. Although purely trained on synthetic
multimodal data, Euclid shows strong generalization ability to novel geometry
shapes. For instance, Euclid outperforms the best closed-source model,
Gemini-1.5-Pro, by up to 58.56% on certain Geoperception benchmark tasks and
10.65% on average across all tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 22 figures, 5 tables, 7 algorithms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LatentQA: Teaching LLMs to Decode Activations Into Natural Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Pan, Lijie Chen, Jacob Steinhardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpretability methods seek to understand language model representations,
yet the outputs of most such methods -- circuits, vectors, scalars -- are not
immediately human-interpretable. In response, we introduce LatentQA, the task
of answering open-ended questions about model activations in natural language.
Towards solving LatentQA, we propose Latent Interpretation Tuning (LIT), which
finetunes a decoder LLM on a dataset of activations and associated
question-answer pairs, similar to how visual instruction tuning trains on
question-answer pairs associated with images. We use the decoder for diverse
reading applications, such as extracting relational knowledge from
representations or uncovering system prompts governing model behavior. Our
decoder also specifies a differentiable loss that we use to control models,
such as debiasing models on stereotyped sentences and controlling the sentiment
of generations. Finally, we extend LatentQA to reveal harmful model
capabilities, such as generating recipes for bioweapons and code for hacking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page is at https://latentqa.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast <span class="highlight-title">Prompt</span> Alignment for Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khalil Mrini, Hanlin Lu, Linjie Yang, Weilin Huang, Heng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image generation has advanced rapidly, yet aligning complex textual
prompts with generated visuals remains challenging, especially with intricate
object relationships and fine-grained details. This paper introduces Fast
Prompt Alignment (FPA), a prompt optimization framework that leverages a
one-pass approach, enhancing text-to-image alignment efficiency without the
iterative overhead typical of current methods like OPT2I. FPA uses large
language models (LLMs) for single-iteration prompt paraphrasing, followed by
fine-tuning or in-context learning with optimized prompts to enable real-time
inference, reducing computational demands while preserving alignment fidelity.
Extensive evaluations on the COCO Captions and PartiPrompts datasets
demonstrate that FPA achieves competitive text-image alignment scores at a
fraction of the processing time, as validated through both automated metrics
(TIFA, VQA) and human evaluation. A human study with expert annotators further
reveals a strong correlation between human alignment judgments and automated
scores, underscoring the robustness of FPA's improvements. The proposed method
showcases a scalable, efficient alternative to iterative prompt optimization,
enabling broader applicability in real-time, high-demand settings. The codebase
is provided to facilitate further research:
https://github.com/tiktok/fast_prompt_alignment
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TikTok Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Latent Language Modeling with Next-Token Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutao Sun, Hangbo Bao, Wenhui Wang, Zhiliang Peng, Li Dong, Shaohan Huang, Jianyong Wang, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal generative models require a unified approach to handle both
discrete data (e.g., text and code) and continuous data (e.g., image, audio,
video). In this work, we propose Latent Language Modeling (LatentLM), which
seamlessly integrates continuous and discrete data using causal Transformers.
Specifically, we employ a variational autoencoder (VAE) to represent continuous
data as latent vectors and introduce next-token diffusion for autoregressive
generation of these vectors. Additionally, we develop $\sigma$-VAE to address
the challenges of variance collapse, which is crucial for autoregressive
modeling. Extensive experiments demonstrate the effectiveness of LatentLM
across various modalities. In image generation, LatentLM surpasses Diffusion
Transformers in both performance and scalability. When integrated into
multimodal large language models, LatentLM provides a general-purpose interface
that unifies multimodal generation and understanding. Experimental results show
that LatentLM achieves favorable performance compared to Transfusion and vector
quantized models in the setting of scaling up training tokens. In
text-to-speech synthesis, LatentLM outperforms the state-of-the-art VALL-E 2
model in speaker similarity and robustness, while requiring 10x fewer decoding
steps. The results establish LatentLM as a highly effective and scalable
approach to advance large multimodal models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting the Index Gradients for Optimization-Based Jailbreaking on
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahui Li, Yongchang Hao, Haoyu Xu, Xing Wang, Yu Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the advancements in training Large Language Models (LLMs) with
alignment techniques to enhance the safety of generated content, these models
remain susceptible to jailbreak, an adversarial attack method that exposes
security vulnerabilities in LLMs. Notably, the Greedy Coordinate Gradient (GCG)
method has demonstrated the ability to automatically generate adversarial
suffixes that jailbreak state-of-the-art LLMs. However, the optimization
process involved in GCG is highly time-consuming, rendering the jailbreaking
pipeline inefficient. In this paper, we investigate the process of GCG and
identify an issue of Indirect Effect, the key bottleneck of the GCG
optimization. To this end, we propose the Model Attack Gradient Index GCG
(MAGIC), that addresses the Indirect Effect by exploiting the gradient
information of the suffix tokens, thereby accelerating the procedure by having
less computation and fewer iterations. Our experiments on AdvBench show that
MAGIC achieves up to a 1.5x speedup, while maintaining Attack Success Rates
(ASR) on par or even higher than other baselines. Our MAGIC achieved an ASR of
74% on the Llama-2 and an ASR of 54% when conducting transfer attacks on
GPT-3.5. Code is available at https://github.com/jiah-li/magic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages,2 figures, accepted by The 31st International Conference on
  Computational Linguistics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Der Effizienz- und Intelligenzbegriff in der Lexikographie und
  kuenstlichen Intelligenz: kann Chat<span class="highlight-title">GPT</span> die lexikographische Textsorte
  nachbilden? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Arias-Arias, Maria Jose Dominguez Vazquez, Carlos Valcarcel Riveiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By means of pilot experiments for the language pair German and Galician, this
paper examines the concept of efficiency and intelligence in lexicography and
artificial intelligence, AI. The aim of the experiments is to gain empirically
and statistically based insights into the lexicographical text type,dictionary
article, in the responses of ChatGPT 3.5, as well as into the lexicographical
data on which this chatbot was trained. Both quantitative and qualitative
methods are used for this purpose. The analysis is based on the evaluation of
the outputs of several sessions with the same prompt in ChatGPT 3.5. On the one
hand, the algorithmic performance of intelligent systems is evaluated in
comparison with data from lexicographical works. On the other hand, the ChatGPT
data supplied is analysed using specific text passages of the aforementioned
lexicographical text type. The results of this study not only help to evaluate
the efficiency of this chatbot regarding the creation of dictionary articles,
but also to delve deeper into the concept of intelligence, the thought
processes and the actions to be carried out in both disciplines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, in German language</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Single- and Multi-task Text Classification through Large
  Language Model Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Zhao, Qile P. Chen, Yijing Barry Zhang, Gang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Both encoder-only models (e.g., BERT, RoBERTa) and large language models
(LLMs, e.g., Llama3) have been widely used for text classification tasks.
However, there is a lack of systematic studies comparing the performance of
encoder-based models and LLMs in text classification, particularly when
fine-tuning is involved. This study employed a diverse range of models and
methods, varying in size and architecture, and including both fine-tuned and
pre-trained approaches. We first assessed the performances of these LLMs on the
20 Newsgroups (20NG) and MASSIVE datasets, comparing them to encoder-only
RoBERTa models. Additionally, we explored the multi-task capabilities of both
model types by combining multiple classification tasks, including intent
detection and slot-filling, into a single model using data from both datasets.
Our results indicate that fully fine-tuned Llama3-70B models outperform
RoBERTa-large and other decoder LLMs across various classification tasks and
datasets. Moreover, the consolidated multi-task fine-tuned LLMs matched the
performance of dual-model setups in both tasks across both datasets. Overall,
our study provides a comprehensive benchmark of encoder-only and LLM models on
text classification tasks and demonstrates a method to combine two or more
fully fine-tuned decoder LLMs for reduced latency and equivalent performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning Information Retrieval and Summarisation to Support
  Systematic <span class="highlight-title">Review</span> on Outcomes Based Contracting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iman Munire Bilal, Zheng Fang, Miguel Arana-Catania, Felix-Anselm van Lier, Juliana Outes Velarde, Harry Bregazzi, Eleanor Carter, Mara Airoldi, Rob Procter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As academic literature proliferates, traditional review methods are
increasingly challenged by the sheer volume and diversity of available
research. This article presents a study that aims to address these challenges
by enhancing the efficiency and scope of systematic reviews in the social
sciences through advanced machine learning (ML) and natural language processing
(NLP) tools. In particular, we focus on automating stages within the systematic
reviewing process that are time-intensive and repetitive for human annotators
and which lend themselves to immediate scalability through tools such as
information retrieval and summarisation guided by expert advice. The article
concludes with a summary of lessons learnt regarding the integrated approach
towards systematic reviews and future directions for improvement, including
explainability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can We Generate Visual Programs Without <span class="highlight-title">Prompt</span>ing LLMs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Shlapentokh-Rothman, Yu-Xiong Wang, Derek Hoiem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual programming prompts LLMs (large language mod-els) to generate
executable code for visual tasks like visual question answering (VQA).
Prompt-based methods are difficult to improve while also being unreliable and
costly in both time and money. Our goal is to develop an efficient visual
programming system without 1) using prompt-based LLMs at inference time and 2)
a large set of program and answer annotations. We develop a synthetic data
augmentation approach and alternative program generation method based on
decoupling programs into higher-level skills called templates and the
corresponding arguments. Our results show that with data augmentation,
prompt-free smaller LLMs ($\approx$ 1B parameters) are competitive with
state-of-the art models with the added benefit of much faster inference
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bilevel Joint Unsupervised and Supervised Training for Automatic Speech
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaodong Cui, A F M Saif, Songtao Lu, Lisha Chen, Tianyi Chen, Brian Kingsbury, George Saon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a bilevel joint unsupervised and supervised
training (BL-JUST) framework for automatic speech recognition. Compared to the
conventional pre-training and fine-tuning strategy which is a disconnected
two-stage process, BL-JUST tries to optimize an acoustic model such that it
simultaneously minimizes both the unsupervised and supervised loss functions.
Because BL-JUST seeks matched local optima of both loss functions, acoustic
representations learned by the acoustic model strike a good balance between
being generic and task-specific. We solve the BL-JUST problem using
penalty-based bilevel gradient descent and evaluate the trained deep neural
network acoustic models on various datasets with a variety of architectures and
loss functions. We show that BL-JUST can outperform the widely-used
pre-training and fine-tuning strategy and some other popular semi-supervised
techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE/ACM Transactions on Audio, Speech and Language
  Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MaestroMotif: Skill Design from Artificial Intelligence Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Klissarov, Mikael Henaff, Roberta Raileanu, Shagun Sodhani, Pascal Vincent, Amy Zhang, Pierre-Luc Bacon, Doina Precup, Marlos C. Machado, Pierluca D'Oro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Describing skills in natural language has the potential to provide an
accessible way to inject human knowledge about decision-making into an AI
system. We present MaestroMotif, a method for AI-assisted skill design, which
yields high-performing and adaptable agents. MaestroMotif leverages the
capabilities of Large Language Models (LLMs) to effectively create and reuse
skills. It first uses an LLM's feedback to automatically design rewards
corresponding to each skill, starting from their natural language description.
Then, it employs an LLM's code generation abilities, together with
reinforcement learning, for training the skills and combining them to implement
complex behaviors specified in language. We evaluate MaestroMotif using a suite
of complex tasks in the NetHack Learning Environment (NLE), demonstrating that
it surpasses existing approaches in both performance and usability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TECO: Improving Multimodal Intent Recognition with Text Enhancement
  through Commonsense Knowledge Extraction <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08529v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08529v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quynh-Mai Thi Nguyen, Lan-Nhi Thi Nguyen, Cam-Van Thi Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The objective of multimodal intent recognition (MIR) is to leverage various
modalities-such as text, video, and audio-to detect user intentions, which is
crucial for understanding human language and context in dialogue systems.
Despite advances in this field, two main challenges persist: (1) effectively
extracting and utilizing semantic information from robust textual features; (2)
aligning and fusing non-verbal modalities with verbal ones effectively. This
paper proposes a Text Enhancement with CommOnsense Knowledge Extractor (TECO)
to address these challenges. We begin by extracting relations from both
generated and retrieved knowledge to enrich the contextual information in the
text modality. Subsequently, we align and integrate visual and acoustic
representations with these enhanced text features to form a cohesive multimodal
representation. Our experimental results show substantial improvements over
existing baseline methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at PACLIC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Learning for Encoder-only Language Models via a Discrete
  Key-Value Bottleneck 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andor Diera, Lukas Galke, Fabian Karl, Ansgar Scherp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning remains challenging across various natural language
understanding tasks. When models are updated with new training data, they risk
catastrophic forgetting of prior knowledge. In the present work, we introduce a
discrete key-value bottleneck for encoder-only language models, allowing for
efficient continual learning by requiring only localized updates. Inspired by
the success of a discrete key-value bottleneck in vision, we address new and
NLP-specific challenges. We experiment with different bottleneck architectures
to find the most suitable variants regarding language, and present a generic
discrete key initialization technique for NLP that is task independent. We
evaluate the discrete key-value bottleneck in four continual learning NLP
scenarios and demonstrate that it alleviates catastrophic forgetting. We
showcase that it offers competitive performance to other popular continual
learning methods, with lower computational costs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache
  Compression Based on Global-Local Importance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingxin Li, Ye Li, Yuan Meng, Xinzhu Ma, Zihan Geng, Shutao Xia, Zhi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) continue to advance, the demand for higher
quality and faster processing of long contexts across various applications is
growing. KV cache is widely adopted as it stores previously generated key and
value tokens, effectively reducing redundant computations during inference.
However, as memory overhead becomes a significant concern, efficient
compression of KV cache has gained increasing attention. Most existing methods
perform compression from two perspectives: identifying important tokens and
designing compression strategies. However, these approaches often produce
biased distributions of important tokens due to the influence of accumulated
attention scores or positional encoding. Furthermore, they overlook the
sparsity and redundancy across different heads, which leads to difficulties in
preserving the most effective information at the head level. To this end, we
propose EMS to overcome these limitations, while achieving better KV cache
compression under extreme compression ratios. Specifically, we introduce a
Global-Local score that combines accumulated attention scores from both global
and local KV tokens to better identify the token importance. For the
compression strategy, we design an adaptive and unified Evict-then-Merge
framework that accounts for the sparsity and redundancy of KV tokens across
different heads. Additionally, we implement the head-wise parallel compression
through a zero-class mechanism to enhance efficiency. Extensive experiments
demonstrate our SOTA performance even under extreme compression ratios. EMS
consistently achieves the lowest perplexity, improves scores by over 1.28
points across four LLMs on LongBench under a 256 cache budget, and preserves
95% retrieval accuracy with a cache budget less than 2% of the context length
in the Needle-in-a-Haystack task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GR-NLP-TOOLKIT: An Open-Source NLP Toolkit for Modern Greek <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lefteris Loukas, Nikolaos Smyrnioudis, Chrysa Dikonomaki, Spyros Barbakos, Anastasios Toumazatos, John Koutsikakis, Manolis Kyriakakis, Mary Georgiou, Stavros Vassos, John Pavlopoulos, Ion Androutsopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present GR-NLP-TOOLKIT, an open-source natural language processing (NLP)
toolkit developed specifically for modern Greek. The toolkit provides
state-of-the-art performance in five core NLP tasks, namely part-of-speech
tagging, morphological tagging, dependency parsing, named entity recognition,
and Greeklishto-Greek transliteration. The toolkit is based on pre-trained
Transformers, it is freely available, and can be easily installed in Python
(pip install gr-nlp-toolkit). It is also accessible through a demonstration
platform on HuggingFace, along with a publicly available API for non-commercial
use. We discuss the functionality provided for each task, the underlying
methods, experiments against comparable open-source toolkits, and future
possible enhancements. The toolkit is available at:
https://github.com/nlpaueb/gr-nlp-toolkit
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted Demo Paper @ COLING 2025 (Github:
  https://github.com/nlpaueb/gr-nlp-toolkit/, Demo:
  https://huggingface.co/spaces/AUEB-NLP/greek-nlp-toolkit-demo, API:
  https://huggingface.co/spaces/AUEB-NLP/The-Greek-NLP-API)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Relevance and Reasoning: Rationale Distillation in
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengyue Jia, Derong Xu, Xiaopeng Li, Zhaocheng Du, Xiangyang Li, Xiangyu Zhao, Yichao Wang, Yuhao Wang, Huifeng Guo, Ruiming Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reranker and generator are two critical components in the
Retrieval-Augmented Generation (i.e., RAG) pipeline, responsible for ranking
relevant documents and generating responses. However, due to differences in
pre-training data and objectives, there is an inevitable gap between the
documents ranked as relevant by the reranker and those required by the
generator to support answering the query. To address this gap, we propose
RADIO, a novel and practical preference alignment framework with RAtionale
DIstillatiOn. Specifically, We first propose a rationale extraction method that
leverages the reasoning capabilities of Large Language Models (LLMs) to extract
the rationales necessary for answering the query. Subsequently, a
rationale-based alignment process is designed to rerank the documents based on
the extracted rationales, and fine-tune the reranker to align the preferences.
We conduct extensive experiments on two tasks across three datasets to
demonstrate the effectiveness of our approach compared to baseline methods. Our
code is released online to ease reproduction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparative Opinion Mining in Product <span class="highlight-title">Review</span>s: Multi-perspective
  <span class="highlight-title">Prompt</span>-based Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai-Yen Thi Nguyen, Cam-Van Thi Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Comparative reviews are pivotal in understanding consumer preferences and
influencing purchasing decisions. Comparative Quintuple Extraction (COQE) aims
to identify five key components in text: the target entity, compared entities,
compared aspects, opinions on these aspects, and polarity. Extracting precise
comparative information from product reviews is challenging due to nuanced
language and sequential task errors in traditional methods. To mitigate these
problems, we propose MTP-COQE, an end-to-end model designed for COQE.
Leveraging multi-perspective prompt-based learning, MTP-COQE effectively guides
the generative model in comparative opinion mining tasks. Evaluation on the
Camera-COQE (English) and VCOM (Vietnamese) datasets demonstrates MTP-COQE's
efficacy in automating COQE, achieving superior performance with a 1.41% higher
F1 score than the previous baseline models on the English dataset.
Additionally, we designed a strategy to limit the generative model's creativity
to ensure the output meets expectations. We also performed data augmentation to
address data imbalance and to prevent the model from becoming biased towards
dominant samples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-perspective Alignment for Increasing Naturalness in Neural Machine
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiyuan Lai, Esther Ploeger, Rik van Noord, Antonio Toral
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural machine translation (NMT) systems amplify lexical biases present in
their training data, leading to artificially impoverished language in output
translations. These language-level characteristics render automatic
translations different from text originally written in a language and human
translations, which hinders their usefulness in for example creating evaluation
datasets. Attempts to increase naturalness in NMT can fall short in terms of
content preservation, where increased lexical diversity comes at the cost of
translation accuracy. Inspired by the reinforcement learning from human
feedback framework, we introduce a novel method that rewards both naturalness
and content preservation. We experiment with multiple perspectives to produce
more natural translations, aiming at reducing machine and human translationese.
We evaluate our method on English-to-Dutch literary translation, and find that
our best model produces translations that are lexically richer and exhibit more
properties of human-written language, without loss in translation accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bootstrapping Language-Guided Navigation Learning with Self-Refining
  Data Flywheel 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08467v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08467v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zun Wang, Jialu Li, Yicong Hong, Songze Li, Kunchang Li, Shoubin Yu, Yi Wang, Yu Qiao, Yali Wang, Mohit Bansal, Limin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating high-quality data for training robust language-instructed agents is
a long-lasting challenge in embodied AI. In this paper, we introduce a
Self-Refining Data Flywheel (SRDF) that generates high-quality and large-scale
navigational instruction-trajectory pairs by iteratively refining the data pool
through the collaboration between two models, the instruction generator and the
navigator, without any human-in-the-loop annotation. Specifically, SRDF starts
with using a base generator to create an initial data pool for training a base
navigator, followed by applying the trained navigator to filter the data pool.
This leads to higher-fidelity data to train a better generator, which can, in
turn, produce higher-quality data for training the next-round navigator. Such a
flywheel establishes a data self-refining process, yielding a continuously
improved and highly effective dataset for large-scale language-guided
navigation learning. Our experiments demonstrate that after several flywheel
rounds, the navigator elevates the performance boundary from 70% to 78% SPL on
the classic R2R test set, surpassing human performance (76%) for the first
time. Meanwhile, this process results in a superior generator, evidenced by a
SPICE increase from 23.5 to 26.2, better than all previous VLN instruction
generation methods. Finally, we demonstrate the scalability of our method
through increasing environment and instruction diversity, and the
generalization ability of our pre-trained navigator across various downstream
navigation tasks, surpassing state-of-the-art methods by a large margin in all
cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, Code and data are available at
  https://github.com/wz0919/VLN-SRDF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Out-of-Entity Errors in Named Entity Recognition: A
  Sentence-Level Strategy <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guochao Jiang, Ziqin Luo, Chengwei Hu, Zepeng Ding, Deqing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many previous models of named entity recognition (NER) suffer from the
problem of Out-of-Entity (OOE), i.e., the tokens in the entity mentions of the
test samples have not appeared in the training samples, which hinders the
achievement of satisfactory performance. To improve OOE-NER performance, in
this paper, we propose a new framework, namely S+NER, which fully leverages
sentence-level information. Our S+NER achieves better OOE-NER performance
mainly due to the following two particular designs. 1) It first exploits the
pre-trained language model's capability of understanding the target entity's
sentence-level context with a template set. 2) Then, it refines the
sentence-level representation based on the positive and negative templates,
through a contrastive learning strategy and template pooling method, to obtain
better NER results. Our extensive experiments on five benchmark datasets have
demonstrated that, our S+NER outperforms some state-of-the-art OOE-NER models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing Personalized AI Mentoring with Large Language Models in the
  Computing Field 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Luo, Sean O'Connell, Shamima Mithun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper provides an in-depth evaluation of three state-of-the-art Large
Language Models (LLMs) for personalized career mentoring in the computing
field, using three distinct student profiles that consider gender, race, and
professional levels. We evaluated the performance of GPT-4, LLaMA 3, and Palm 2
using a zero-shot learning approach without human intervention. A quantitative
evaluation was conducted through a custom natural language processing analytics
pipeline to highlight the uniqueness of the responses and to identify words
reflecting each student's profile, including race, gender, or professional
level. The analysis of frequently used words in the responses indicates that
GPT-4 offers more personalized mentoring compared to the other two LLMs.
Additionally, a qualitative evaluation was performed to see if human experts
reached similar conclusions. The analysis of survey responses shows that GPT-4
outperformed the other two LLMs in delivering more accurate and useful
mentoring while addressing specific challenges with encouragement languages.
Our work establishes a foundation for developing personalized mentoring tools
based on LLMs, incorporating human mentors in the process to deliver a more
impactful and tailored mentoring experience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Conversational Mental Manipulation with Intent-Aware <span class="highlight-title">Prompt</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayuan Ma, Hongbin Na, Zimu Wang, Yining Hua, Yue Liu, Wei Wang, Ling Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mental manipulation severely undermines mental wellness by covertly and
negatively distorting decision-making. While there is an increasing interest in
mental health care within the natural language processing community, progress
in tackling manipulation remains limited due to the complexity of detecting
subtle, covert tactics in conversations. In this paper, we propose Intent-Aware
Prompting (IAP), a novel approach for detecting mental manipulations using
large language models (LLMs), providing a deeper understanding of manipulative
tactics by capturing the underlying intents of participants. Experimental
results on the MentalManip dataset demonstrate superior effectiveness of IAP
against other advanced prompting strategies. Notably, our approach
substantially reduces false negatives, helping detect more instances of mental
manipulation with minimal misjudgment of positive cases. The code of this paper
is available at https://github.com/Anton-Jiayuan-MA/Manip-IAP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Dialect Robustness of Language Models via Conversation
  Understanding <span class="chip">COLING'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05688v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05688v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dipankar Srirag, Nihar Ranjan Sahoo, Aditya Joshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With an evergrowing number of LLMs reporting superlative performance for
English, their ability to perform equitably for different dialects of English
($\textit{i.e.}$, dialect robustness) needs to be ascertained. Specifically, we
use English language (US English or Indian English) conversations between
humans who play the word-guessing game of 'taboo'. We formulate two evaluative
tasks: target word prediction (TWP) ($\textit{i.e.}$, predict the masked target
word in a conversation) and target word selection (TWS) ($\textit{i.e.}$,
select the most likely masked target word in a conversation, from among a set
of candidate words). Extending MD3, an existing dialectic dataset of
taboo-playing conversations, we introduce M-MD3, a target-word-masked version
of MD3 with the en-US and en-IN subsets. We create two subsets: en-MV (where
en-US is transformed to include dialectal information) and en-TR (where
dialectal information is removed from en-IN). We evaluate one open-source
(Llama3) and two closed-source (GPT-4/3.5) LLMs. LLMs perform significantly
better for US English than Indian English for both TWP and TWS tasks, for all
settings, exhibiting marginalisation against the Indian dialect of English.
While GPT-based models perform the best, the comparatively smaller models work
more equitably after fine-tuning. Our error analysis shows that the LLMs can
understand the dialect better after fine-tuning using dialectal data. Our
evaluation methodology exhibits a novel way to examine attributes of language
models using pre-existing dialogue datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SUMEval@COLING'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Jack of All Trades to Master of One: Specializing LLM-based
  Autoraters to a Test Set 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15387v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15387v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mara Finkelstein, Dan Deutsch, Parker Riley, Juraj Juraska, Geza Kovacs, Markus Freitag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As LLMs continue to become more powerful and versatile, human evaluation has
quickly become intractable at scale and reliance on automatic metrics has
become the norm. Recently, it has been shown that LLMs are themselves
state-of-the-art evaluators for many tasks. These Autoraters are typically
designed so that they generalize to new systems and test sets. In practice,
however, evaluation is performed on a small set of fixed, canonical test sets,
which are carefully curated to measure certain capabilities of interest and are
not changed frequently. In this work, we design a method which specializes a
prompted Autorater to a given test set, by leveraging historical ratings on the
test set to construct in-context learning (ICL) examples. We evaluate our
Specialist method on the task of fine-grained machine translation evaluation,
and show that it dramatically outperforms the state-of-the-art XCOMET metric by
54% and 119% on the WMT'23 and WMT'24 test sets, respectively. We perform
extensive analyses to understand the representations learned by our Specialist
metrics, and how variability in rater behavior affects their performance. We
also verify the generalizability and robustness of our Specialist method for
designing automatic metrics across different numbers of ICL examples, LLM
backbones, systems to evaluate, and evaluation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improve Mathematical Reasoning in Language Models by Automated Process
  Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06592v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06592v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, Abhinav Rastogi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complex multi-step reasoning tasks, such as solving mathematical problems or
generating code, remain a significant hurdle for even the most advanced large
language models (LLMs). Verifying LLM outputs with an Outcome Reward Model
(ORM) is a standard inference-time technique aimed at enhancing the reasoning
performance of LLMs. However, this still proves insufficient for reasoning
tasks with a lengthy or multi-hop reasoning chain, where the intermediate
outcomes are neither properly rewarded nor penalized. Process supervision
addresses this limitation by assigning intermediate rewards during the
reasoning process. To date, the methods used to collect process supervision
data have relied on either human annotation or per-step Monte Carlo estimation,
both prohibitively expensive to scale, thus hindering the broad application of
this technique. In response to this challenge, we propose a novel
divide-and-conquer style Monte Carlo Tree Search (MCTS) algorithm named
\textit{OmegaPRM} for the efficient collection of high-quality process
supervision data. This algorithm swiftly identifies the first error in the
Chain of Thought (CoT) with binary search and balances the positive and
negative examples, thereby ensuring both efficiency and quality. As a result,
we are able to collect over 1.5 million process supervision annotations to
train Process Reward Models (PRMs). This fully automated process supervision
alongside the weighted self-consistency algorithm is able to enhance LLMs' math
reasoning performances. We improved the success rates of the instruction-tuned
Gemini Pro model from 51\% to 69.4\% on MATH500 and from 86.4\% to 93.6\% on
GSM8K. Similarly, we boosted the success rates of Gemma2 27B from 42.3\% to
58.2\% on MATH500 and from 74.0\% to 92.2\% on GSM8K. The entire process
operates without any human intervention or supervision, making our method both
financially and ...
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 5 figures, 2 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ELBA: Learning by Asking for Embodied Visual Navigation and Task
  Completion <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.04865v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.04865v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Shen, Daniel Bis, Cynthia Lu, Ismini Lourentzou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The research community has shown increasing interest in designing intelligent
embodied agents that can assist humans in accomplishing tasks. Although there
have been significant advancements in related vision-language benchmarks, most
prior work has focused on building agents that follow instructions rather than
endowing agents the ability to ask questions to actively resolve ambiguities
arising naturally in embodied environments. To address this gap, we propose an
Embodied Learning-By-Asking (ELBA) model that learns when and what questions to
ask to dynamically acquire additional information for completing the task. We
evaluate ELBA on the TEACh vision-dialog navigation and task completion
dataset. Experimental results show that the proposed method achieves improved
task performance compared to baseline models without question-answering
capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 10 figures, WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Large Language Models Understand Symbolic Graphics Programs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08313v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08313v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeju Qiu, Weiyang Liu, Haiwen Feng, Zhen Liu, Tim Z. Xiao, Katherine M. Collins, Joshua B. Tenenbaum, Adrian Weller, Michael J. Black, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Against the backdrop of enthusiasm for large language models (LLMs), there is
an urgent need to scientifically assess their capabilities and shortcomings.
This is nontrivial in part because it is difficult to find tasks which the
models have not encountered during training. Utilizing symbolic graphics
programs, we propose a domain well-suited to test multiple spatial-semantic
reasoning skills of LLMs. Popular in computer graphics, these programs
procedurally generate visual data. While LLMs exhibit impressive skills in
general program synthesis and analysis, symbolic graphics programs offer a new
layer of evaluation: they allow us to test an LLM's ability to answer
different-grained semantic-level questions of the images or 3D geometries
without a vision encoder. To semantically understand the symbolic programs,
LLMs would need to possess the ability to "imagine" and reason how the
corresponding graphics content would look with only the symbolic description.
We use this task to evaluate LLMs by creating a large benchmark for the
semantic visual understanding of symbolic graphics programs, built procedurally
with minimal human effort. Particular emphasis is placed on transformations of
images that leave the image level semantics invariant while introducing
significant changes to the underlying program. We evaluate commercial and
open-source LLMs on our benchmark to assess their ability to reason about
visual output of programs, finding that LLMs considered stronger at reasoning
generally perform better. Lastly, we introduce a novel method to improve this
ability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned
with pre-collected instruction data on symbolic graphics programs.
Interestingly, we find that SIT not only improves LLM's understanding on
symbolic programs, but it also improves general reasoning ability on various
other benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report v3 (47 pages, 26 figures, project page:
  https://sgp-bench.github.io/, added visual illusion examples)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LISTN: Lexicon induction with socio-temporal nuance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19257v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19257v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christine de Kock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-group language is an important signifier of group dynamics. This paper
proposes a novel method for inducing lexicons of in-group language, which
incorporates its socio-temporal context. Existing methods for lexicon induction
do not capture the evolving nature of in-group language, nor the social
structure of the community. Using dynamic word and user embeddings trained on
conversations from online anti-women communities, our approach outperforms
prior methods for lexicon induction. We develop a test set for the task of
lexicon induction and a new lexicon of manosphere language, validated by human
experts, which quantifies the relevance of each term to a specific
sub-community at a given point in time. Finally, we present novel insights on
in-group language which illustrate the utility of this approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Categorical Syllogisms Revisited: A <span class="highlight-title">Review</span> of the Logical Reasoning
  Abilities of LLMs for Analyzing Categorical Syllogism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18762v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18762v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Zong, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There have been a huge number of benchmarks proposed to evaluate how large
language models (LLMs) behave for logic inference tasks. However, it remains an
open question how to properly evaluate this ability. In this paper, we provide
a systematic overview of prior works on the logical reasoning ability of LLMs
for analyzing categorical syllogisms. We first investigate all the possible
variations for the categorical syllogisms from a purely logical perspective and
then examine the underlying configurations (i.e., mood and figure) tested by
the existing datasets. Our results indicate that compared to template-based
synthetic datasets, crowdsourcing approaches normally sacrifice the coverage of
configurations (i.e., mood and figure) of categorical syllogisms for more
language variations, thus bringing challenges to fully testing LLMs under
different situations. We then proceed to summarize the findings and
observations for the performances of LLMs to infer the validity of syllogisms
from the current literature. The error rate breakdown analyses suggest that the
interpretation of the quantifiers seems to be the current bottleneck that
limits the performances of the LLMs and is thus worth more attention. Finally,
we discuss several points that might be worth considering when researchers plan
on the future release of categorical syllogism datasets. We hope our work will
not only provide a timely review of the current literature regarding
categorical syllogisms, but also motivate more interdisciplinary research
between communities, specifically computational linguists and logicians.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Acquired TASTE: Multimodal Stance Detection with Textual and Structural
  Embeddings <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03681v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03681v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Barel, Oren Tsur, Dan Vilenchik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stance detection plays a pivotal role in enabling an extensive range of
downstream applications, from discourse parsing to tracing the spread of fake
news and the denial of scientific facts. While most stance classification
models rely on textual representation of the utterance in question, prior work
has demonstrated the importance of the conversational context in stance
detection. In this work we introduce TASTE -- a multimodal architecture for
stance detection that harmoniously fuses Transformer-based content embedding
with unsupervised structural embedding. Through the fine-tuning of a pretrained
transformer and the amalgamation with social embedding via a Gated Residual
Network (GRN) layer, our model adeptly captures the complex interplay between
content and conversational structure in determining stance. TASTE achieves
state-of-the-art results on common benchmarks, significantly outperforming an
array of strong baselines. Comparative evaluations underscore the benefits of
social grounding -- emphasizing the criticality of concurrently harnessing both
content and structure for enhanced stance detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Deduplication Techniques for Economic Research Paper Titles
  with a Focus on Semantic Similarity using NLP and LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01141v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01141v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Doohee You, Samuel Fraiberger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates efficient deduplication techniques for a large NLP
dataset of economic research paper titles. We explore various pairing methods
alongside established distance measures (Levenshtein distance, cosine
similarity) and a sBERT model for semantic evaluation. Our findings suggest a
potentially low prevalence of duplicates based on the observed semantic
similarity across different methods. Further exploration with a human-annotated
ground truth set is completed for a more conclusive assessment. The result
supports findings from the NLP, LLM based distance metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study
  on Two Materials <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05348v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05348v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satanu Ghosh, Neal R. Brodnik, Carolina Frey, Collin Holgate, Tresa M. Pollock, Samantha Daly, Samuel Carton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the ability of GPT-4 to perform ad-hoc schema based information
extraction from scientific literature. We assess specifically whether it can,
with a basic prompting approach, replicate two existing material science
datasets, given the manuscripts from which they were originally manually
extracted. We employ materials scientists to perform a detailed manual error
analysis to assess where the model struggles to faithfully extract the desired
information, and draw on their insights to suggest research directions to
address this broadly important task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Update on 12/11/2024: Added some relevant literature that we missed
  in previous version of the paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fully Open Source Moxin-7B Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06845v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06845v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pu Zhao, Xuan Shen, Zhenglun Kong, Yixin Shen, Sung-En Chang, Timothy Rupprecht, Lei Lu, Enfu Nan, Changdi Yang, Yumei He, Xingchen Xu, Yu Huang, Wei Wang, Yue Chen, Yong He, Yanzhi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Large Language Models (LLMs) have undergone a significant
transformation, marked by a rapid rise in both their popularity and
capabilities. Leading this evolution are proprietary LLMs like GPT-4 and
GPT-o1, which have captured widespread attention in the AI community due to
their remarkable performance and versatility. Simultaneously, open-source LLMs,
such as LLaMA and Mistral, have made great contributions to the ever-increasing
popularity of LLMs due to the ease to customize and deploy the models across
diverse applications. Although open-source LLMs present unprecedented
opportunities for innovation and research, the commercialization of LLMs has
raised concerns about transparency, reproducibility, and safety. Many
open-source LLMs fail to meet fundamental transparency requirements by
withholding essential components like training code and data, and some use
restrictive licenses whilst claiming to be "open-source," which may hinder
further innovations on LLMs. To mitigate this issue, we introduce Moxin 7B, a
fully open-source LLM developed in accordance with the Model Openness Framework
(MOF), a ranked classification system that evaluates AI models based on model
completeness and openness, adhering to principles of open science, open source,
open data, and open access. Our model achieves the highest MOF classification
level of "open science" through the comprehensive release of pre-training code
and configurations, training and fine-tuning datasets, and intermediate and
final checkpoints. Experiments show that our model achieves superior
performance in zero-shot evaluation compared with popular 7B models and
performs competitively in few-shot evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fusing Domain-Specific Content from Large Language Models into Knowledge
  Graphs for Enhanced Zero Shot Object State Classification <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12151v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12151v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filippos Gouidis, Katerina Papantoniou, Konstantinos Papoutsakis, Theodore Patkos, Antonis Argyros, Dimitris Plexousakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain-specific knowledge can significantly contribute to addressing a wide
variety of vision tasks. However, the generation of such knowledge entails
considerable human labor and time costs. This study investigates the potential
of Large Language Models (LLMs) in generating and providing domain-specific
information through semantic embeddings. To achieve this, an LLM is integrated
into a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors
in the context of the Vision-based Zero-shot Object State Classification task.
We thoroughly examine the behavior of the LLM through an extensive ablation
study. Our findings reveal that the integration of LLM-based embeddings, in
combination with general-purpose pre-trained embeddings, leads to substantial
performance improvements. Drawing insights from this ablation study, we conduct
a comparative analysis against competing models, thereby highlighting the
state-of-the-art performance achieved by the proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the AAAI-MAKE 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rainbow Teaming: Open-Ended Generation of Diverse Adversarial <span class="highlight-title">Prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16822v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16822v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikayel Samvelyan, Sharath Chandra Raparthy, Andrei Lupu, Eric Hambro, Aram H. Markosyan, Manish Bhatt, Yuning Mao, Minqi Jiang, Jack Parker-Holder, Jakob Foerster, Tim Rocktäschel, Roberta Raileanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) become increasingly prevalent across many
real-world applications, understanding and enhancing their robustness to
adversarial attacks is of paramount importance. Existing methods for
identifying adversarial prompts tend to focus on specific domains, lack
diversity, or require extensive human annotations. To address these
limitations, we present Rainbow Teaming, a novel black-box approach for
producing a diverse collection of adversarial prompts. Rainbow Teaming casts
adversarial prompt generation as a quality-diversity problem and uses
open-ended search to generate prompts that are both effective and diverse.
Focusing on the safety domain, we use Rainbow Teaming to target various
state-of-the-art LLMs, including the Llama 2 and Llama 3 models. Our approach
reveals hundreds of effective adversarial prompts, with an attack success rate
exceeding 90% across all tested models. Furthermore, we demonstrate that
prompts generated by Rainbow Teaming are highly transferable and that
fine-tuning models with synthetic data generated by our method significantly
enhances their safety without sacrificing general performance or helpfulness.
We additionally explore the versatility of Rainbow Teaming by applying it to
question answering and cybersecurity, showcasing its potential to drive robust
open-ended self-improvement in a wide range of applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparative Analysis of Pooling Mechanisms in LLMs: A Sentiment Analysis
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14654v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14654v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinming Xing, Ruilin Xing, Yan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have revolutionized natural language processing
(NLP) by delivering state-of-the-art performance across a variety of tasks.
Among these, Transformer-based models like BERT and GPT rely on pooling layers
to aggregate token-level embeddings into sentence-level representations. Common
pooling mechanisms such as Mean, Max, and Weighted Sum play a pivotal role in
this aggregation process. Despite their widespread use, the comparative
performance of these strategies on different LLM architectures remains
underexplored. To address this gap, this paper investigates the effects of
these pooling mechanisms on two prominent LLM families -- BERT and GPT, in the
context of sentence-level sentiment analysis. Comprehensive experiments reveal
that each pooling mechanism exhibits unique strengths and weaknesses depending
on the task's specific requirements. Our findings underline the importance of
selecting pooling methods tailored to the demands of particular applications,
prompting a re-evaluation of common assumptions regarding pooling operations.
By offering actionable insights, this study contributes to the optimization of
LLM-based models for downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision-and-Language <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.01772v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.01772v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thong Nguyen, Cong-Duy Nguyen, Xiaobao Wu, See-Kiong Ng, Anh Tuan Luu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the burgeoning amount of data of image-text pairs and diversity of
Vision-and-Language (V\&L) tasks, scholars have introduced an abundance of deep
learning models in this research domain. Furthermore, in recent years, transfer
learning has also shown tremendous success in Computer Vision for tasks such as
Image Classification, Object Detection, etc., and in Natural Language
Processing for Question Answering, Machine Translation, etc. Inheriting the
spirit of Transfer Learning, research works in V\&L have devised multiple
pretraining techniques on large-scale datasets in order to enhance the
performance of downstream tasks. The aim of this article is to provide a
comprehensive revision of contemporary V\&L pretraining models. In particular,
we categorize and delineate pretraining approaches, along with the summary of
state-of-the-art vision-and-language pretrained models. Moreover, a list of
training datasets and downstream tasks is supplied to further polish the
perspective into V\&L pretraining. Lastly, we decided to take a further step to
discuss numerous directions for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The content of the paper has been outdated. I would like to rewrite a
  new version with completely new information.</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VoiceBench: Benchmarking LLM-Based Voice Assistants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17196v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17196v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Chen, Xianghu Yue, Chen Zhang, Xiaoxue Gao, Robby T. Tan, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building on the success of large language models (LLMs), recent advancements
such as GPT-4o have enabled real-time speech interactions through LLM-based
voice assistants, offering a significantly improved user experience compared to
traditional text-based interactions. However, the absence of benchmarks
designed to evaluate these speech interaction capabilities has hindered
progress of LLM-based voice assistants development. Current evaluations focus
primarily on automatic speech recognition (ASR) or general knowledge evaluation
with clean speeches, neglecting the more intricate, real-world scenarios that
involve diverse speaker characteristics, environmental and content factors. To
address this, we introduce VoiceBench, the first benchmark designed to provide
a multi-faceted evaluation of LLM-based voice assistants. VoiceBench also
includes both real and synthetic spoken instructions that incorporate the above
three key real-world variations. Extensive experiments reveal the limitations
of current LLM-based voice assistant models and offer valuable insights for
future research and development in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress. Data is available at
  https://github.com/MatthewCYM/VoiceBench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topic Classification of Case Law Using a Large Language Model and a New
  Taxonomy for UK Law: AI Insights into Summary Judgment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.12910v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.12910v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Holli Sargeant, Ahmed Izzidien, Felix Steffek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses a critical gap in legal analytics by developing and
applying a novel taxonomy for topic classification of summary judgment cases in
the United Kingdom. Using a curated dataset of summary judgment cases, we use
the Large Language Model Claude 3 Opus to explore functional topics and trends.
We find that Claude 3 Opus correctly classified the topic with an accuracy of
87.13% and an F1 score of 0.87. The analysis reveals distinct patterns in the
application of summary judgments across various legal domains. As case law in
the United Kingdom is not originally labelled with keywords or a topic
filtering option, the findings not only refine our understanding of the
thematic underpinnings of summary judgments but also illustrate the potential
of combining traditional and AI-driven approaches in legal classification.
Therefore, this paper provides a new and general taxonomy for UK law. The
implications of this work serve as a foundation for further research and policy
discussions in the field of judicial administration and computational legal
research methodologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MLE-bench: Evaluating Machine Learning Agents on Machine Learning
  Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07095v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07095v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, Lilian Weng, Aleksander Mądry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MLE-bench, a benchmark for measuring how well AI agents perform
at machine learning engineering. To this end, we curate 75 ML
engineering-related competitions from Kaggle, creating a diverse set of
challenging tasks that test real-world ML engineering skills such as training
models, preparing datasets, and running experiments. We establish human
baselines for each competition using Kaggle's publicly available leaderboards.
We use open-source agent scaffolds to evaluate several frontier language models
on our benchmark, finding that the best-performing setup--OpenAI's o1-preview
with AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in
16.9% of competitions. In addition to our main results, we investigate various
forms of resource scaling for AI agents and the impact of contamination from
pre-training. We open-source our benchmark code (github.com/openai/mle-bench/)
to facilitate future research in understanding the ML engineering capabilities
of AI agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 17 pages appendix. Equal contribution by first seven
  authors, authors randomized. Corrected footnote 4</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Filipino Benchmarks for Measuring Sexist and Homophobic Bias in
  Multilingual Language Models from Southeast Asia <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07303v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07303v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lance Calvin Lim Gamboa, Mark Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bias studies on multilingual models confirm the presence of gender-related
stereotypes in masked models processing languages with high NLP resources. We
expand on this line of research by introducing Filipino CrowS-Pairs and
Filipino WinoQueer: benchmarks that assess both sexist and anti-queer biases in
pretrained language models (PLMs) handling texts in Filipino, a low-resource
language from the Philippines. The benchmarks consist of 7,074 new challenge
pairs resulting from our cultural adaptation of English bias evaluation
datasets, a process that we document in detail to guide similar forthcoming
efforts. We apply the Filipino benchmarks on masked and causal multilingual
models, including those pretrained on Southeast Asian data, and find that they
contain considerable amounts of bias. We also find that for multilingual
models, the extent of bias learned for a particular language is influenced by
how much pretraining data in that language a model was exposed to. Our
benchmarks and insights can serve as a foundation for future work analyzing and
mitigating bias in multilingual models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at The First Workshop on Language Models
  for Low-Resource Languages (LoResLM) at The 31st International Conference on
  Computational Linguistics (COLING 2025)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">18</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ jina-clip-v2: Multilingual Multimodal Embeddings for Text and Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Koukounas, Georgios Mastrapas, Bo Wang, Mohammad Kalim Akram, Sedigheh Eslami, Michael Günther, Isabelle Mohr, Saba Sturua, Scott Martens, Nan Wang, Han Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language-Image Pretraining (CLIP) is a highly effective method
for aligning images and texts in a shared embedding space. These models are
widely used for tasks such as cross-modal information retrieval and multi-modal
understanding. However, CLIP models often struggle with text-only tasks,
underperforming compared to specialized text models. This performance disparity
forces retrieval systems to rely on separate models for text-only and
multi-modal tasks. In this work, we build upon our previous model,
jina-clip-v1, by introducing a refined framework that utilizes multi-task,
multi-stage contrastive learning across multiple languages, coupled with an
improved training recipe to enhance text-only retrieval. The resulting model,
jina-clip-v2, outperforms its predecessor on text-only and multimodal tasks,
while adding multilingual support, better understanding of complex visual
documents and efficiency gains thanks to Matryoshka Representation Learning and
vector truncation. The model performs comparably to the state-of-the-art in
both multilingual-multimodal and multilingual text retrieval benchmarks,
addressing the challenge of unifying text-only and multi-modal retrieval
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 1-10 main paper, 10-12 refs, 12-21 benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reducing Popularity Influence by Addressing Position Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrii Dzhoha, Alexey Kurennoy, Vladimir Vlasov, Marjan Celikik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Position bias poses a persistent challenge in recommender systems, with much
of the existing research focusing on refining ranking relevance and driving
user engagement. However, in practical applications, the mitigation of position
bias does not always result in detectable short-term improvements in ranking
relevance. This paper provides an alternative, practically useful view of what
position bias reduction methods can achieve. It demonstrates that position
debiasing can spread visibility and interactions more evenly across the
assortment, effectively reducing a skew in the popularity of items induced by
the position bias through a feedback loop. We offer an explanation of how
position bias affects item popularity. This includes an illustrative model of
the item popularity histogram and the effect of the position bias on its
skewness. Through offline and online experiments on our large-scale e-commerce
platform, we show that position debiasing can significantly improve assortment
utilization, without any degradation in user engagement or financial metrics.
This makes the ranking fairer and helps attract more partners or content
providers, benefiting the customers and the business in the long term.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preference Discerning with LLM-Enhanced Generative Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Paischer, Liu Yang, Linfeng Liu, Shuai Shao, Kaveh Hassani, Jiacheng Li, Ricky Chen, Zhang Gabriel Li, Xialo Gao, Wei Shao, Xue Feng, Nima Noorshams, Sem Park, Bo Long, Hamid Eghbalzadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation systems aim to provide personalized recommendations
for users based on their interaction history. To achieve this, they often
incorporate auxiliary information, such as textual descriptions of items and
auxiliary tasks, like predicting user preferences and intent. Despite numerous
efforts to enhance these models, they still suffer from limited
personalization. To address this issue, we propose a new paradigm, which we
term preference discerning. In preference dscerning, we explicitly condition a
generative sequential recommendation system on user preferences within its
context. To this end, we generate user preferences using Large Language Models
(LLMs) based on user reviews and item-specific data. To evaluate preference
discerning capabilities of sequential recommendation systems, we introduce a
novel benchmark that provides a holistic evaluation across various scenarios,
including preference steering and sentiment following. We assess current
state-of-the-art methods using our benchmark and show that they struggle to
accurately discern user preferences. Therefore, we propose a new method named
Mender ($\textbf{M}$ultimodal Prefer$\textbf{en}$ce
$\textbf{d}$iscern$\textbf{er}$), which improves upon existing methods and
achieves state-of-the-art performance on our benchmark. Our results show that
Mender can be effectively guided by human preferences even though they have not
been observed during training, paving the way toward more personalized
sequential recommendation systems. We will open-source the code and benchmarks
upon publication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages + references and appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Graph-RAG and <span class="highlight-title">Prompt</span> Engineering to Enhance LLM-Based
  Automated Requirement Traceability and Compliance Checks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arsalan Masoudifard, Mohammad Mowlavi Sorond, Moein Madadi, Mohammad Sabokrou, Elahe Habibi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring that Software Requirements Specifications (SRS) align with
higher-level organizational or national requirements is vital, particularly in
regulated environments such as finance and aerospace. In these domains,
maintaining consistency, adhering to regulatory frameworks, minimizing errors,
and meeting critical expectations are essential for the reliable functioning of
systems. The widespread adoption of large language models (LLMs) highlights
their immense potential, yet there remains considerable scope for improvement
in retrieving relevant information and enhancing reasoning capabilities. This
study demonstrates that integrating a robust Graph-RAG framework with advanced
prompt engineering techniques, such as Chain of Thought and Tree of Thought,
can significantly enhance performance. Compared to baseline RAG methods and
simple prompting strategies, this approach delivers more accurate and
context-aware results. While this method demonstrates significant improvements
in performance, it comes with challenges. It is both costly and more complex to
implement across diverse contexts, requiring careful adaptation to specific
scenarios. Additionally, its effectiveness heavily relies on having complete
and accurate input data, which may not always be readily available, posing
further limitations to its scalability and practicality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AltFS: Agency-light Feature Selection with Large Language Models in Deep
  Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengyue Jia, Zhaocheng Du, Yichao Wang, Xiangyu Zhao, Xiaopeng Li, Yuhao Wang, Qidong Liu, Huifeng Guo, Ruiming Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature selection is crucial in recommender systems for improving model
efficiency and predictive performance. Traditional methods rely on agency
models, such as decision trees or neural networks, to estimate feature
importance. However, this approach is inherently limited, as the agency models
may fail to learn effectively in all scenarios due to suboptimal training
conditions (e.g., feature collinearity, high-dimensional sparsity, and data
insufficiency). In this paper, we propose AltFS, an Agency-light Feature
Selection method for deep recommender systems. AltFS integrates semantic
reasoning from Large Language Models (LLMs) with task-specific learning from
agency models. Initially, LLMs will generate a semantic ranking of feature
importance, which is then refined by an agency model, combining world knowledge
with task-specific insights. Extensive experiments on three public datasets
from real-world recommender platforms demonstrate the effectiveness of AltFS.
Our code is publicly available for reproducibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InvDiff: Invariant Guidance for Bias Mitigation in Diffusion Models <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Hou, Yueying Wu, Chang Xu, Yu-Hao Huang, Chenxi Bai, Le Wu, Jiang Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As one of the most successful generative models, diffusion models have
demonstrated remarkable efficacy in synthesizing high-quality images. These
models learn the underlying high-dimensional data distribution in an
unsupervised manner. Despite their success, diffusion models are highly
data-driven and prone to inheriting the imbalances and biases present in
real-world data. Some studies have attempted to address these issues by
designing text prompts for known biases or using bias labels to construct
unbiased data. While these methods have shown improved results, real-world
scenarios often contain various unknown biases, and obtaining bias labels is
particularly challenging. In this paper, we emphasize the necessity of
mitigating bias in pre-trained diffusion models without relying on auxiliary
bias annotations. To tackle this problem, we propose a framework, InvDiff,
which aims to learn invariant semantic information for diffusion guidance.
Specifically, we propose identifying underlying biases in the training data and
designing a novel debiasing training objective. Then, we employ a lightweight
trainable module that automatically preserves invariant semantic information
and uses it to guide the diffusion model's sampling process toward unbiased
outcomes simultaneously. Notably, we only need to learn a small number of
parameters in the lightweight learnable module without altering the pre-trained
diffusion model. Furthermore, we provide a theoretical guarantee that the
implementation of InvDiff is equivalent to reducing the error upper bound of
generalization. Extensive experimental results on three publicly available
benchmarks demonstrate that InvDiff effectively reduces biases while
maintaining the quality of image generation. Our code is available at
https://github.com/Hundredl/InvDiff.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>KDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NyayaAnumana & INLegalLlama: The Largest Indian Legal Judgment
  Prediction <span class="highlight-title">Dataset</span> and Specialized Language Model for Enhanced Decision
  Analysis <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubham Kumar Nigam, Balaramamahanthi Deepak Patnaik, Shivam Mishra, Noel Shallum, Kripabandhu Ghosh, Arnab Bhattacharya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of artificial intelligence (AI) in legal judgment prediction
(LJP) has the potential to transform the legal landscape, particularly in
jurisdictions like India, where a significant backlog of cases burdens the
legal system. This paper introduces NyayaAnumana, the largest and most diverse
corpus of Indian legal cases compiled for LJP, encompassing a total of 7,02,945
preprocessed cases. NyayaAnumana, which combines the words "Nyay" (judgment)
and "Anuman" (prediction or inference) respectively for most major Indian
languages, includes a wide range of cases from the Supreme Court, High Courts,
Tribunal Courts, District Courts, and Daily Orders and, thus, provides
unparalleled diversity and coverage. Our dataset surpasses existing datasets
like PredEx and ILDC, offering a comprehensive foundation for advanced AI
research in the legal domain.
  In addition to the dataset, we present INLegalLlama, a domain-specific
generative large language model (LLM) tailored to the intricacies of the Indian
legal system. It is developed through a two-phase training approach over a base
LLaMa model. First, Indian legal documents are injected using continual
pretraining. Second, task-specific supervised finetuning is done. This method
allows the model to achieve a deeper understanding of legal contexts.
  Our experiments demonstrate that incorporating diverse court data
significantly boosts model accuracy, achieving approximately 90% F1-score in
prediction tasks. INLegalLlama not only improves prediction accuracy but also
offers comprehensible explanations, addressing the need for explainability in
AI-assisted legal decisions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted on COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Augmenting Sequential Recommendation with Balanced Relevance and
  Diversity <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhou Dang, Jiahui Zhang, Yuting Liu, Enneng Yang, Yuliang Liang, Guibing Guo, Jianzhe Zhao, Xingwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By generating new yet effective data, data augmentation has become a
promising method to mitigate the data sparsity problem in sequential
recommendation. Existing works focus on augmenting the original data but rarely
explore the issue of imbalanced relevance and diversity for augmented data,
leading to semantic drift problems or limited performance improvements. In this
paper, we propose a novel Balanced data Augmentation Plugin for Sequential
Recommendation (BASRec) to generate data that balance relevance and diversity.
BASRec consists of two modules: Single-sequence Augmentation and Cross-sequence
Augmentation. The former leverages the randomness of the heuristic operators to
generate diverse sequences for a single user, after which the diverse and the
original sequences are fused at the representation level to obtain relevance.
Further, we devise a reweighting strategy to enable the model to learn the
preferences based on the two properties adaptively. The Cross-sequence
Augmentation performs nonlinear mixing between different sequence
representations from two directions. It produces virtual sequence
representations that are diverse enough but retain the vital semantics of the
original sequences. These two modules enhance the model to discover
fine-grained preferences knowledge from single-user and cross-user
perspectives. Extensive experiments verify the effectiveness of BASRec. The
average improvement is up to 72.0% on GRU4Rec, 33.8% on SASRec, and 68.5% on
FMLP-Rec. We demonstrate that BASRec generates data with a better balance
between relevance and diversity than existing methods. The source code is
available at https://github.com/KingGugu/BASRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models for Scholarly Ontology Generation: An Extensive
  Analysis in the Engineering Field 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanay Aggarwal, Angelo Salatino, Francesco Osborne, Enrico Motta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ontologies of research topics are crucial for structuring scientific
knowledge, enabling scientists to navigate vast amounts of research, and
forming the backbone of intelligent systems such as search engines and
recommendation systems. However, manual creation of these ontologies is
expensive, slow, and often results in outdated and overly general
representations. As a solution, researchers have been investigating ways to
automate or semi-automate the process of generating these ontologies. This
paper offers a comprehensive analysis of the ability of large language models
(LLMs) to identify semantic relationships between different research topics,
which is a critical step in the development of such ontologies. To this end, we
developed a gold standard based on the IEEE Thesaurus to evaluate the task of
identifying four types of relationships between pairs of topics: broader,
narrower, same-as, and other. Our study evaluates the performance of seventeen
LLMs, which differ in scale, accessibility (open vs. proprietary), and model
type (full vs. quantised), while also assessing four zero-shot reasoning
strategies. Several models have achieved outstanding results, including
Mixtral-8x7B, Dolphin-Mistral-7B, and Claude 3 Sonnet, with F1-scores of 0.847,
0.920, and 0.967, respectively. Furthermore, our findings demonstrate that
smaller, quantised models, when optimised through prompt engineering, can
deliver performance comparable to much larger proprietary models, while
requiring significantly fewer computational resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to Information Processing & Management</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Multidimensional Checkworthiness: Designing AI-assisted Claim
  Prioritization for Human Fact-checkers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08185v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08185v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Houjiang Liu, Jacek Gwizdka, Matthew Lease
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the massive volume of potentially false claims circulating online,
claim prioritization is essential in allocating limited human resources
available for fact-checking. In this study, we perceive claim prioritization as
an information retrieval (IR) task: just as multidimensional IR relevance, with
many factors influencing which search results a user deems relevant,
checkworthiness is also multi-faceted, subjective, and even personal, with many
factors influencing how fact-checkers triage and select which claims to check.
Our study investigates both the multidimensional nature of checkworthiness and
effective tool support to assist fact-checkers in claim prioritization.
Methodologically, we pursue Research through Design combined with mixed-method
evaluation. We develop an AI-assisted claim prioritization prototype as a probe
to explore how fact-checkers use multidimensional checkworthiness factors in
claim prioritization, simultaneously probing fact-checker needs while also
exploring the design space to meet those needs.
  Our study with 16 professional fact-checkers investigates: 1) how
participants assessed the relative importance of different checkworthy
dimensions and apply different priorities in claim selection; 2) how they
created customized GPT-based search filters and the corresponding benefits and
limitations; and 3) their overall user experiences with our prototype. Our work
makes a conceptual contribution between multidimensional IR relevance and
fact-checking checkworthiness, with findings demonstrating the value of
corresponding tooling support. Specifically, we uncovered a hierarchical
prioritization strategy fact-checkers implicitly use, revealing an
underexplored aspect of their workflow, with actionable design recommendations
for improving claim triage across multi-dimensional checkworthiness and
tailoring this process with LLM integration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Difference Learning for Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changhong Li, Zhiqiang Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendations have drawn significant attention in modeling the
user's historical behaviors to predict the next item. With the booming
development of multimodal data (e.g., image, text) on internet platforms,
sequential recommendation also benefits from the incorporation of multimodal
data. Most methods introduce modal features of items as side information and
simply concatenates them to learn unified user interests. Nevertheless, these
methods encounter the limitation in modeling multimodal differences. We argue
that user interests and item relationships vary across different modalities. To
address this problem, we propose a novel Multimodal Difference Learning
framework for Sequential Recommendation, MDSRec for brevity. Specifically, we
first explore the differences in item relationships by constructing modal-aware
item relation graphs with behavior signal to enhance item representations.
Then, to capture the differences in user interests across modalities, we design
a interest-centralized attention mechanism to independently model user sequence
representations in different modalities. Finally, we fuse the user embeddings
from multiple modalities to achieve accurate item recommendation. Experimental
results on five real-world datasets demonstrate the superiority of MDSRec over
state-of-the-art baselines and the efficacy of multimodal difference learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Tutorial of Personalized Federated Recommender Systems: Recent
  Advances and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Jiang, Chunxu Zhang, Honglei Zhang, Zhiwei Li, Yidong Li, Bo Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalization stands as the cornerstone of recommender systems (RecSys),
striving to sift out redundant information and offer tailor-made services for
users. However, the conventional cloud-based RecSys necessitates centralized
data collection, posing significant risks of user privacy breaches. In response
to this challenge, federated recommender systems (FedRecSys) have emerged,
garnering considerable attention. FedRecSys enable users to retain personal
data locally and solely share model parameters with low privacy sensitivity for
global model training, significantly bolstering the system's privacy protection
capabilities. Within the distributed learning framework, the pronounced non-iid
nature of user behavior data introduces fresh hurdles to federated
optimization. Meanwhile, the ability of federated learning to concurrently
learn multiple models presents an opportunity for personalized user modeling.
Consequently, the development of personalized FedRecSys (PFedRecSys) is crucial
and holds substantial significance. This tutorial seeks to provide an
introduction to PFedRecSys, encompassing (1) an overview of existing studies on
PFedRecSys, (2) a comprehensive taxonomy of PFedRecSys spanning four pivotal
research directions-client-side adaptation, server-side aggregation,
communication efficiency, privacy and protection, and (3) exploration of open
challenges and promising future directions in PFedRecSys. This tutorial aims to
establish a robust foundation and spark new perspectives for subsequent
exploration and practical implementations in the evolving realm of RecSys.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A technical tutorial will appear at The Web Conference 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cluster-Enhanced Federated Graph Neural Network for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyan Wang, Ye Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personal interaction data can be effectively modeled as individual graphs for
each user in recommender systems.Graph Neural Networks (GNNs)-based
recommendation techniques have become extremely popular since they can capture
high-order collaborative signals between users and items by aggregating the
individual graph into a global interactive graph.However, this centralized
approach inherently poses a threat to user privacy and security. Recently,
federated GNN-based recommendation techniques have emerged as a promising
solution to mitigate privacy concerns. Nevertheless, current implementations
either limit on-device training to an unaccompanied individual graphs or
necessitate reliance on an extra third-party server to touch other individual
graphs, which also increases the risk of privacy leakage. To address this
challenge, we propose a Cluster-enhanced Federated Graph Neural Network
framework for Recommendation, named CFedGR, which introduces high-order
collaborative signals to augment individual graphs in a privacy preserving
manner. Specifically, the server clusters the pretrained user representations
to identify high-order collaborative signals. In addition, two efficient
strategies are devised to reduce communication between devices and the server.
Extensive experiments on three benchmark datasets validate the effectiveness of
our proposed methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RALI@TREC iKAT 2024: Achieving Personalization via Retrieval Fusion in
  Conversational Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Hui, Fengran Mo, Milan Mao, Jian-Yun Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Recherche Appliquee en Linguistique Informatique (RALI) team participated
in the 2024 TREC Interactive Knowledge Assistance (iKAT) Track. In personalized
conversational search, effectively capturing a user's complex search intent
requires incorporating both contextual information and key elements from the
user profile into query reformulation. The user profile often contains many
relevant pieces, and each could potentially complement the user's information
needs. It is difficult to disregard any of them, whereas introducing an
excessive number of these pieces risks drifting from the original query and
hinders search performance. This is a challenge we denote as
over-personalization. To address this, we propose different strategies by
fusing ranking lists generated from the queries with different levels of
personalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work presented at NIST Text Retrieval Conference 2024.
  https://www.nist.gov/news-events/events/2024/11/trec2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study
  on Two Materials <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05348v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05348v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satanu Ghosh, Neal R. Brodnik, Carolina Frey, Collin Holgate, Tresa M. Pollock, Samantha Daly, Samuel Carton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the ability of GPT-4 to perform ad-hoc schema based information
extraction from scientific literature. We assess specifically whether it can,
with a basic prompting approach, replicate two existing material science
datasets, given the manuscripts from which they were originally manually
extracted. We employ materials scientists to perform a detailed manual error
analysis to assess where the model struggles to faithfully extract the desired
information, and draw on their insights to suggest research directions to
address this broadly important task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Update on 12/11/2024: Added some relevant literature that we missed
  in previous version of the paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CURE: A <span class="highlight-title">dataset</span> for Clinical Understanding & Retrieval Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06954v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06954v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nadia Sheikh, Anne-Laure Jousse, Daniel Buades Marcos, Akintunde Oladipo, Olivier Rousseau, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the dominance of dense retrievers that do not generalize well beyond
their training dataset distributions, domain-specific test sets are essential
in evaluating retrieval. There are few test datasets for retrieval systems
intended for use by healthcare providers in a point-of-care setting. To fill
this gap we have collaborated with medical professionals to create CURE, an
ad-hoc retrieval test dataset for passage ranking with 2000 queries spanning 10
medical domains with a monolingual (English) and two cross-lingual
(French/Spanish -> English) conditions. In this paper, we describe how CURE was
constructed and provide baseline results to showcase its effectiveness as an
evaluation tool. CURE is published with a Creative Commons Attribution Non
Commercial 4.0 license and can be accessed on Hugging Face.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatial-Temporal Federated Learning for Lifelong Person
  Re-identification on Distributed Edges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.11759v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.11759v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Zhang, Guanyu Gao, Huaizheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data drift is a thorny challenge when deploying person re-identification
(ReID) models into real-world devices, where the data distribution is
significantly different from that of the training environment and keeps
changing. To tackle this issue, we propose a federated spatial-temporal
incremental learning approach, named FedSTIL, which leverages both lifelong
learning and federated learning to continuously optimize models deployed on
many distributed edge clients. Unlike previous efforts, FedSTIL aims to mine
spatial-temporal correlations among the knowledge learnt from different edge
clients. Specifically, the edge clients first periodically extract general
representations of drifted data to optimize their local models. Then, the
learnt knowledge from edge clients will be aggregated by centralized parameter
server, where the knowledge will be selectively and attentively distilled from
spatial- and temporal-dimension with carefully designed mechanisms. Finally,
the distilled informative spatial-temporal knowledge will be sent back to
correlated edge clients to further improve the recognition accuracy of each
edge client with a lifelong learning method. Extensive experiments on a mixture
of five real-world datasets demonstrate that our method outperforms others by
nearly 4% in Rank-1 accuracy, while reducing communication cost by 62%. All
implementation codes are publicly available on
https://github.com/MSNLAB/Federated-Lifelong-Person-ReID
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation Learning with Large Language Models for Recommendation <span class="chip">WWW'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15950v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15950v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, Chao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have seen significant advancements with the influence of
deep learning and graph neural networks, particularly in capturing complex
user-item relationships. However, these graph-based recommenders heavily depend
on ID-based data, potentially disregarding valuable textual information
associated with users and items, resulting in less informative learned
representations. Moreover, the utilization of implicit feedback data introduces
potential noise and bias, posing challenges for the effectiveness of user
preference learning. While the integration of large language models (LLMs) into
traditional ID-based recommenders has gained attention, challenges such as
scalability issues, limitations in text-only reliance, and prompt input
constraints need to be addressed for effective implementation in practical
recommender systems. To address these challenges, we propose a model-agnostic
framework RLMRec that aims to enhance existing recommenders with LLM-empowered
representation learning. It proposes a recommendation paradigm that integrates
representation learning with LLMs to capture intricate semantic aspects of user
behaviors and preferences. RLMRec incorporates auxiliary textual signals,
develops a user/item profiling paradigm empowered by LLMs, and aligns the
semantic space of LLMs with the representation space of collaborative
relational signals through a cross-view alignment framework. This work further
establish a theoretical foundation demonstrating that incorporating textual
signals through mutual information maximization enhances the quality of
representations. In our evaluation, we integrate RLMRec with state-of-the-art
recommender models, while also analyzing its efficiency and robustness to noise
data. Our implementation codes are available at
https://github.com/HKUDS/RLMRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a WWW'24 full paper</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mel-Refine: A Plug-and-Play Approach to Refine Mel-Spectrogram in Audio
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongming Guo, Ruibo Fu, Yizhong Geng, Shuai Liu, Shuchen Shi, Tao Wang, Chunyu Qiang, Chenxing Li, Ya Li, Zhengqi Wen, Yukun Liu, Xuefei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-audio (TTA) model is capable of generating diverse audio from textual
prompts. However, most mainstream TTA models, which predominantly rely on
Mel-spectrograms, still face challenges in producing audio with rich content.
The intricate details and texture required in Mel-spectrograms for such audio
often surpass the models' capacity, leading to outputs that are blurred or lack
coherence. In this paper, we begin by investigating the critical role of U-Net
in Mel-spectrogram generation. Our analysis shows that in U-Net structure,
high-frequency components in skip-connections and the backbone influence
texture and detail, while low-frequency components in the backbone are critical
for the diffusion denoising process. We further propose ``Mel-Refine'', a
plug-and-play approach that enhances Mel-spectrogram texture and detail by
adjusting different component weights during inference. Our method requires no
additional training or fine-tuning and is fully compatible with any
diffusion-based TTA architecture. Experimental results show that our approach
boosts performance metrics of the latest TTA model Tango2 by 25\%,
demonstrating its effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PointTalk: Audio-Driven Dynamic Lip Point Cloud for 3D Gaussian-based
  Talking Head Synthesis <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Xie, Tao Feng, Xin Zhang, Xiangyang Luo, Zixuan Guo, Weijiang Yu, Heng Chang, Fei Ma, Fei Richard Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Talking head synthesis with arbitrary speech audio is a crucial challenge in
the field of digital humans. Recently, methods based on radiance fields have
received increasing attention due to their ability to synthesize high-fidelity
and identity-consistent talking heads from just a few minutes of training
video. However, due to the limited scale of the training data, these methods
often exhibit poor performance in audio-lip synchronization and visual quality.
In this paper, we propose a novel 3D Gaussian-based method called PointTalk,
which constructs a static 3D Gaussian field of the head and deforms it in sync
with the audio. It also incorporates an audio-driven dynamic lip point cloud as
a critical component of the conditional information, thereby facilitating the
effective synthesis of talking heads. Specifically, the initial step involves
generating the corresponding lip point cloud from the audio signal and
capturing its topological structure. The design of the dynamic difference
encoder aims to capture the subtle nuances inherent in dynamic lip movements
more effectively. Furthermore, we integrate the audio-point enhancement module,
which not only ensures the synchronization of the audio signal with the
corresponding lip point cloud within the feature space, but also facilitates a
deeper understanding of the interrelations among cross-modal conditional
features. Extensive experiments demonstrate that our method achieves superior
high-fidelity and audio-lip synchronization in talking head synthesis compared
to previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Dual-Module Denoising Approach with Curriculum Learning for Enhancing
  Multimodal Aspect-Based Sentiment Analysis <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nguyen Van Doan, Dat Tran Nguyen, Cam-Van Thi Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Aspect-Based Sentiment Analysis (MABSA) combines text and images
to perform sentiment analysis but often struggles with irrelevant or misleading
visual information. Existing methodologies typically address either
sentence-image denoising or aspect-image denoising but fail to comprehensively
tackle both types of noise. To address these limitations, we propose DualDe, a
novel approach comprising two distinct components: the Hybrid Curriculum
Denoising Module (HCD) and the Aspect-Enhance Denoising Module (AED). The HCD
module enhances sentence-image denoising by incorporating a flexible curriculum
learning strategy that prioritizes training on clean data. Concurrently, the
AED module mitigates aspect-image noise through an aspect-guided attention
mechanism that filters out noisy visual regions which unrelated to the specific
aspects of interest. Our approach demonstrates effectiveness in addressing both
sentence-image and aspect-image noise, as evidenced by experimental evaluations
on benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at PACLIC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ POINTS1.5: Building a Vision-Language Model towards Real World
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Liu, Le Tian, Xiao Zhou, Xinyu Gao, Kavio Yu, Yang Yu, Jie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models have made significant strides recently, demonstrating
superior performance across a range of tasks, e.g. optical character
recognition and complex diagram analysis. Building on this trend, we introduce
a new vision-language model, POINTS1.5, designed to excel in various real-world
applications. POINTS1.5 is an enhancement of POINTS1.0 and incorporates several
key innovations: i) We replace the original CLIP vision encoder, which had a
fixed image resolution, with a NaViT-style vision encoder that supports native
dynamic high resolution. This allows POINTS1.5 to process images of any
resolution without needing to split them into tiles. ii) We add bilingual
support to POINTS1.5, significantly enhancing its capability in Chinese. Due to
the scarcity of open-source Chinese datasets for vision-language models, we
collect numerous images from the Internet and annotate them using a combination
of manual and automatic methods. iii) We propose a set of rigorous filtering
methods for visual instruction tuning datasets. We comprehensively evaluate all
these filtering methods, and choose the most effective ones to obtain the final
visual instruction tuning set. Thanks to these innovations, POINTS1.5
significantly outperforms POINTS1.0 and demonstrates strong performance across
a range of real-world applications. Notably, POINTS1.5-7B is trained on fewer
than 4 billion tokens and ranks first on the OpenCompass leaderboard among
models with fewer than 10 billion parameters
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Model For Voice and Accent Conversion In Speech and Singing
  using <span class="highlight-title">Self-Supervised</span> Learning and Feature Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sowmya Cheripally
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new voice conversion model capable of transforming both
speaking and singing voices. It addresses key challenges in current systems,
such as conveying emotions, managing pronunciation and accent changes, and
reproducing non-verbal sounds. One of the model's standout features is its
ability to perform accent conversion on hybrid voice samples that encompass
both speech and singing, allowing it to change the speaker's accent while
preserving the original content and prosody. The proposed model uses an
encoder-decoder architecture: the encoder is based on HuBERT to process the
speech's acoustic and linguistic content, while the HiFi-GAN decoder audio
matches the target speaker's voice. The model incorporates fundamental
frequency (f0) features and singer embeddings to enhance performance while
ensuring the pitch & tone accuracy and vocal identity are preserved during
transformation. This approach improves how naturally and flexibly voice style
can be transformed, showing strong potential for applications in voice dubbing,
content creation, and technologies like Text-to-Speech (TTS) and Interactive
Voice Response (IVR) systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoMuSE: Momentum Multi-modal Target Speaker Extraction for Real-time
  Scenarios with Impaired Visual Cues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Li, Ke Zhang, Shuai Wang, Kong Aik Lee, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual Target Speaker Extraction (AV-TSE) aims to isolate the speech of
a specific target speaker from an audio mixture using time-synchronized visual
cues. In real-world scenarios, visual cues are not always available due to
various impairments, which undermines the stability of AV-TSE. Despite this
challenge, humans can maintain attentional momentum over time, even when the
target speaker is not visible. In this paper, we introduce the Momentum
Multi-modal target Speaker Extraction (MoMuSE), which retains a speaker
identity momentum in memory, enabling the model to continuously track the
target speaker. Designed for real-time inference, MoMuSE extracts the current
speech window with guidance from both visual cues and dynamically updated
speaker momentum. Experimental results demonstrate that MoMuSE exhibits
significant improvement, particularly in scenarios with severe impairment of
visual cues.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAFIRE: Segment Any Forged Image Region <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08197v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08197v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Myung-Joon Kwon, Wonjun Lee, Seung-Hun Nam, Minji Son, Changick Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most techniques approach the problem of image forgery localization as a
binary segmentation task, training neural networks to label original areas as 0
and forged areas as 1. In contrast, we tackle this issue from a more
fundamental perspective by partitioning images according to their originating
sources. To this end, we propose Segment Any Forged Image Region (SAFIRE),
which solves forgery localization using point prompting. Each point on an image
is used to segment the source region containing itself. This allows us to
partition images into multiple source regions, a capability achieved for the
first time. Additionally, rather than memorizing certain forgery traces, SAFIRE
naturally focuses on uniform characteristics within each source region. This
approach leads to more stable and effective learning, achieving superior
performance in both the new task and the traditional binary forgery
localization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2025. Code is available at:
  https://github.com/mjkwon2021/SAFIRE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TextRefiner: Internal Visual Feature as Efficient Refiner for
  Vision-Language Models <span class="highlight-title">Prompt</span> Tuning <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingjing Xie, Yuxin Zhang, Jun Peng, Zhaohong Huang, Liujuan Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the efficiency of prompt learning in transferring vision-language
models (VLMs) to downstream tasks, existing methods mainly learn the prompts in
a coarse-grained manner where the learned prompt vectors are shared across all
categories. Consequently, the tailored prompts often fail to discern
class-specific visual concepts, thereby hindering the transferred performance
for classes that share similar or complex visual attributes. Recent advances
mitigate this challenge by leveraging external knowledge from Large Language
Models (LLMs) to furnish class descriptions, yet incurring notable inference
costs. In this paper, we introduce TextRefiner, a plug-and-play method to
refine the text prompts of existing methods by leveraging the internal
knowledge of VLMs. Particularly, TextRefiner builds a novel local cache module
to encapsulate fine-grained visual concepts derivedfrom local tokens within the
image branch. By aggregating and aligning the cached visual descriptions with
the original output of the text branch, TextRefiner can efficiently refine and
enrich the learned prompts from existing methods without relying on any
external expertise. For example, it improves the performance of CoOp from 71.66
% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise
features for text prompts. Equipped with TextRefiner, PromptKD achieves
state-of-the-art performance and is efficient in inference. Our code is relesed
at https://github.com/xjjxmu/TextRefiner
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative Hybrid Propagator for Temporal Misalignment in
  Audio-Visual Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kexin Li, Zongxin Yang, Yi Yang, Jun Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual video segmentation (AVVS) aims to generate pixel-level maps of
sound-producing objects that accurately align with the corresponding audio.
However, existing methods often face temporal misalignment, where audio cues
and segmentation results are not temporally coordinated. Audio provides two
critical pieces of information: i) target object-level details and ii) the
timing of when objects start and stop producing sounds. Current methods focus
more on object-level information but neglect the boundaries of audio semantic
changes, leading to temporal misalignment. To address this issue, we propose a
Collaborative Hybrid Propagator Framework~(Co-Prop). This framework includes
two main steps: Preliminary Audio Boundary Anchoring and Frame-by-Frame
Audio-Insert Propagation. To Anchor the audio boundary, we employ
retrieval-assist prompts with Qwen large language models to identify control
points of audio semantic changes. These control points split the audio into
semantically consistent audio portions. After obtaining the control point
lists, we propose the Audio Insertion Propagator to process each audio portion
using a frame-by-frame audio insertion propagation and matching approach. We
curated a compact dataset comprising diverse source conversion cases and
devised a metric to assess alignment rates. Compared to traditional
simultaneous processing methods, our approach reduces memory requirements and
facilitates frame alignment. Experimental results demonstrate the effectiveness
of our approach across three datasets and two backbones. Furthermore, our
method can be integrated with existing AVVS approaches, offering plug-and-play
functionality to enhance their performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LatentSpeech: Latent Diffusion for Text-To-Speech Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Lou, Helen Paik, Pari Delir Haghighi, Wen Hu, Lina Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based Generative AI gains significant attention for its superior
performance over other generative techniques like Generative Adversarial
Networks and Variational Autoencoders. While it has achieved notable
advancements in fields such as computer vision and natural language processing,
their application in speech generation remains under-explored. Mainstream
Text-to-Speech systems primarily map outputs to Mel-Spectrograms in the
spectral space, leading to high computational loads due to the sparsity of
MelSpecs. To address these limitations, we propose LatentSpeech, a novel TTS
generation approach utilizing latent diffusion models. By using latent
embeddings as the intermediate representation, LatentSpeech reduces the target
dimension to 5% of what is required for MelSpecs, simplifying the processing
for the TTS encoder and vocoder and enabling efficient high-quality speech
generation. This study marks the first integration of latent diffusion models
in TTS, enhancing the accuracy and naturalness of generated speech.
Experimental results on benchmark datasets demonstrate that LatentSpeech
achieves a 25% improvement in Word Error Rate and a 24% improvement in Mel
Cepstral Distortion compared to existing models, with further improvements
rising to 49.5% and 26%, respectively, with additional training data. These
findings highlight the potential of LatentSpeech to advance the
state-of-the-art in TTS technology
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeRF-NQA: No-Reference Quality Assessment for Scenes Generated by NeRF
  and Neural View Synthesis Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08029v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08029v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Qu, Hanxue Liang, Xiaoming Chen, Yuk Ying Chung, Yiran Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural View Synthesis (NVS) has demonstrated efficacy in generating
high-fidelity dense viewpoint videos using a image set with sparse views.
However, existing quality assessment methods like PSNR, SSIM, and LPIPS are not
tailored for the scenes with dense viewpoints synthesized by NVS and NeRF
variants, thus, they often fall short in capturing the perceptual quality,
including spatial and angular aspects of NVS-synthesized scenes. Furthermore,
the lack of dense ground truth views makes the full reference quality
assessment on NVS-synthesized scenes challenging. For instance, datasets such
as LLFF provide only sparse images, insufficient for complete full-reference
assessments. To address the issues above, we propose NeRF-NQA, the first
no-reference quality assessment method for densely-observed scenes synthesized
from the NVS and NeRF variants. NeRF-NQA employs a joint quality assessment
strategy, integrating both viewwise and pointwise approaches, to evaluate the
quality of NVS-generated scenes. The viewwise approach assesses the spatial
quality of each individual synthesized view and the overall inter-views
consistency, while the pointwise approach focuses on the angular qualities of
scene surface points and their compound inter-point quality. Extensive
evaluations are conducted to compare NeRF-NQA with 23 mainstream visual quality
assessment methods (from fields of image, video, and light-field assessment).
The results demonstrate NeRF-NQA outperforms the existing assessment methods
significantly and it shows substantial superiority on assessing NVS-synthesized
scenes without references. An implementation of this paper are available at
https://github.com/VincentQQu/NeRF-NQA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compression of Higher Order Ambisonics with Multichannel RVQGAN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12008v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12008v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toni Hirvonen, Mahmoud Namazi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A multichannel extension to the RVQGAN neural coding method is proposed, and
realized for data-driven compression of third-order Ambisonics audio. The
input- and output layers of the generator and discriminator models are modified
to accept multiple (16) channels without increasing the model bitrate. We also
propose a loss function for accounting for spatial perception in immersive
reproduction, and transfer learning from single-channel models. Listening test
results with 7.1.4 immersive playback show that the proposed extension is
suitable for coding scene-based, 16-channel Ambisonics content with good
quality at 16 kbps when trained and tested on the EigenScape database. The
model has potential applications for learning other types of content and
multichannel formats.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LinVT: Empower Your Image-level Large Language Model to Understand
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05185v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05185v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lishuai Gao, Yujie Zhong, Yingsen Zeng, Haoxian Tan, Dengjie Li, Zheng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have been widely used in various tasks,
motivating us to develop an LLM-based assistant for videos. Instead of training
from scratch, we propose a module to transform arbitrary well-trained
image-based LLMs into video-LLMs (after being trained on video data). To better
adapt image-LLMs for processing videos, we introduce two design principles:
linear transformation to preserve the original visual-language alignment and
representative information condensation from redundant video content. Guided by
these principles, we propose a plug-and-play Linear Video Tokenizer(LinVT),
which enables existing image-LLMs to understand videos. We benchmark LinVT with
six recent visual LLMs: Aquila, Blip-3, InternVL2, Mipha, Molmo and Qwen2-VL,
showcasing the high compatibility of LinVT. LinVT-based LLMs achieve
state-of-the-art performance across various video benchmarks, illustrating the
effectiveness of LinVT in multi-modal video understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preserving Speaker Information in Direct Speech-to-Speech Translation
  with Non-Autoregressive Generation and <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07316v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07316v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Zhou, Akinori Ito, Takashi Nose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech-to-Speech Translation (S2ST) refers to the conversion of speech in one
language into semantically equivalent speech in another language, facilitating
communication between speakers of different languages. Speech-to-Discrete Unit
Translation (S2UT), a mainstream approach for end-to-end S2ST, addresses
challenges such as error propagation across modules and slow inference speed
often encountered in traditional cascade systems. However, as discrete units
primarily capture content information, conventional S2UT methods fail to retain
speaker-specific characteristics from the source. Our previous work, SC-S2UT,
introduced a speaker adapter and a unit-to-mel structure, enabling the
preservation of speaker information and non-autoregressive speech generation.
Building on this foundation, this study proposes a self-supervised pretraining
method to enrich the information extracted by both the speaker adapter and the
unit-to-mel structure. Additionally, we investigate different feature fusion
strategies to further improve the integration of speaker and content features.
Experiments conducted on the CVSS-T dataset for ES-EN and FR-EN tasks
demonstrate that our proposed method achieves a BLEU score improvement of 1.14
compared to SC-S2UT, along with significant enhancements in MOS and speaker
similarity. Furthermore, our approach achieves translation quality comparable
to traditional S2UT, with only a minimal increase of 0.04s per utterance in
inference time, while maintaining high speaker similarity. These results
validate the effectiveness of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-10T00:00:00Z">2024-12-10</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">20</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmark for Evaluation and Analysis of Citation Recommendation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07713v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07713v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Puja Maharjan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Citation recommendation systems have attracted much academic interest,
resulting in many studies and implementations. These systems help authors
automatically generate proper citations by suggesting relevant references based
on the text they have written. However, the methods used in citation
recommendation differ across various studies and implementations. Some
approaches focus on the overall content of papers, while others consider the
context of the citation text. Additionally, the datasets used in these studies
include different aspects of papers, such as metadata, citation context, or
even the full text of the paper in various formats and structures. The
diversity in models, datasets, and evaluation metrics makes it challenging to
assess and compare citation recommendation methods effectively. To address this
issue, a standardized dataset and evaluation metrics are needed to evaluate
these models consistently. Therefore, we propose developing a benchmark
specifically designed to analyze and compare citation recommendation models.
This benchmark will evaluate the performance of models on different features of
the citation context and provide a comprehensive evaluation of the models
across all these tasks, presenting the results in a standardized way. By
creating a benchmark with standardized evaluation metrics, researchers and
practitioners in the field of citation recommendation will have a common
platform to assess and compare different models. This will enable meaningful
comparisons and help identify promising approaches for further research and
development in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniDocBench: Benchmarking Diverse PDF Document Parsing with
  Comprehensive Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linke Ouyang, Yuan Qu, Hongbin Zhou, Jiawei Zhu, Rui Zhang, Qunshu Lin, Bin Wang, Zhiyuan Zhao, Man Jiang, Xiaomeng Zhao, Jin Shi, Fan Wu, Pei Chu, Minghao Liu, Zhenxiang Li, Chao Xu, Bo Zhang, Botian Shi, Zhongying Tu, Conghui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document content extraction is crucial in computer vision, especially for
meeting the high-quality data needs of large language models (LLMs) and
retrieval-augmented generation (RAG) technologies. However, current document
parsing methods suffer from significant limitations in terms of diversity and
comprehensive evaluation. To address these challenges, we introduce
OmniDocBench, a novel multi-source benchmark designed to advance automated
document content extraction. OmniDocBench includes a meticulously curated and
annotated high-quality evaluation dataset comprising nine diverse document
types, such as academic papers, textbooks, slides, among others. Our benchmark
provides a flexible and comprehensive evaluation framework with 19 layout
category labels and 14 attribute labels, enabling multi-level assessments
across entire datasets, individual modules, or specific data types. Using
OmniDocBench, we perform an exhaustive comparative analysis of existing modular
pipelines and multimodal end-to-end methods, highlighting their limitations in
handling document diversity and ensuring fair evaluation. OmniDocBench
establishes a robust, diverse, and fair evaluation standard for the document
content extraction field, offering crucial insights for future advancements and
fostering the development of document parsing technologies. The codes and
dataset is available in https://github.com/opendatalab/OmniDocBench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SST framework for Document Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youchao Zhou, Heyan Huang, Zhijing Wu, Yuhang Liu, Xinglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-form document matching aims to judge the relevance between two documents
and has been applied to various scenarios. Most existing works utilize
hierarchical or long context models to process documents, which achieve coarse
understanding but may ignore details. Some researchers construct a document
view with similar sentences about aligned document subtopics to focus on
detailed matching signals. However, a long document generally contains multiple
subtopics. The matching signals are heterogeneous from multiple topics.
Considering only the homologous aligned subtopics may not be representative
enough and may cause biased modeling. In this paper, we introduce a new
framework to model representative matching signals. First, we propose to
capture various matching signals through subtopics of document pairs. Next, We
construct multiple document views based on subtopics to cover heterogeneous and
valuable details. However, existing spatial aggregation methods like attention,
which integrate all these views simultaneously, are hard to integrate
heterogeneous information. Instead, we propose temporal aggregation, which
effectively integrates different views gradually as the training progresses.
Experimental results show that our learning framework is effective on several
document-matching tasks, including news duplication and legal case retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bilingual BSARD: Extending Statutory Article Retrieval to Dutch <span class="chip">COLING</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Lotfi, Nikolay Banar, Nerses Yuzbashyan, Walter Daelemans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Statutory article retrieval plays a crucial role in making legal information
more accessible to both laypeople and legal professionals. Multilingual
countries like Belgium present unique challenges for retrieval models due to
the need for handling legal issues in multiple languages. Building on the
Belgian Statutory Article Retrieval Dataset (BSARD) in French, we introduce the
bilingual version of this dataset, bBSARD. The dataset contains parallel
Belgian statutory articles in both French and Dutch, along with legal questions
from BSARD and their Dutch translation. Using bBSARD, we conduct extensive
benchmarking of retrieval models available for Dutch and French. Our
benchmarking setup includes lexical models, zero-shot dense models, and
fine-tuned small foundation models. Our experiments show that BM25 remains a
competitive baseline compared to many zero-shot dense models in both languages.
We also observe that while proprietary models outperform open alternatives in
the zero-shot setting, they can be matched or surpassed by fine-tuning small
language-specific models. Our dataset and evaluation code are publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be presented at RegNLP-2025 (COLING)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAG-based Question Answering over Heterogeneous Data and Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Christmann, Gerhard Weikum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article presents the QUASAR system for question answering over
unstructured text, structured tables, and knowledge graphs, with unified
treatment of all sources. The system adopts a RAG-based architecture, with a
pipeline of evidence retrieval followed by answer generation, with the latter
powered by a moderate-sized language model. Additionally and uniquely, QUASAR
has components for question understanding, to derive crisper input for evidence
retrieval, and for re-ranking and filtering the retrieved evidence before
feeding the most informative pieces into the answer generation. Experiments
with three different benchmarks demonstrate the high answering quality of our
approach, being on par with or better than large GPT models, while keeping the
computational cost and energy consumption orders of magnitude lower.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Data Engineering Bulletin -- December 2024 Edition on RAG</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RLT4Rec: Reinforcement Learning <span class="highlight-title">Transformer</span> for User Cold Start and Item
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dilina Chandika Rajapakse, Douglas Leith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new sequential transformer reinforcement learning architecture
RLT4Rec and demonstrate that it achieves excellent performance in a range of
item recommendation tasks. RLT4Rec uses a relatively simple transformer
architecture that takes as input the user's (item,rating) history and outputs
the next item to present to the user. Unlike existing RL approaches, there is
no need to input a state observation or estimate. RLT4Rec handles new users and
established users within the same consistent framework and automatically
balances the "exploration" needed to discover the preferences of a new user
with the "exploitation" that is more appropriate for established users.
Training of RLT4Rec is robust and fast and is insensitive to the choice of
training data, learning to generate "good" personalised sequences that the user
tends to rate highly even when trained on "bad" data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Linear Item-Item Model for Sequential Recommendation <span class="chip">WSDM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07382v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07382v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seongmin Park, Mincheol Yoon, Minjin Choi, Jongwuk Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In sequential recommendation (SR), neural models have been actively explored
due to their remarkable performance, but they suffer from inefficiency inherent
to their complexity. On the other hand, linear SR models exhibit high
efficiency and achieve competitive or superior accuracy compared to neural
models. However, they solely deal with the sequential order of items (i.e.,
sequential information) and overlook the actual timestamp (i.e., temporal
information). It is limited to effectively capturing various user preference
drifts over time. To address this issue, we propose a novel linear SR model,
named TemporAl LinEar item-item model (TALE), incorporating temporal
information while preserving training/inference efficiency, with three key
components. (i) Single-target augmentation concentrates on a single target
item, enabling us to learn the temporal correlation for the target item. (ii)
Time interval-aware weighting utilizes the actual timestamp to discern the item
correlation depending on time intervals. (iii) Trend-aware normalization
reflects the dynamic shift of item popularity over time. Our empirical studies
show that TALE outperforms ten competing SR models by up to 18.71% gains on
five benchmark datasets. It also exhibits remarkable effectiveness in
evaluating long-tail items by up to 30.45% gains. The source code is available
at https://github.com/psm1206/TALE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WSDM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IntellectSeeker: A Personalized Literature Management System with the
  Probabilistic Model and Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weizhen Bian, Siyan Liu, Yubo Zhou, Dezhi Chen, Yijie Liao, Zhenzhen Fan, Aobo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Faced with the burgeoning volume of academic literature, researchers often
need help with uncertain article quality and mismatches in term searches using
traditional academic engines. We introduce IntellectSeeker, an innovative and
personalized intelligent academic literature management platform to address
these challenges. This platform integrates a Large Language Model (LLM)--based
semantic enhancement bot with a sophisticated probability model to personalize
and streamline literature searches. We adopted the GPT-3.5-turbo model to
transform everyday language into professional academic terms across various
scenarios using multiple rounds of few-shot learning. This adaptation mainly
benefits academic newcomers, effectively bridging the gap between general
inquiries and academic terminology. The probabilistic model intelligently
filters academic articles to align closely with the specific interests of
users, which are derived from explicit needs and behavioral patterns. Moreover,
IntellectSeeker incorporates an advanced recommendation system and text
compression tools. These features enable intelligent article recommendations
based on user interactions and present search results through concise one-line
summaries and innovative word cloud visualizations, significantly enhancing
research efficiency and user experience. IntellectSeeker offers academic
researchers a highly customizable literature management solution with
exceptional search precision and matching capabilities. The code can be found
here: https://github.com/LuckyBian/ISY5001
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MARec: Metadata Alignment for cold-start Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13298v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13298v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julien Monteil, Volodymyr Vaskovych, Wentao Lu, Anirban Majumder, Anton van den Hengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For many recommender systems, the primary data source is a historical record
of user clicks. The associated click matrix is often very sparse, as the number
of users x products can be far larger than the number of clicks. Such sparsity
is accentuated in cold-start settings, which makes the efficient use of
metadata information of paramount importance. In this work, we propose a simple
approach to address cold-start recommendations by leveraging content metadata,
Metadata Alignment for cold-start Recommendation. We show that this approach
can readily augment existing matrix factorization and autoencoder approaches,
enabling a smooth transition to top performing algorithms in warmer set-ups.
Our experimental results indicate three separate contributions: first, we show
that our proposed framework largely beats SOTA results on 4 cold-start datasets
with different sparsity and scale characteristics, with gains ranging from
+8.4% to +53.8% on reported ranking metrics; second, we provide an ablation
study on the utility of semantic features, and proves the additional gain
obtained by leveraging such features ranges between +46.8% and +105.5%; and
third, our approach is by construction highly competitive in warm set-ups, and
we propose a closed-form solution outperformed by SOTA results by only 0.8% on
average.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Retrieval: Generating Narratives in Conversational Recommender
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16780v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16780v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krishna Sayana, Raghavendra Vasudeva, Yuri Vasilevski, Kun Su, Liam Hebert, James Pine, Hubert Pham, Ambarish Jash, Sukhdeep Sodhi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advances in Large Language Model's generation and reasoning
capabilities present an opportunity to develop truly conversational
recommendation systems. However, effectively integrating recommender system
knowledge into LLMs for natural language generation which is tailored towards
recommendation tasks remains a challenge. This paper addresses this challenge
by making two key contributions.
  First, we introduce a new dataset (REGEN) for natural language generation
tasks in conversational recommendations. REGEN (Reviews Enhanced with
GEnerative Narratives) extends the Amazon Product Reviews dataset with rich
user narratives, including personalized explanations of product preferences,
product endorsements for recommended items, and summaries of user purchase
history. REGEN is made publicly available to facilitate further research.
Furthermore, we establish benchmarks using well-known generative metrics, and
perform an automated evaluation of the new dataset using a rater LLM. Second,
the paper introduces a fusion architecture (CF model with an LLM) which serves
as a baseline for REGEN. And to the best of our knowledge, represents the first
attempt to analyze the capabilities of LLMs in understanding recommender
signals and generating rich narratives. We demonstrate that LLMs can
effectively learn from simple fusion architectures utilizing interaction-based
CF embeddings, and this can be further enhanced using the metadata and
personalization data associated with items. Our experiments show that combining
CF and content embeddings leads to improvements of 4-12% in key language
metrics compared to using either type of embedding individually. We also
provide an analysis to interpret how CF and content embeddings contribute to
this new generative task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimistic Query Routing in Clustering-based Approximate Maximum Inner
  Product Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.12207v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.12207v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Bruch, Aditya Krishnan, Franco Maria Nardini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clustering-based nearest neighbor search is an effective method in which
points are partitioned into geometric shards to form an index, with only a few
shards searched during query processing to find a set of top-$k$ vectors. Even
though the search efficacy is heavily influenced by the algorithm that
identifies the shards to probe, it has received little attention in the
literature. This work bridges that gap by studying routing in clustering-based
maximum inner product search. We unpack existing routers and notice the
surprising contribution of optimism. We then take a page from the sequential
decision making literature and formalize that insight following the principle
of ``optimism in the face of uncertainty.'' In particular, we present a
framework that incorporates the moments of the distribution of inner products
within each shard to estimate the maximum inner product. We then present an
instance of our algorithm that uses only the first two moments to reach the
same accuracy as state-of-the-art routers such as ScaNN by probing up to $50\%$
fewer points on benchmark datasets. Our algorithm is also space-efficient: we
design a sketch of the second moment whose size is independent of the number of
points and requires $\mathcal{O}(1)$ vectors per shard.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model
  Recommender System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05668v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05668v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yashar Deldjoo, Tommaso di Noia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work takes a critical stance on previous studies concerning fairness
evaluation in Large Language Model (LLM)-based recommender systems, which have
primarily assessed consumer fairness by comparing recommendation lists
generated with and without sensitive user attributes. Such approaches
implicitly treat discrepancies in recommended items as biases, overlooking
whether these changes might stem from genuine personalization aligned with true
preferences of users. Moreover, these earlier studies typically address single
sensitive attributes in isolation, neglecting the complex interplay of
intersectional identities. In response to these shortcomings, we introduce
CFaiRLLM, an enhanced evaluation framework that not only incorporates true
preference alignment but also rigorously examines intersectional fairness by
considering overlapping sensitive attributes. Additionally, CFaiRLLM introduces
diverse user profile sampling strategies-random, top-rated, and
recency-focused-to better understand the impact of profile generation fed to
LLMs in light of inherent token limitations in these systems. Given that
fairness depends on accurately understanding users' tastes and preferences,,
these strategies provide a more realistic assessment of fairness within
RecLLMs.
  The results demonstrated that true preference alignment offers a more
personalized and fair assessment compared to similarity-based measures,
revealing significant disparities when sensitive and intersectional attributes
are incorporated. Notably, our study finds that intersectional attributes
amplify fairness gaps more prominently, especially in less structured domains
such as music recommendations in LastFM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S+t-SNE -- Bringing Dimensionality Reduction to Data Streams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17643v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17643v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro C. Vieira, João P. Montrezol, João T. Vieira, João Gama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handle
infinite data streams. The core idea behind S+t-SNE is to update the t-SNE
embedding incrementally as new data arrives, ensuring scalability and
adaptability to handle streaming scenarios. By selecting the most important
points at each step, the algorithm ensures scalability while keeping
informative visualisations. By employing a blind method for drift management,
the algorithm adjusts the embedding space, which facilitates the visualisation
of evolving data dynamics. Our experimental evaluations demonstrate the
effectiveness and efficiency of S+t-SNE, whilst highlighting its ability to
capture patterns in a streaming scenario. We hope our approach offers
researchers and practitioners a real-time tool for understanding and
interpreting high-dimensional data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint has undergone peer review but does not have any
  post-submission improvements or corrections. Full version after peer-review
  and post-acceptance improvements was presented at IDA2024
  (https://ida2024.org/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hypergrah-Enhanced Dual Convolutional Network for Bundle Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11018v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11018v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Li, Kangbo Liu, Yaoxin Wu, Zhaoxuan Wang, Erik Cambria, Xiaoxu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bundle recommendations strive to offer users a set of items as a package
named bundle, enhancing convenience and contributing to the seller's revenue.
While previous approaches have demonstrated notable performance, we argue that
they may compromise the ternary relationship among users, items, and bundles.
This compromise can result in information loss, ultimately impacting the
overall model performance. To address this gap, we develop a unified model for
bundle recommendation, termed hypergraph-enhanced dual convolutional neural
network (HED). Our approach is characterized by two key aspects. Firstly, we
construct a complete hypergraph to capture interaction dynamics among users,
items, and bundles. Secondly, we incorporate U-B interaction information to
enhance the information representation derived from users and bundle embedding
vectors. Extensive experimental results on the Youshu and Netease datasets have
demonstrated that HED surpasses state-of-the-art baselines, proving its
effectiveness. In addition, various ablation studies and sensitivity analyses
revealed the working mechanism and proved our effectiveness. Codes and datasets
are available at https://github.com/AAI-Lab/HED
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Deconfounding via Confounder Disentanglement for Dual-Target
  Cross-Domain Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11180v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11180v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajie Zhu, Yan Wang, Feng Zhu, Zhu Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, dual-target Cross-Domain Recommendation (CDR) has been
proposed to capture comprehensive user preferences in order to ultimately
enhance the recommendation accuracy in both data-richer and data-sparser
domains simultaneously. However, in addition to users' true preferences, the
user-item interactions might also be affected by confounders (e.g., free
shipping, sales promotion). As a result, dual-target CDR has to meet two
challenges: (1) how to effectively decouple observed confounders, including
single-domain confounders and cross-domain confounders, and (2) how to preserve
the positive effects of observed confounders on predicted interactions, while
eliminating their negative effects on capturing comprehensive user preferences.
To address the above two challenges, we propose a Causal Deconfounding
framework via Confounder Disentanglement for dual-target Cross-Domain
Recommendation, called CD2CDR. In CD2CDR, we first propose a confounder
disentanglement module to effectively decouple observed single-domain and
cross-domain confounders. We then propose a causal deconfounding module to
preserve the positive effects of such observed confounders and eliminate their
negative effects via backdoor adjustment, thereby enhancing the recommendation
accuracy in each domain. Extensive experiments conducted on five real-world
datasets demonstrate that CD2CDR significantly outperforms the state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Argumentative Experience: Reducing Confirmation Bias on Controversial
  Issues through LLM-Generated Multi-Persona Debates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04629v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04629v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Shi, Houjiang Liu, Yian Wong, Utkarsh Mujumdar, Dan Zhang, Jacek Gwizdka, Matthew Lease
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are enabling designers to give life to exciting
new user experiences for information access. In this work, we present a system
that generates LLM personas to debate a topic of interest from different
perspectives. How might information seekers use and benefit from such a system?
Can centering information access around diverse viewpoints help to mitigate
thorny challenges like confirmation bias in which information seekers
over-trust search results matching existing beliefs? How do potential biases
and hallucinations in LLMs play out alongside human users who are also fallible
and possibly biased?
  Our study exposes participants to multiple viewpoints on controversial issues
via a mixed-methods, within-subjects study. We use eye-tracking metrics to
quantitatively assess cognitive engagement alongside qualitative feedback.
Compared to a baseline search system, we see more creative interactions and
diverse information-seeking with our multi-persona debate system, which more
effectively reduces user confirmation bias and conviction toward their initial
beliefs. Overall, our study contributes to the emerging design space of
LLM-based information access systems, specifically investigating the potential
of simulated personas to promote greater exposure to information diversity,
emulate collective intelligence, and mitigate bias in information seeking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LEARN: Knowledge Adaptation from Large Language Model to Recommendation
  for Practical Industrial Application <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.03988v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.03988v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Jia, Yipei Wang, Yan Li, Honggang Chen, Xuehan Bai, Zhaocheng Liu, Jian Liang, Quan Chen, Han Li, Peng Jiang, Kun Gai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contemporary recommendation systems predominantly rely on ID embedding to
capture latent associations among users and items. However, this approach
overlooks the wealth of semantic information embedded within textual
descriptions of items, leading to suboptimal performance and poor
generalizations. Leveraging the capability of large language models to
comprehend and reason about textual content presents a promising avenue for
advancing recommendation systems. To achieve this, we propose an Llm-driven
knowlEdge Adaptive RecommeNdation (LEARN) framework that synergizes open-world
knowledge with collaborative knowledge. We address computational complexity
concerns by utilizing pretrained LLMs as item encoders and freezing LLM
parameters to avoid catastrophic forgetting and preserve open-world knowledge.
To bridge the gap between the open-world and collaborative domains, we design a
twin-tower structure supervised by the recommendation task and tailored for
practical industrial application. Through experiments on the real large-scale
industrial dataset and online A/B tests, we demonstrate the efficacy of our
approach in industry application. We also achieve state-of-the-art performance
on six Amazon Review datasets to verify the superiority of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs-as-Judges: A Comprehensive <span class="highlight-title">Survey</span> on LLM-based Evaluation Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05579v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05579v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitao Li, Qian Dong, Junjie Chen, Huixue Su, Yujia Zhou, Qingyao Ai, Ziyi Ye, Yiqun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Large Language Models (LLMs) has driven their
expanding application across various fields. One of the most promising
applications is their role as evaluators based on natural language responses,
referred to as ''LLMs-as-judges''. This framework has attracted growing
attention from both academia and industry due to their excellent effectiveness,
ability to generalize across tasks, and interpretability in the form of natural
language. This paper presents a comprehensive survey of the LLMs-as-judges
paradigm from five key perspectives: Functionality, Methodology, Applications,
Meta-evaluation, and Limitations. We begin by providing a systematic definition
of LLMs-as-Judges and introduce their functionality (Why use LLM judges?). Then
we address methodology to construct an evaluation system with LLMs (How to use
LLM judges?). Additionally, we investigate the potential domains for their
application (Where to use LLM judges?) and discuss methods for evaluating them
in various contexts (How to evaluate LLM judges?). Finally, we provide a
detailed analysis of the limitations of LLM judges and discuss potential future
directions. Through a structured and comprehensive analysis, we aim aims to
provide insights on the development and application of LLMs-as-judges in both
research and practice. We will continue to maintain the relevant resource list
at https://github.com/CSHaitao/Awesome-LLMs-as-Judges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>60 pages, comprehensive and continuously updated</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Personalized Federated Collaborative Filtering: A Variational
  AutoEncoder Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08931v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08931v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Li, Guodong Long, Tianyi Zhou, Jing Jiang, Chengqi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Collaborative Filtering (FedCF) is an emerging field focused on
developing a new recommendation framework with preserving privacy in a
federated setting. Existing FedCF methods typically combine distributed
Collaborative Filtering (CF) algorithms with privacy-preserving mechanisms, and
then preserve personalized information into a user embedding vector. However,
the user embedding is usually insufficient to preserve the rich information of
the fine-grained personalization across heterogeneous clients. This paper
proposes a novel personalized FedCF method by preserving users' personalized
information into a latent variable and a neural model simultaneously.
Specifically, we decompose the modeling of user knowledge into two encoders,
each designed to capture shared knowledge and personalized knowledge
separately. A personalized gating network is then applied to balance
personalization and generalization between the global and local encoders.
Moreover, to effectively train the proposed framework, we model the CF problem
as a specialized Variational AutoEncoder (VAE) task by integrating user
interaction vector reconstruction with missing value prediction. The decoder is
trained to reconstruct the implicit feedback from items the user has interacted
with, while also predicting items the user might be interested in but has not
yet interacted with. Experimental results on benchmark datasets demonstrate
that the proposed method outperforms other baseline methods, showcasing
superior performance. Our code is available at https://github.com/mtics/FedDAE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures, 4 tables, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BayesCNS: A Unified Bayesian Approach to Address Cold Start and
  Non-Stationarity in Search Systems at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02126v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02126v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Randy Ardywibowo, Rakesh Sunki, Lucy Kuo, Sankalp Nayak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information Retrieval (IR) systems used in search and recommendation
platforms frequently employ Learning-to-Rank (LTR) models to rank items in
response to user queries. These models heavily rely on features derived from
user interactions, such as clicks and engagement data. This dependence
introduces cold start issues for items lacking user engagement and poses
challenges in adapting to non-stationary shifts in user behavior over time. We
address both challenges holistically as an online learning problem and propose
BayesCNS, a Bayesian approach designed to handle cold start and non-stationary
distribution shifts in search systems at scale. BayesCNS achieves this by
estimating prior distributions for user-item interactions, which are
continuously updated with new user interactions gathered online. This online
learning procedure is guided by a ranker model, enabling efficient exploration
of relevant items using contextual information provided by the ranker. We
successfully deployed BayesCNS in a large-scale search system and demonstrated
its efficacy through comprehensive offline and online experiments. Notably, an
online A/B experiment showed a 10.60% increase in new item interactions and a
1.05% improvement in overall success metrics over the existing production
baseline.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Frechet Music Distance: A Metric For Generative Symbolic Music
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Retkowski, Jakub Stępniak, Mateusz Modrzejewski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we introduce the Frechet Music Distance (FMD), a novel
evaluation metric for generative symbolic music models, inspired by the Frechet
Inception Distance (FID) in computer vision and Frechet Audio Distance (FAD) in
generative audio. FMD calculates the distance between distributions of
reference and generated symbolic music embeddings, capturing abstract musical
features. We validate FMD across several datasets and models. Results indicate
that FMD effectively differentiates model quality, providing a domain-specific
metric for evaluating symbolic music generation, and establishing a
reproducible standard for future research in symbolic music modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Latency Scalable Streaming for Event-Based Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Hamara, Benjamin Kilpatrick, Alex Baratta, Brendon Kofink, Andrew C. Freeman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, we have witnessed the rise of novel ``event-based'' camera sensors
for high-speed, low-power video capture. Rather than recording discrete image
frames, these sensors output asynchronous ``event'' tuples with microsecond
precision, only when the brightness change of a given pixel exceeds a certain
threshold. Although these sensors have enabled compelling new computer vision
applications, these applications often require expensive, power-hungry GPU
systems, rendering them incompatible for deployment on the low-power devices
for which event cameras are optimized. Whereas receiver-driven rate adaptation
is a crucial feature of modern video streaming solutions, this topic is
underexplored in the realm of event-based vision systems. On a real-world event
camera dataset, we first demonstrate that a state-of-the-art object detection
application is resilient to dramatic data loss, and that this loss may be
weighted towards the end of each temporal window. We then propose a scalable
streaming method for event-based data based on Media Over QUIC, prioritizing
object detection performance and low latency. The application server can
receive complementary event data across several streams simultaneously, and
drop streams as needed to maintain a certain latency. With a latency target of
5 ms for end-to-end transmission across a small network, we observe an average
reduction in detection mAP as low as 0.36. With a more relaxed latency target
of 50 ms, we observe an average mAP reduction as low as 0.19.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STIV: Scalable Text and Image Conditioned Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongyu Lin, Wei Liu, Chen Chen, Jiasen Lu, Wenze Hu, Tsu-Jui Fu, Jesse Allardice, Zhengfeng Lai, Liangchen Song, Bowen Zhang, Cha Chen, Yiran Fei, Yifan Jiang, Lezhi Li, Yizhou Sun, Kai-Wei Chang, Yinfei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of video generation has made remarkable advancements, yet there
remains a pressing need for a clear, systematic recipe that can guide the
development of robust and scalable models. In this work, we present a
comprehensive study that systematically explores the interplay of model
architectures, training recipes, and data curation strategies, culminating in a
simple and scalable text-image-conditioned video generation method, named STIV.
Our framework integrates image condition into a Diffusion Transformer (DiT)
through frame replacement, while incorporating text conditioning via a joint
image-text conditional classifier-free guidance. This design enables STIV to
perform both text-to-video (T2V) and text-image-to-video (TI2V) tasks
simultaneously. Additionally, STIV can be easily extended to various
applications, such as video prediction, frame interpolation, multi-view
generation, and long video generation, etc. With comprehensive ablation studies
on T2I, T2V, and TI2V, STIV demonstrate strong performance, despite its simple
design. An 8.7B model with 512 resolution achieves 83.1 on VBench T2V,
surpassing both leading open and closed-source models like CogVideoX-5B, Pika,
Kling, and Gen-3. The same-sized model also achieves a state-of-the-art result
of 90.1 on VBench I2V task at 512 resolution. By providing a transparent and
extensible recipe for building cutting-edge video generation models, we aim to
empower future research and accelerate progress toward more versatile and
reliable video generation solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning <span class="highlight-title">Self-Supervised</span> Audio-Visual Representations for Sound
  Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sudha Krishnamurthy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel self-supervised approach for learning audio and visual
representations from unlabeled videos, based on their correspondence. The
approach uses an attention mechanism to learn the relative importance of
convolutional features extracted at different resolutions from the audio and
visual streams and uses the attention features to encode the audio and visual
input based on their correspondence. We evaluated the representations learned
by the model to classify audio-visual correlation as well as to recommend sound
effects for visual scenes. Our results show that the representations generated
by the attention model improves the correlation accuracy compared to the
baseline, by 18% and the recommendation accuracy by 10% for VGG-Sound, which is
a public video dataset. Additionally, audio-visual representations learned by
training the attention model with cross-modal contrastive learning further
improves the recommendation performance, based on our evaluation using
VGG-Sound and a more challenging dataset consisting of gameplay video
recordings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the Proceedings of the International Symposium on Visual
  Computing, 2021 https://dl.acm.org/doi/10.1007/978-3-030-90436-4_10</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Sentiment Analysis Based on Causal Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuhai Chen, Pengpeng Huang, Xuri Ge, Jie Huang, Zishuo Bao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of multimedia, the shift from unimodal textual
sentiment analysis to multimodal image-text sentiment analysis has obtained
academic and industrial attention in recent years. However, multimodal
sentiment analysis is affected by unimodal data bias, e.g., text sentiment is
misleading due to explicit sentiment semantic, leading to low accuracy in the
final sentiment classification. In this paper, we propose a novel
CounterFactual Multimodal Sentiment Analysis framework (CF-MSA) using causal
counterfactual inference to construct multimodal sentiment causal inference.
CF-MSA mitigates the direct effect from unimodal bias and ensures heterogeneity
across modalities by differentiating the treatment variables between
modalities. In addition, considering the information complementarity and bias
differences between modalities, we propose a new optimisation objective to
effectively integrate different modalities and reduce the inherent bias from
each modality. Experimental results on two public datasets, MVSA-Single and
MVSA-Multiple, demonstrate that the proposed CF-MSA has superior debiasing
capability and achieves new state-of-the-art performances. We will release the
code and datasets to facilitate future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reducing Traffic Wastage in Video Streaming via Bandwidth-Efficient
  Bitrate Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hairong Su, Shibo Wang, Shusen Yang, Tianchi Huang, Xuebin Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bitrate adaptation (also known as ABR) is a crucial technique to improve the
quality of experience (QoE) for video streaming applications. However, existing
ABR algorithms suffer from severe traffic wastage, which refers to the traffic
cost of downloading the video segments that users do not finally consume, for
example, due to early departure or video skipping. In this paper, we carefully
formulate the dynamics of buffered data volume (BDV), a strongly correlated
indicator of traffic wastage, which, to the best of our knowledge, is the first
time to rigorously clarify the effect of downloading plans on potential
wastage. To reduce wastage while keeping a high QoE, we present a
bandwidth-efficient bitrate adaptation algorithm (named BE-ABR), achieving
consistently low BDV without distinct QoE losses. Specifically, we design a
precise, time-aware transmission delay prediction model over the Transformer
architecture, and develop a fine-grained buffer control scheme. Through
extensive experiments conducted on emulated and real network environments
including WiFi, 4G, and 5G, we demonstrate that BE-ABR performs well in both
QoE and bandwidth savings, enabling a 60.87\% wastage reduction and a
comparable, or even better, QoE, compared to the state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PTSBench: A Comprehensive Post-Training Sparsity Benchmark Towards
  Algorithms and Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zining Wnag, Jinyang Guo, Ruihao Gong, Yang Yong, Aishan Liu, Yushi Huang, Jiaheng Liu, Xianglong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increased attention to model efficiency, post-training sparsity
(PTS) has become more and more prevalent because of its effectiveness and
efficiency. However, there remain questions on better practice of PTS
algorithms and the sparsification ability of models, which hinders the further
development of this area. Therefore, a benchmark to comprehensively investigate
the issues above is urgently needed. In this paper, we propose the first
comprehensive post-training sparsity benchmark called PTSBench towards
algorithms and models. We benchmark 10+ PTS general-pluggable fine-grained
techniques on 3 typical tasks using over 40 off-the-shelf model architectures.
Through extensive experiments and analyses, we obtain valuable conclusions and
provide several insights from both algorithms and model aspects. Our PTSBench
can provide (1) new observations for a better understanding of the PTS
algorithms, (2) in-depth and comprehensive evaluations for the sparsification
ability of models, and (3) a well-structured and easy-integrate open-source
framework. We hope this work will provide illuminating conclusions and advice
for future studies of post-training sparsity methods and
sparsification-friendly model design. The code for our PTSBench is released at
\href{https://github.com/ModelTC/msbench}{https://github.com/ModelTC/msbench}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RoboMM: All-in-One Multimodal Large Model for Robotic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Yan, Fanfan Liu, Liming Zheng, Yufeng Zhong, Yiyang Huang, Zechao Guan, Chengjian Feng, Lin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, robotics has advanced significantly through the integration
of larger models and large-scale datasets. However, challenges remain in
applying these models to 3D spatial interactions and managing data collection
costs. To address these issues, we propose the multimodal robotic manipulation
model, RoboMM, along with the comprehensive dataset, RoboData. RoboMM enhances
3D perception through camera parameters and occupancy supervision. Building on
OpenFlamingo, it incorporates Modality-Isolation-Mask and multimodal decoder
blocks, improving modality fusion and fine-grained perception. RoboData offers
the complete evaluation system by integrating several well-known datasets,
achieving the first fusion of multi-view images, camera parameters, depth maps,
and actions, and the space alignment facilitates comprehensive learning from
diverse robotic datasets. Equipped with RoboData and the unified physical
space, RoboMM is the generalist policy that enables simultaneous evaluation
across all tasks within multiple datasets, rather than focusing on limited
selection of data or tasks. Its design significantly enhances robotic
manipulation performance, increasing the average sequence length on the CALVIN
from 1.7 to 3.3 and ensuring cross-embodiment capabilities, achieving
state-of-the-art results across multiple datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Annotation Techniques for Judo Combat Phase Classification from
  Tournament Footage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Miyaguchi, Jed Moutahir, Tanmay Sutar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a semi-supervised approach to extracting and analyzing
combat phases in judo tournaments using live-streamed footage. The objective is
to automate the annotation and summarization of live streamed judo matches. We
train models that extract relevant entities and classify combat phases from
fixed-perspective judo recordings. We employ semi-supervised methods to address
limited labeled data in the domain. We build a model of combat phases via
transfer learning from a fine-tuned object detector to classify the presence,
activity, and standing state of the match. We evaluate our approach on a
dataset of 19 thirty-second judo clips, achieving an F1 score on a $20\%$ test
hold-out of 0.66, 0.78, and 0.87 for the three classes, respectively. Our
results show initial promise for automating more complex information retrieval
tasks using rigorous methods with limited labeled data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EvRepSL: Event-Stream Representation via <span class="highlight-title">Self-Supervised</span> Learning for
  Event-Based Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Qu, Xiaoming Chen, Yuk Ying Chung, Yiran Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event-stream representation is the first step for many computer vision tasks
using event cameras. It converts the asynchronous event-streams into a
formatted structure so that conventional machine learning models can be applied
easily. However, most of the state-of-the-art event-stream representations are
manually designed and the quality of these representations cannot be guaranteed
due to the noisy nature of event-streams. In this paper, we introduce a
data-driven approach aiming at enhancing the quality of event-stream
representations. Our approach commences with the introduction of a new
event-stream representation based on spatial-temporal statistics, denoted as
EvRep. Subsequently, we theoretically derive the intrinsic relationship between
asynchronous event-streams and synchronous video frames. Building upon this
theoretical relationship, we train a representation generator, RepGen, in a
self-supervised learning manner accepting EvRep as input. Finally, the
event-streams are converted to high-quality representations, termed as EvRepSL,
by going through the learned RepGen (without the need of fine-tuning or
retraining). Our methodology is rigorously validated through extensive
evaluations on a variety of mainstream event-based classification and optical
flow datasets (captured with various types of event cameras). The experimental
results highlight not only our approach's superior performance over existing
event-stream representations but also its versatility, being agnostic to
different event cameras and tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published on IEEE Transactions on Image Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CHORDONOMICON: A <span class="highlight-title">Dataset</span> of 666,000 Songs and their Chord Progressions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22046v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22046v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Spyridon Kantarelis, Konstantinos Thomas, Vassilis Lyberatos, Edmund Dervakos, Giorgos Stamou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chord progressions encapsulate important information about music, pertaining
to its structure and conveyed emotions. They serve as the backbone of musical
composition, and in many cases, they are the sole information required for a
musician to play along and follow the music. Despite their importance, chord
progressions as a data domain remain underexplored. There is a lack of
large-scale datasets suitable for deep learning applications, and limited
research exploring chord progressions as an input modality. In this work, we
present Chordonomicon, a dataset of over 666,000 songs and their chord
progressions, annotated with structural parts, genre, and release date -
created by scraping various sources of user-generated progressions and
associated metadata. We demonstrate the practical utility of the Chordonomicon
dataset for classification and generation tasks, and discuss its potential to
provide valuable insights to the research community. Chord progressions are
unique in their ability to be represented in multiple formats (e.g. text,
graph) and the wealth of information chords convey in given contexts, such as
their harmonic function . These characteristics make the Chordonomicon an ideal
testbed for exploring advanced machine learning techniques, including
transformers, graph machine learning, and hybrid systems that combine knowledge
representation and machine learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12140v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12140v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Shashank Kalakonda, Shubh Maheshwari, Ravi Kiran Sarvadevabhatla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MoRAG, a novel multi-part fusion based retrieval-augmented
generation strategy for text-based human motion generation. The method enhances
motion diffusion models by leveraging additional knowledge obtained through an
improved motion retrieval process. By effectively prompting large language
models (LLMs), we address spelling errors and rephrasing issues in motion
retrieval. Our approach utilizes a multi-part retrieval strategy to improve the
generalizability of motion retrieval across the language space. We create
diverse samples through the spatial composition of the retrieved motions.
Furthermore, by utilizing low-level, part-specific motion information, we can
construct motion samples for unseen text descriptions. Our experiments
demonstrate that our framework can serve as a plug-and-play module, improving
the performance of motion diffusion models. Code, pretrained models and sample
videos are available at: https://motion-rag.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SOMONITOR: Combining Explainable AI & Large Language Models for
  Marketing Analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13117v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13117v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandr Farseev, Qi Yang, Marlo Ongpin, Ilia Gossoudarev, Yu-Yi Chu-Farseeva, Sergey Nikolenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online marketing faces formidable challenges in managing and interpreting
immense volumes of data necessary for competitor analysis, content research,
and strategic branding. It is impossible to review hundreds to thousands of
transient online content items by hand, and partial analysis often leads to
suboptimal outcomes and poorly performing campaigns. We introduce an
explainable AI framework SOMONITOR that aims to synergize human intuition with
AI-based efficiency, helping marketers across all stages of the marketing
funnel, from strategic planning to content creation and campaign execution.
SOMONITOR incorporates a CTR prediction and ranking model for advertising
content and uses large language models (LLMs) to process high-performing
competitor content, identifying core content pillars such as target audiences,
customer needs, and product features. These pillars are then organized into
broader categories, including communication themes and targeted customer
personas. By integrating these insights with data from the brand's own
advertising campaigns, SOMONITOR constructs a narrative for addressing new
customer personas and simultaneously generates detailed content briefs in the
form of user stories that, as shown in the conducted case study, can be
directly applied by marketing teams to streamline content production and
campaign execution. The adoption of SOMONITOR in daily operations allows
digital marketers to quickly parse through extensive datasets, offering
actionable insights that significantly enhance campaign effectiveness and
overall job satisfaction.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-09T00:00:00Z">2024-12-09</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge
  Distillation for Question Answering <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirhossein Abaskohi, Spandana Gella, Giuseppe Carenini, Issam H. Laradji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal multihop question answering is a complex task that requires
reasoning over multiple sources of information, such as images and text, to
answer questions. While there has been significant progress in visual question
answering, the multihop setting remains unexplored due to the lack of
high-quality datasets. Current methods focus on single-hop question answering
or a single modality, which makes them unsuitable for real-world scenarios such
as analyzing multimodal educational materials, summarizing lengthy academic
articles, or interpreting scientific studies that combine charts, images, and
text. To address this gap, we propose a novel methodology, introducing the
first framework for creating a high-quality dataset that enables training
models for multimodal multihop question answering. Our approach consists of a
5-stage pipeline that involves acquiring relevant multimodal documents from
Wikipedia, synthetically generating high-level questions and answers, and
validating them through rigorous criteria to ensure quality data. We evaluate
our methodology by training models on our synthesized dataset and testing on
two benchmarks, our results demonstrate that, with an equal sample size, models
trained on our synthesized data outperform those trained on human-collected
data by 1.9 in exact match (EM) on average. We believe our data synthesis
method will serve as a strong foundation for training and evaluating multimodal
multihop question answering models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 11 figures, 10 tables, Submitted to CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Conversational and Collaborative Signals for Conversational
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Bin Rabiah, Nafis Sadeq, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational recommendation systems (CRS) leverage contextual information
from conversations to generate recommendations but often struggle due to a lack
of collaborative filtering (CF) signals, which capture user-item interaction
patterns essential for accurate recommendations. We introduce Reddit-ML32M, a
dataset that links reddit conversations with interactions on MovieLens 32M, to
enrich item representations by leveraging collaborative knowledge and
addressing interaction sparsity in conversational datasets. We propose an
LLM-based framework that uses Reddit-ML32M to align LLM-generated
recommendations with CF embeddings, refining rankings for better performance.
We evaluate our framework against three sets of baselines: CF-based
recommenders using only interactions from CRS tasks, traditional CRS models,
and LLM-based methods relying on conversational context without item
representations. Our approach achieves consistent improvements, including a
12.32% increase in Hit Rate and a 9.9% improvement in NDCG, outperforming the
best-performing baseline that relies on conversational context but lacks
collaborative item representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient user history modeling with amortized inference for deep
  learning recommendation models <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lars Hertel, Neil Daftary, Fedor Borisyuk, Aman Gupta, Rahul Mazumder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study user history modeling via Transformer encoders in deep learning
recommendation models (DLRM). Such architectures can significantly improve
recommendation quality, but usually incur high latency cost necessitating
infrastructure upgrades or very small Transformer models. An important part of
user history modeling is early fusion of the candidate item and various methods
have been studied. We revisit early fusion and compare concatenation of the
candidate to each history item against appending it to the end of the list as a
separate item. Using the latter method, allows us to reformulate the recently
proposed amortized history inference algorithm M-FALCON \cite{zhai2024actions}
for the case of DLRM models. We show via experimental results that appending
with cross-attention performs on par with concatenation and that amortization
significantly reduces inference costs. We conclude with results from deploying
this model on the LinkedIn Feed and Ads surfaces, where amortization reduces
latency by 30\% compared to non-amortized inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, WWW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DEEPER: Dense Electroencephalography Passage Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niall McGuire, Yashar Moshfeghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information retrieval systems have historically relied on explicit query
formulation, requiring users to translate their information needs into text.
This process is particularly disruptive during reading tasks, where users must
interrupt their natural flow to formulate queries. We present DEEPER (Dense
Electroencephalography Passage Retrieval), a novel framework that enables
direct retrieval of relevant passages from users' neural signals during
naturalistic reading without intermediate text translation. Building on dense
retrieval architectures, DEEPER employs a dual-encoder approach with
specialised components for processing neural data, mapping EEG signals and text
passages into a shared semantic space. Through careful architecture design and
cross-modal negative sampling strategies, our model learns to align neural
patterns with their corresponding textual content. Experimental results on the
ZuCo dataset demonstrate that direct brain-to-passage retrieval significantly
outperforms current EEG-to-text baselines, achieving a 571% improvement in
Precision@1. Our ablation studies reveal that the model successfully learns
aligned representations between EEG and text modalities (0.29 cosine
similarity), while our hard negative sampling strategy contributes to overall
performance increases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Search and Recommendation Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aryan Duhan, Aryan Singhal, Shourya Sharma,  Neeraj, Arti MK
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a new semantic search algorithm that uses Word2Vec and
Annoy Index to improve the efficiency of information retrieval from large
datasets. The proposed approach addresses the limitations of traditional search
methods by offering enhanced speed, accuracy, and scalability. Testing on
datasets up to 100GB demonstrates the method's effectiveness in processing vast
amounts of data while maintaining high precision and performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PRECISE: <span class="highlight-title">Pre-train</span>ing Sequential Recommenders with Collaborative and
  Semantic Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chonggang Song, Chunxu Shen, Hao Gu, Yaoming Wu, Lingling Yi, Jie Wen, Chuan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world recommendation systems commonly offer diverse content scenarios
for users to interact with. Considering the enormous number of users in
industrial platforms, it is infeasible to utilize a single unified
recommendation model to meet the requirements of all scenarios. Usually,
separate recommendation pipelines are established for each distinct scenario.
This practice leads to challenges in comprehensively grasping users' interests.
Recent research endeavors have been made to tackle this problem by pre-training
models to encapsulate the overall interests of users. Traditional pre-trained
recommendation models mainly capture user interests by leveraging collaborative
signals. Nevertheless, a prevalent drawback of these systems is their
incapacity to handle long-tail items and cold-start scenarios. With the recent
advent of large language models, there has been a significant increase in
research efforts focused on exploiting LLMs to extract semantic information for
users and items. However, text-based recommendations highly rely on elaborate
feature engineering and frequently fail to capture collaborative similarities.
To overcome these limitations, we propose a novel pre-training framework for
sequential recommendation, termed PRECISE. This framework combines
collaborative signals with semantic information. Moreover, PRECISE employs a
learning framework that initially models users' comprehensive interests across
all recommendation scenarios and subsequently concentrates on the specific
interests of target-scene behaviors. We demonstrate that PRECISE precisely
captures the entire range of user interests and effectively transfers them to
the target interests. Empirical findings reveal that the PRECISE framework
attains outstanding performance on both public and industrial datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Methods for Legal Citation Prediction in the Age of LLMs: An Australian
  Law Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Shareghi, Jiuzhou Han, Paul Burgess
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Large Language Models (LLMs) have shown great potential
across a wide range of legal tasks. Despite these advances, mitigating
hallucination remains a significant challenge, with state-of-the-art LLMs still
frequently generating incorrect legal references. In this paper, we focus on
the problem of legal citation prediction within the Australian law context,
where correctly identifying and citing relevant legislations or precedents is
critical. We compare several approaches: prompting general purpose and
law-specialised LLMs, retrieval-only pipelines with both generic and
domain-specific embeddings, task-specific instruction-tuning of LLMs, and
hybrid strategies that combine LLMs with retrieval augmentation, query
expansion, or voting ensembles. Our findings indicate that domain-specific
pre-training alone is insufficient for achieving satisfactory citation accuracy
even after law-specialised pre-training. In contrast, instruction tuning on our
task-specific dataset dramatically boosts performance reaching the best results
across all settings. We also highlight that database granularity along with the
type of embeddings play a critical role in the performance of retrieval
systems. Among retrieval-based approaches, hybrid methods consistently
outperform retrieval-only setups, and among these, ensemble voting delivers the
best result by combining the predictive quality of instruction-tuned LLMs with
the retrieval system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For code, data, and models see https://auslawbench.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predictive Models in Sequential Recommendations: Bridging Performance
  Laws with Data Quality Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00430v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00430v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingjia Shen, Hao Wang, Chuhan Wu, Jin Yao Chin, Wei Guo, Yong Liu, Huifeng Guo, Defu Lian, Ruiming Tang, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential Recommendation (SR) plays a critical role in predicting users'
sequential preferences. Despite its growing prominence in various industries,
the increasing scale of SR models incurs substantial computational costs and
unpredictability, challenging developers to manage resources efficiently. Under
this predicament, Scaling Laws have achieved significant success by examining
the loss as models scale up. However, there remains a disparity between loss
and model performance, which is of greater concern in practical applications.
Moreover, as data continues to expand, it incorporates repetitive and
inefficient data. In response, we introduce the Performance Law for SR models,
which aims to theoretically investigate and model the relationship between
model performance and data quality. Specifically, we first fit the HR and NDCG
metrics to transformer-based SR models. Subsequently, we propose Approximate
Entropy (ApEn) to assess data quality, presenting a more nuanced approach
compared to traditional data quantity metrics. Our method enables accurate
predictions across various dataset scales and model sizes, demonstrating a
strong correlation in large SR models and offering insights into achieving
optimal performance for any given model configuration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Croissant: A Metadata Format for ML-Ready <span class="highlight-title">Dataset</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19546v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19546v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mubashara Akhtar, Omar Benjelloun, Costanza Conforti, Luca Foschini, Joan Giner-Miguelez, Pieter Gijsbers, Sujata Goswami, Nitisha Jain, Michalis Karamousadakis, Michael Kuchnik, Satyapriya Krishna, Sylvain Lesage, Quentin Lhoest, Pierre Marcenac, Manil Maskey, Peter Mattson, Luis Oala, Hamidah Oderinwale, Pierre Ruyssen, Tim Santos, Rajat Shinde, Elena Simperl, Arjun Suresh, Goeffry Thomas, Slava Tykhonov, Joaquin Vanschoren, Susheel Varma, Jos van der Velde, Steffen Vogler, Carole-Jean Wu, Luyao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data is a critical resource for machine learning (ML), yet working with data
remains a key friction point. This paper introduces Croissant, a metadata
format for datasets that creates a shared representation across ML tools,
frameworks, and platforms. Croissant makes datasets more discoverable,
portable, and interoperable, thereby addressing significant challenges in ML
data management. Croissant is already supported by several popular dataset
repositories, spanning hundreds of thousands of datasets, enabling easy loading
into the most commonly-used ML frameworks, regardless of where the data is
stored. Our initial evaluation by human raters shows that Croissant metadata is
readable, understandable, complete, yet concise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at the NeurIPS 2024 Datasets and Benchmark Track. A shorter
  version appeared earlier in Proceedings of ACM SIGMOD/PODS'24 Data Management
  for End-to-End Machine Learning (DEEM) Workshop
  https://dl.acm.org/doi/10.1145/3650203.3663326</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Graph Contrastive Learning with Reliable and Informative
  Augmentation for Recommendation <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05633v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05633v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Zheng, Junjie Zhang, Hongyu Lu, Yu Chen, Ming Chen, Wayne Xin Zhao, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural network(GNN) has been a powerful approach in collaborative
filtering(CF) due to its ability to model high-order user-item relationships.
Recently, to alleviate the data sparsity and enhance representation learning,
many efforts have been conducted to integrate contrastive learning(CL) with
GNNs. Despite the promising improvements, the contrastive view generation based
on structure and representation perturbations in existing methods potentially
disrupts the collaborative information in contrastive views, resulting in
limited effectiveness of positive alignment. To overcome this issue, we propose
CoGCL, a novel framework that aims to enhance graph contrastive learning by
constructing contrastive views with stronger collaborative information via
discrete codes. The core idea is to map users and items into discrete codes
rich in collaborative information for reliable and informative contrastive view
generation. To this end, we initially introduce a multi-level vector quantizer
in an end-to-end manner to quantize user and item representations into discrete
codes. Based on these discrete codes, we enhance the collaborative information
of contrastive views by considering neighborhood structure and semantic
relevance respectively. For neighborhood structure, we propose virtual neighbor
augmentation by treating discrete codes as virtual neighbors, which expands an
observed user-item interaction into multiple edges involving discrete codes.
Regarding semantic relevance, we identify similar users/items based on shared
discrete codes and interaction targets to generate the semantically relevant
view. Through these strategies, we construct contrastive views with stronger
collaborative information and develop a triple-view graph contrastive learning
approach. Extensive experiments on four public datasets demonstrate the
effectiveness of our proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing FKG.in: automating Indian food composition analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saransh Kumar Gupta, Lipika Dey, Partha Pratim Das, Geeta Trilok-Kumar, Ramesh Jain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to compute food composition data for
Indian recipes using a knowledge graph for Indian food (FKG.in) and LLMs. The
primary focus is to provide a broad overview of an automated food composition
analysis workflow and describe its core functionalities: nutrition data
aggregation, food composition analysis, and LLM-augmented information
resolution. This workflow aims to complement FKG.in and iteratively supplement
food composition data from verified knowledge bases. Additionally, this paper
highlights the challenges of representing Indian food and accessing food
composition data digitally. It also reviews three key sources of food
composition data: the Indian Food Composition Tables, the Indian Nutrient
Databank, and the Nutritionix API. Furthermore, it briefly outlines how users
can interact with the workflow to obtain diet-based health recommendations and
detailed food composition information for numerous recipes. We then explore the
complex challenges of analyzing Indian recipe information across dimensions
such as structure, multilingualism, and uncertainty as well as present our
ongoing work on LLM-based solutions to address these issues. The methods
proposed in this workshop paper for AI-driven knowledge curation and
information resolution are application-agnostic, generalizable, and replicable
for any domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures, 30 references, International Conference on
  Pattern Recognition 2024 - Multimedia Assisted Dietary Management Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ xRAG: Extreme Context Compression for Retrieval-augmented Generation
  with One Token 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13792v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13792v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Cheng, Xun Wang, Xingxing Zhang, Tao Ge, Si-Qing Chen, Furu Wei, Huishuai Zhang, Dongyan Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces xRAG, an innovative context compression method tailored
for retrieval-augmented generation. xRAG reinterprets document embeddings in
dense retrieval--traditionally used solely for retrieval--as features from the
retrieval modality. By employing a modality fusion methodology, xRAG seamlessly
integrates these embeddings into the language model representation space,
effectively eliminating the need for their textual counterparts and achieving
an extreme compression rate. In xRAG, the only trainable component is the
modality bridge, while both the retriever and the language model remain frozen.
This design choice allows for the reuse of offline-constructed document
embeddings and preserves the plug-and-play nature of retrieval augmentation.
Experimental results demonstrate that xRAG achieves an average improvement of
over 10% across six knowledge-intensive tasks, adaptable to various language
model backbones, ranging from a dense 7B model to an 8x7B Mixture of Experts
configuration. xRAG not only significantly outperforms previous context
compression methods but also matches the performance of uncompressed models on
several datasets, while reducing overall FLOPs by a factor of 3.53. Our work
pioneers new directions in retrieval-augmented generation from the perspective
of multimodality fusion, and we hope it lays the foundation for future
efficient and scalable retrieval-augmented systems
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Neurips 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniEvalKit: A Modular, Lightweight Toolbox for Evaluating Large
  Language Model and its Omni-Extensions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Kai Zhang, Xu-Xiang Zhong, Shiyin Lu, Qing-Guo Chen, De-Chuan Zhan, Han-Jia Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancements in Large Language Models (LLMs) have significantly
expanded their applications, ranging from multilingual support to
domain-specific tasks and multimodal integration. In this paper, we present
OmniEvalKit, a novel benchmarking toolbox designed to evaluate LLMs and their
omni-extensions across multilingual, multidomain, and multimodal capabilities.
Unlike existing benchmarks that often focus on a single aspect, OmniEvalKit
provides a modular, lightweight, and automated evaluation system. It is
structured with a modular architecture comprising a Static Builder and Dynamic
Data Flow, promoting the seamless integration of new models and datasets.
OmniEvalKit supports over 100 LLMs and 50 evaluation datasets, covering
comprehensive evaluations across thousands of model-dataset combinations.
OmniEvalKit is dedicated to creating an ultra-lightweight and fast-deployable
evaluation framework, making downstream applications more convenient and
versatile for the AI community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MuMu-LLaMA: Multi-modal Music Understanding and Generation via Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shansong Liu, Atin Sakkeer Hussain, Qilong Wu, Chenshuo Sun, Ying Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on large language models has advanced significantly across text,
speech, images, and videos. However, multi-modal music understanding and
generation remain underexplored due to the lack of well-annotated datasets. To
address this, we introduce a dataset with 167.69 hours of multi-modal data,
including text, images, videos, and music annotations. Based on this dataset,
we propose MuMu-LLaMA, a model that leverages pre-trained encoders for music,
images, and videos. For music generation, we integrate AudioLDM 2 and MusicGen.
Our evaluation across four tasks--music understanding, text-to-music
generation, prompt-based music editing, and multi-modal music
generation--demonstrates that MuMu-LLaMA outperforms state-of-the-art models,
showing its potential for multi-modal music applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI TrackMate: Finally, Someone Who Will Give Your Music More Than Just
  "Sounds Great!" <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Lin Jiang, Chia-Ho Hsiung, Yen-Tung Yeh, Lu-Rong Chen, Bo-Yu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of "bedroom producers" has democratized music creation, while
challenging producers to objectively evaluate their work. To address this, we
present AI TrackMate, an LLM-based music chatbot designed to provide
constructive feedback on music productions. By combining LLMs' inherent musical
knowledge with direct audio track analysis, AI TrackMate offers
production-specific insights, distinguishing it from text-only approaches. Our
framework integrates a Music Analysis Module, an LLM-Readable Music Report, and
Music Production-Oriented Feedback Instruction, creating a plug-and-play,
training-free system compatible with various LLMs and adaptable to future
advancements. We demonstrate AI TrackMate's capabilities through an interactive
web interface and present findings from a pilot study with a music producer. By
bridging AI capabilities with the needs of independent producers, AI TrackMate
offers on-demand analytical feedback, potentially supporting the creative
process and skill development in music production. This system addresses the
growing demand for objective self-assessment tools in the evolving landscape of
independent music production.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for the NeurIPS 2024 Creative AI Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Controllable Speech Synthesis in the Era of Large Language
  Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianxin Xie, Yan Rong, Pengfei Zhang, Li Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-speech (TTS), also known as speech synthesis, is a prominent research
area that aims to generate natural-sounding human speech from text. Recently,
with the increasing industrial demand, TTS technologies have evolved beyond
synthesizing human-like speech to enabling controllable speech generation. This
includes fine-grained control over various attributes of synthesized speech
such as emotion, prosody, timbre, and duration. Besides, advancements in deep
learning, such as diffusion and large language models, have significantly
enhanced controllable TTS over the past several years. In this paper, we
conduct a comprehensive survey of controllable TTS, covering approaches ranging
from basic control techniques to methods utilizing natural language prompts,
aiming to provide a clear understanding of the current state of research. We
examine the general controllable TTS pipeline, challenges, model architectures,
and control strategies, offering a comprehensive and clear taxonomy of existing
methods. Additionally, we provide a detailed summary of datasets and evaluation
metrics and shed some light on the applications and future directions of
controllable TTS. To the best of our knowledge, this survey paper provides the
first comprehensive review of emerging controllable TTS methods, which can
serve as a beneficial resource for both academic researchers and industry
practitioners.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A comprehensive survey on controllable TTS, 23 pages, 6 tables, 4
  figures, 280 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 4D Gaussian Splatting with Scale-aware Residual Field and Adaptive
  Optimization for Real-time Rendering of Temporally Complex Dynamic Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinbo Yan, Rui Peng, Luyang Tang, Ronggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing dynamic scenes from video sequences is a highly promising task
in the multimedia domain. While previous methods have made progress, they often
struggle with slow rendering and managing temporal complexities such as
significant motion and object appearance/disappearance. In this paper, we
propose SaRO-GS as a novel dynamic scene representation capable of achieving
real-time rendering while effectively handling temporal complexities in dynamic
scenes. To address the issue of slow rendering speed, we adopt a Gaussian
primitive-based representation and optimize the Gaussians in 4D space, which
facilitates real-time rendering with the assistance of 3D Gaussian Splatting.
Additionally, to handle temporally complex dynamic scenes, we introduce a
Scale-aware Residual Field. This field considers the size information of each
Gaussian primitive while encoding its residual feature and aligns with the
self-splitting behavior of Gaussian primitives. Furthermore, we propose an
Adaptive Optimization Schedule, which assigns different optimization strategies
to Gaussian primitives based on their distinct temporal properties, thereby
expediting the reconstruction of dynamic regions. Through evaluations on
monocular and multi-view datasets, our method has demonstrated state-of-the-art
performance. Please see our project page at
https://yjb6.github.io/SaRO-GS.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MSCrackMamba: Leveraging Vision Mamba for Crack Detection in Fused
  Multispectral Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinfeng Zhu, Yuan Fang, Lei Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crack detection is a critical task in structural health monitoring, aimed at
assessing the structural integrity of bridges, buildings, and roads to prevent
potential failures. Vision-based crack detection has become the mainstream
approach due to its ease of implementation and effectiveness. Fusing infrared
(IR) channels with red, green and blue (RGB) channels can enhance feature
representation and thus improve crack detection. However, IR and RGB channels
often differ in resolution. To align them, higher-resolution RGB images
typically need to be downsampled to match the IR image resolution, which leads
to the loss of fine details. Moreover, crack detection performance is
restricted by the limited receptive fields and high computational complexity of
traditional image segmentation networks. Inspired by the recently proposed
Mamba neural architecture, this study introduces a two-stage paradigm called
MSCrackMamba, which leverages Vision Mamba along with a super-resolution
network to address these challenges. Specifically, to align IR and RGB
channels, we first apply super-resolution to IR channels to match the
resolution of RGB channels for data fusion. Vision Mamba is then adopted as the
backbone network, while UperNet is employed as the decoder for crack detection.
Our approach is validated on the large-scale Crack Detection dataset Crack900,
demonstrating an improvement of 3.55% in mIoU compared to the best-performing
baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sound2Vision: Generating Diverse Visuals from Audio through Cross-Modal
  Latent Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kim Sung-Bin, Arda Senocak, Hyunwoo Ha, Tae-Hyun Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How does audio describe the world around us? In this work, we propose a
method for generating images of visual scenes from diverse in-the-wild sounds.
This cross-modal generation task is challenging due to the significant
information gap between auditory and visual signals. We address this challenge
by designing a model that aligns audio-visual modalities by enriching audio
features with visual information and translating them into the visual latent
space. These features are then fed into the pre-trained image generator to
produce images. To enhance image quality, we use sound source localization to
select audio-visual pairs with strong cross-modal correlations. Our method
achieves substantially better results on the VEGAS and VGGSound datasets
compared to previous work and demonstrates control over the generation process
through simple manipulations to the input waveform or latent space.
Furthermore, we analyze the geometric properties of the learned embedding space
and demonstrate that our learning approach effectively aligns audio-visual
signals for cross-modal generation. Based on this analysis, we show that our
method is agnostic to specific design choices, showing its generalizability by
integrating various model architectures and different types of audio-visual
data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under-review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pilot-guided Multimodal Semantic Communication for Audio-Visual Event
  Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Yu, Zhe Xiang, Nan Che, Zhuoran Zhang, Yuandi Li, Junxiao Xue, Zhiguo Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal semantic communication, which integrates various data modalities
such as text, images, and audio, significantly enhances communication
efficiency and reliability. It has broad application prospects in fields such
as artificial intelligence, autonomous driving, and smart homes. However,
current research primarily relies on analog channels and assumes constant
channel states (perfect CSI), which is inadequate for addressing dynamic
physical channels and noise in real-world scenarios. Existing methods often
focus on single modality tasks and fail to handle multimodal stream data, such
as video and audio, and their corresponding tasks. Furthermore, current
semantic encoding and decoding modules mainly transmit single modality
features, neglecting the need for multimodal semantic enhancement and
recognition tasks.
  To address these challenges, this paper proposes a pilot-guided framework for
multimodal semantic communication specifically tailored for audio-visual event
localization tasks. This framework utilizes digital pilot codes and channel
modules to guide the state of analog channels in real-wold scenarios and
designs Euler-based multimodal semantic encoding and decoding that consider
time-frequency characteristics based on dynamic channel state. This approach
effectively handles multimodal stream source data, especially for audio-visual
event localization tasks. Extensive numerical experiments demonstrate the
robustness of the proposed framework in channel changes and its support for
various communication scenarios. The experimental results show that the
framework outperforms existing benchmark methods in terms of Signal-to-Noise
Ratio (SNR), highlighting its advantage in semantic communication quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ M$^{2}$UGen: Multi-modal Music Understanding and Generation with the
  Power of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11255v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11255v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shansong Liu, Atin Sakkeer Hussain, Qilong Wu, Chenshuo Sun, Ying Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current landscape of research leveraging large language models (LLMs) is
experiencing a surge. Many works harness the powerful reasoning capabilities of
these models to comprehend various modalities, such as text, speech, images,
videos, etc. They also utilize LLMs to understand human intention and generate
desired outputs like images, videos, and music. However, research that combines
both understanding and generation using LLMs is still limited and in its
nascent stage. To address this gap, we introduce a Multi-modal Music
Understanding and Generation (M$^{2}$UGen) framework that integrates LLM's
abilities to comprehend and generate music for different modalities. The
M$^{2}$UGen framework is purpose-built to unlock creative potential from
diverse sources of inspiration, encompassing music, image, and video through
the use of pretrained MERT, ViT, and ViViT models, respectively. To enable
music generation, we explore the use of AudioLDM 2 and MusicGen. Bridging
multi-modal understanding and music generation is accomplished through the
integration of the LLaMA 2 model. Furthermore, we make use of the MU-LLaMA
model to generate extensive datasets that support text/image/video-to-music
generation, facilitating the training of our M$^{2}$UGen framework. We conduct
a thorough evaluation of our proposed framework. The experimental results
demonstrate that our model achieves or surpasses the performance of the current
state-of-the-art models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StableMoFusion: Towards Robust and Efficient Diffusion-based Motion
  Generation Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05691v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05691v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiheng Huang, Hui Yang, Chuanchen Luo, Yuxi Wang, Shibiao Xu, Zhaoxiang Zhang, Man Zhang, Junran Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thanks to the powerful generative capacity of diffusion models, recent years
have witnessed rapid progress in human motion generation. Existing
diffusion-based methods employ disparate network architectures and training
strategies. The effect of the design of each component is still unclear. In
addition, the iterative denoising process consumes considerable computational
overhead, which is prohibitive for real-time scenarios such as virtual
characters and humanoid robots. For this reason, we first conduct a
comprehensive investigation into network architectures, training strategies,
and inference processs. Based on the profound analysis, we tailor each
component for efficient high-quality human motion generation. Despite the
promising performance, the tailored model still suffers from foot skating which
is an ubiquitous issue in diffusion-based solutions. To eliminate footskate, we
identify foot-ground contact and correct foot motions along the denoising
process. By organically combining these well-designed components together, we
present StableMoFusion, a robust and efficient framework for human motion
generation. Extensive experimental results show that our StableMoFusion
performs favorably against current state-of-the-art methods. Project page:
https://h-y1heng.github.io/StableMoFusion-page/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Emotion Analysis in Short-form Videos: A Large-Scale <span class="highlight-title">Dataset</span> and
  Baseline 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17335v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17335v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuecheng Wu, Heli Sun, Junxiao Xue, Jiayu Nie, Xiangyan Kong, Ruofan Zhai, Liang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, short-form videos (SVs) are essential to web information
acquisition and sharing in our daily life. The prevailing use of SVs to spread
emotions leads to the necessity of conducting video emotion analysis (VEA)
towards SVs. Considering the lack of SVs emotion data, we introduce a
large-scale dataset named eMotions, comprising 27,996 videos. Meanwhile, we
alleviate the impact of subjectivities on labeling quality by emphasizing
better personnel allocations and multi-stage annotations. In addition, we
provide the category-balanced and test-oriented variants through targeted data
sampling. Some commonly used videos, such as facial expressions, have been well
studied. However, it is still challenging to analysis the emotions in SVs.
Since the broader content diversity brings more distinct semantic gaps and
difficulties in learning emotion-related features, and there exists local
biases and collective information gaps caused by the emotion inconsistence
under the prevalently audio-visual co-expressions. To tackle these challenges,
we present an end-to-end audio-visual baseline AV-CANet which employs the video
transformer to better learn semantically relevant representations. We further
design the Local-Global Fusion Module to progressively capture the correlations
of audio-visual features. The EP-CE Loss is then introduced to guide model
optimization. Extensive experimental results on seven datasets demonstrate the
effectiveness of AV-CANet, while providing broad insights for future works.
Besides, we investigate the key components of AV-CANet by ablation studies.
Datasets and code will be fully open soon.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-08T00:00:00Z">2024-12-08</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">9</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixture-of-PageRanks: Replacing Long-Context with Real-Time, Sparse
  GraphRAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Alonso, Beren Millidge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances have extended the context window of frontier LLMs
dramatically, from a few thousand tokens up to millions, enabling entire books
and codebases to fit into context. However, the compute costs of inferencing
long-context LLMs are massive and often prohibitive in practice. RAG offers an
efficient and effective alternative: retrieve and process only the subset of
the context most important for the current task. Although promising, recent
work applying RAG to long-context tasks has two core limitations: 1) there has
been little focus on making the RAG pipeline compute efficient, and 2) such
works only test on simple QA tasks, and their performance on more challenging
tasks is unclear. To address this, we develop an algorithm based on PageRank, a
graph-based retrieval algorithm, which we call mixture-of-PageRanks (MixPR).
MixPR uses a mixture of PageRank-based graph-retrieval algorithms implemented
using sparse matrices for efficent, cheap retrieval that can deal with a
variety of complex tasks. Our MixPR retriever achieves state-of-the-art results
across a wide range of long-context benchmark tasks, outperforming both
existing RAG methods, specialized retrieval architectures, and long-context
LLMs despite being far more compute efficient. Due to using sparse embeddings,
our retriever is extremely compute efficient, capable of embedding and
retrieving millions of tokens within a few seconds and runs entirely on CPU.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fuzzy Norm-Explicit Product Quantization for Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadreza Jamalifard, Javier Andreu-Perez, Hani Hagras, Luis Martínez López
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the data resources grow, providing recommendations that best meet the
demands has become a vital requirement in business and life to overcome the
information overload problem. However, building a system suggesting relevant
recommendations has always been a point of debate. One of the most
cost-efficient techniques in terms of producing relevant recommendations at a
low complexity is Product Quantization (PQ). PQ approaches have continued
developing in recent years. This system's crucial challenge is improving
product quantization performance in terms of recall measures without
compromising its complexity. This makes the algorithm suitable for problems
that require a greater number of potentially relevant items without
disregarding others, at high-speed and low-cost to keep up with traffic. This
is the case of online shops where the recommendations for the purpose are
important, although customers can be susceptible to scoping other products.
This research proposes a fuzzy approach to perform norm-based product
quantization. Type-2 Fuzzy sets (T2FSs) define the codebook allowing
sub-vectors (T2FSs) to be associated with more than one element of the
codebook, and next, its norm calculus is resolved by means of integration. Our
method finesses the recall measure up, making the algorithm suitable for
problems that require querying at most possible potential relevant items
without disregarding others. The proposed method outperforms all PQ approaches
such as NEQ, PQ, and RQ up to +6%, +5%, and +8% by achieving a recall of 94%,
69%, 59% in Netflix, Audio, Cifar60k datasets, respectively. More and over,
computing time and complexity nearly equals the most computationally efficient
existing PQ method in the state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 1-800-SHARED-TASKS at RegNLP: Lexical Reranking of Semantic Retrieval
  (LeSeR) for Regulatory Question Answering <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jebish Purbey, Drishti Sharma, Siddhant Gupta, Khawaja Murad, Siddartha Pullakhandam, Ram Mohan Rao Kadiyala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the system description of our entry for the COLING 2025
RegNLP RIRAG (Regulatory Information Retrieval and Answer Generation)
challenge, focusing on leveraging advanced information retrieval and answer
generation techniques in regulatory domains. We experimented with a combination
of embedding models, including Stella, BGE, CDE, and Mpnet, and leveraged
fine-tuning and reranking for retrieving relevant documents in top ranks. We
utilized a novel approach, LeSeR, which achieved competitive results with a
recall@10 of 0.8201 and map@10 of 0.6655 for retrievals. This work highlights
the transformative potential of natural language processing techniques in
regulatory applications, offering insights into their capabilities for
implementing a retrieval augmented generation system while identifying areas
for future improvement in robustness and domain adaptation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, Accepted to RegNLP @ COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Manufacturing Scale-Up from Material Discovery Using
  Agentic Web Navigation and Retrieval-Augmented AI for Process Engineering
  Schematics Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sakhinana Sagar Srinivas, Akash Das, Shivam Gupta, Venkataramana Runkana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Process Flow Diagrams (PFDs) and Process and Instrumentation Diagrams (PIDs)
are critical tools for industrial process design, control, and safety. However,
the generation of precise and regulation-compliant diagrams remains a
significant challenge, particularly in scaling breakthroughs from material
discovery to industrial production in an era of automation and digitalization.
This paper introduces an autonomous agentic framework to address these
challenges through a twostage approach involving knowledge acquisition and
generation. The framework integrates specialized sub-agents for retrieving and
synthesizing multimodal data from publicly available online sources and
constructs ontological knowledge graphs using a Graph Retrieval-Augmented
Generation (Graph RAG) paradigm. These capabilities enable the automation of
diagram generation and open-domain question answering (ODQA) tasks with high
contextual accuracy. Extensive empirical experiments demonstrate the frameworks
ability to deliver regulation-compliant diagrams with minimal expert
intervention, highlighting its practical utility for industrial applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Cluster Representatives for Approximate Nearest Neighbor Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Vecchiato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing increasingly efficient and accurate algorithms for approximate
nearest neighbor search is a paramount goal in modern information retrieval. A
primary approach to addressing this question is clustering, which involves
partitioning the dataset into distinct groups, with each group characterized by
a representative data point. By this method, retrieving the top-k data points
for a query requires identifying the most relevant clusters based on their
representatives -- a routing step -- and then conducting a nearest neighbor
search within these clusters only, drastically reducing the search space.
  The objective of this thesis is not only to provide a comprehensive
explanation of clustering-based approximate nearest neighbor search but also to
introduce and delve into every aspect of our novel state-of-the-art method,
which originated from a natural observation: The routing function solves a
ranking problem, making the function amenable to learning-to-rank. The
development of this intuition and applying it to maximum inner product search
has led us to demonstrate that learning cluster representatives using a simple
linear function significantly boosts the accuracy of clustering-based
approximate nearest neighbor search.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Extraction and Creation of FBS Design Reasoning Knowledge
  Graphs from Structured Data in Product Catalogues Lacking Contextual
  Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vijayalaxmi Sahadevan, Sushil Mario, Yash Jaiswal, Divyanshu Bajpai, Vishal Singh, Hiralal Aggarwal, Suhas Suresh, Manjunath Maigur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ontology-based knowledge graphs (KG) are desirable for effective knowledge
management and reuse in various decision making scenarios, including design.
Creating and populating extensive KG based on specific ontological models can
be highly labour and time-intensive unless automated processes are developed
for knowledge extraction and graph creation. Most research and development on
automated extraction and creation of KG is based on extensive unstructured data
sets that provide contextual information. However, some of the most useful
information about the products and services of a company has traditionally been
recorded as structured data. Such structured data sets rarely follow a standard
ontology, do not capture explicit mapping of relationships between the
entities, and provide no contextual information. Therefore, this research
reports a method and digital workflow developed to address this gap. The
developed method and workflow employ rule-based techniques to extract and
create a Function Behaviour-Structure (FBS) ontology-based KG from legacy
structured data, especially specification sheets and product catalogues. The
solution approach consists of two main components: a process for deriving
context and context-based classification rules for FBS ontology concepts and a
workflow for populating and retrieving the FBS ontology-based KG. KG and
Natural Language Processing (NLP) are used to automate knowledge extraction,
representation, and retrieval. The workflow's effectiveness is demonstrated via
pilot implementation in an industrial context. Insights gained from the pilot
study are reported regarding the challenges and opportunities, including
discussing the FBS ontology and concepts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, with 17 figures and 10 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Model Powered Digital Biology with BRAD 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02864v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02864v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Pickard, Ram Prakash, Marc Andrew Choi, Natalie Oliven, Cooper Stansbury, Jillian Cwycyshyn, Alex Gorodetsky, Alvaro Velasquez, Indika Rajapakse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) are transforming biology,
computer science, engineering, and every day life. However, integrating the
wide array of computational tools, databases, and scientific literature
continues to pose a challenge to biological research. LLMs are well-suited for
unstructured integration, efficient information retrieval, and automating
standard workflows and actions from these diverse resources. To harness these
capabilities in bioinformatics, we present a prototype Bioinformatics Retrieval
Augmented Digital assistant (BRAD). BRAD is a chatbot and agentic system that
integrates a variety of bioinformatics tools. The Python package implements an
AI \texttt{Agent} that is powered by LLMs and connects to a local file system,
online databases, and a user's software. The \texttt{Agent} is highly
configurable, enabling tasks such as Retrieval-Augmented Generation, searches
across bioinformatics databases, and the execution of software pipelines.
BRAD's coordinated integration of bioinformatics tools delivers a context-aware
and semi-autonomous system that extends beyond the capabilities of conventional
LLM-based chatbots. A graphical user interface (GUI) provides an intuitive
interface to the system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures, 1 table. See: https://github.com/Jpickard1/BRAD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CPRM: A LLM-based Continual <span class="highlight-title">Pre-train</span>ing Framework for Relevance
  Modeling in Commercial Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01269v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01269v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaixin Wu, Yixin Ji, Zeyuan Chen, Qiang Wang, Cunxiang Wang, Hong Liu, Baijun Ji, Jia Xu, Zhongyi Liu, Jinjie Gu, Yuan Zhou, Linjian Mo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relevance modeling between queries and items stands as a pivotal component in
commercial search engines, directly affecting the user experience. Given the
remarkable achievements of large language models (LLMs) in various natural
language processing (NLP) tasks, LLM-based relevance modeling is gradually
being adopted within industrial search systems. Nevertheless, foundational LLMs
lack domain-specific knowledge and do not fully exploit the potential of
in-context learning. Furthermore, structured item text remains underutilized,
and there is a shortage in the supply of corresponding queries and background
knowledge. We thereby propose CPRM (Continual Pre-training for Relevance
Modeling), a framework designed for the continual pre-training of LLMs to
address these issues. Our CPRM framework includes three modules: 1) employing
both queries and multi-field item to jointly pre-train for enhancing domain
knowledge, 2) applying in-context pre-training, a novel approach where LLMs are
pre-trained on a sequence of related queries or items, and 3) conducting
reading comprehension on items to produce associated domain knowledge and
background information (e.g., generating summaries and corresponding queries)
to further strengthen LLMs. Results on offline experiments and online A/B
testing demonstrate that our model achieves convincing performance compared to
strong baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FairSort: Learning to Fair Rank for Personalized Recommendations in
  Two-Sided Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00424v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00424v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoli Wu, Zhiyong Feng, Shizhan Chen, Hongyue Wu, Xiao Xue, Jianmao Xiao, Guodong Fan, Hongqi Chen, Jingyu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional recommendation systems focus on maximizing user satisfaction by
suggesting their favourite items. This user-centric approach may lead to unfair
exposure distribution among the providers. On the contrary, a provider-centric
design might become unfair to the users. Therefore, this paper proposes a
re-ranking model FairSort to find a trade-off solution among user-side
fairness, provider-side fairness, and personalized recommendations utility.
Previous works habitually treat this issue as a knapsack problem, incorporating
both-side fairness as constraints.
  In this paper, we adopt a novel perspective, treating each recommendation
list as a runway rather than a knapsack. In this perspective, each item on the
runway gains a velocity and runs within a specific time, achieving re-ranking
for both-side fairness. Meanwhile, we ensure the Minimum Utility Guarantee for
personalized recommendations by designing a Binary Search approach. This can
provide more reliable recommendations compared to the conventional greedy
strategy based on the knapsack problem. We further broaden the applicability of
FairSort, designing two versions for online and offline recommendation
scenarios. Theoretical analysis and extensive experiments on real-world
datasets indicate that FairSort can ensure more reliable personalized
recommendations while considering fairness for both the provider and user.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M6: Multi-generator, Multi-domain, Multi-lingual and cultural,
  Multi-genres, Multi-instrument Machine-Generated Music Detection Databases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yupei Li, Hanqian Li, Lucia Specia, Björn W. Schuller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine-generated music (MGM) has emerged as a powerful tool with
applications in music therapy, personalised editing, and creative inspiration
for the music community. However, its unregulated use threatens the
entertainment, education, and arts sectors by diminishing the value of
high-quality human compositions. Detecting machine-generated music (MGMD) is,
therefore, critical to safeguarding these domains, yet the field lacks
comprehensive datasets to support meaningful progress. To address this gap, we
introduce \textbf{M6}, a large-scale benchmark dataset tailored for MGMD
research. M6 is distinguished by its diversity, encompassing multiple
generators, domains, languages, cultural contexts, genres, and instruments. We
outline our methodology for data selection and collection, accompanied by
detailed data analysis, providing all WAV form of music. Additionally, we
provide baseline performance scores using foundational binary classification
models, illustrating the complexity of MGMD and the significant room for
improvement. By offering a robust and multifaceted resource, we aim to empower
future research to develop more effective detection methods for MGM. We believe
M6 will serve as a critical step toward addressing this societal challenge. The
dataset and code will be freely available to support open collaboration and
innovation in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Supervised Contrastive Learning for Controllable Video-to-Music
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanti Stewart, Gouthaman KV, Lie Lu, Andrea Fanelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Content creators often use music to enhance their videos, from soundtracks in
movies to background music in video blogs and social media content. However,
identifying the best music for a video can be a difficult and time-consuming
task. To address this challenge, we propose a novel framework for automatically
retrieving a matching music clip for a given video, and vice versa. Our
approach leverages annotated music labels, as well as the inherent artistic
correspondence between visual and music elements. Distinct from previous
cross-modal music retrieval works, our method combines both self-supervised and
supervised training objectives. We use self-supervised and label-supervised
contrastive learning to train a joint embedding space between music and video.
We show the effectiveness of our approach by using music genre labels for the
supervised training component, and our framework can be generalized to other
music annotations (e.g., emotion, instrument, etc.). Furthermore, our method
enables fine-grained control over how much the retrieval process focuses on
self-supervised vs. label information at inference time. We evaluate the
learned embeddings through a variety of video-to-music and music-to-video
retrieval tasks. Our experiments show that the proposed approach successfully
combines self-supervised and supervised objectives and is effective for
controllable music-video retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages + 1 reference page, 2 figures, 2 tables. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SILMM: Self-Improving Large Multimodal Models for Compositional
  Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05818v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05818v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leigang Qu, Haochuan Li, Wenjie Wang, Xiang Liu, Juncheng Li, Liqiang Nie, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Multimodal Models (LMMs) have demonstrated impressive capabilities in
multimodal understanding and generation, pushing forward advancements in
text-to-image generation. However, achieving accurate text-image alignment for
LMMs, particularly in compositional scenarios, remains challenging. Existing
approaches, such as layout planning for multi-step generation and learning from
human feedback or AI feedback, depend heavily on prompt engineering, costly
human annotations, and continual upgrading, limiting flexibility and
scalability. In this work, we introduce a model-agnostic iterative
self-improvement framework (SILMM) that can enable LMMs to provide helpful and
scalable self-feedback and optimize text-image alignment via Direct Preference
Optimization (DPO). DPO can readily applied to LMMs that use discrete visual
tokens as intermediate image representations; while it is less suitable for
LMMs with continuous visual features, as obtaining generation probabilities is
challenging. To adapt SILMM to LMMs with continuous features, we propose a
diversity mechanism to obtain diverse representations and a kernel-based
continuous DPO for alignment. Extensive experiments on three compositional
text-to-image generation benchmarks validate the effectiveness and superiority
of SILMM, showing improvements exceeding 30% on T2I-CompBench++ and around 20%
on DPG-Bench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://silmm.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SizeGS: Size-aware Compression of 3D Gaussians with Hierarchical Mixed
  Precision Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuzhao Xie, Jiahang Liu, Weixiang Zhang, Shijia Ge, Sicheng Pan, Chen Tang, Yunpeng Bai, Zhi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective compression technology is crucial for 3DGS to adapt to varying
storage and transmission conditions. However, existing methods fail to address
size constraints while maintaining optimal quality. In this paper, we introduce
SizeGS, a framework that compresses 3DGS within a specified size budget while
optimizing visual quality. We start with a size estimator to establish a clear
relationship between file size and hyperparameters. Leveraging this estimator,
we incorporate mixed precision quantization (MPQ) into 3DGS attributes,
structuring MPQ in two hierarchical level -- inter-attribute and
intra-attribute -- to optimize visual quality under the size constraint. At the
inter-attribute level, we assign bit-widths to each attribute channel by
formulating the combinatorial optimization as a 0-1 integer linear program,
which can be efficiently solved. At the intra-attribute level, we divide each
attribute channel into blocks of vectors, quantizing each vector based on the
optimal bit-width derived at the inter-attribute level. Dynamic programming
determines block lengths. Using the size estimator and MPQ, we develop a
calibrated algorithm to identify optimal hyperparameters in just 10 minutes,
achieving a 1.69$\times$ efficiency increase with quality comparable to
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Automatically compressing 3DGS into the desired file size while
  maximizing the visual quality</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emotion-Aligned Contrastive Learning Between Images and Music <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12610v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12610v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanti Stewart, Kleanthis Avramidis, Tiantian Feng, Shrikanth Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional music search engines rely on retrieval methods that match natural
language queries with music metadata. There have been increasing efforts to
expand retrieval methods to consider the audio characteristics of music itself,
using queries of various modalities including text, video, and speech. While
most approaches aim to match general music semantics to the input queries, only
a few focus on affective qualities. In this work, we address the task of
retrieving emotionally-relevant music from image queries by learning an
affective alignment between images and music audio. Our approach focuses on
learning an emotion-aligned joint embedding space between images and music.
This embedding space is learned via emotion-supervised contrastive learning,
using an adapted cross-modal version of the SupCon loss. We evaluate the joint
embeddings through cross-modal retrieval tasks (image-to-music and
music-to-image) based on emotion labels. Furthermore, we investigate the
generalizability of the learned music embeddings via automatic music tagging.
Our experiments show that the proposed approach successfully aligns images and
music, and that the learned embedding space is effective for cross-modal
retrieval applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICASSP 2024. Code:
  https://github.com/shantistewart/Emo-CLIM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI-Driven Virtual Teacher for Enhanced Educational Efficiency:
  Leveraging Large <span class="highlight-title">Pretrain</span> Models for Autonomous Error Analysis and Correction <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09403v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09403v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianlong Xu, Yi-Fan Zhang, Zhendong Chu, Shen Wang, Qingsong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Students frequently make mistakes while solving mathematical problems, and
traditional error correction methods are both time-consuming and
labor-intensive. This paper introduces an innovative \textbf{V}irtual
\textbf{A}I \textbf{T}eacher system designed to autonomously analyze and
correct student \textbf{E}rrors (VATE). Leveraging advanced large language
models (LLMs), the system uses student drafts as a primary source for error
analysis, which enhances understanding of the student's learning process. It
incorporates sophisticated prompt engineering and maintains an error pool to
reduce computational overhead. The AI-driven system also features a real-time
dialogue component for efficient student interaction. Our approach demonstrates
significant advantages over traditional and machine learning-based error
correction methods, including reduced educational costs, high scalability, and
superior generalizability. The system has been deployed on the Squirrel AI
learning platform for elementary mathematics education, where it achieves
78.3\% accuracy in error analysis and shows a marked improvement in student
learning efficiency. Satisfaction surveys indicate a strong positive reception,
highlighting the system's potential to transform educational practices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI/IAAI 2025 Innovative Application Award</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-07T00:00:00Z">2024-12-07</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>Refine: Enhancing Few-Shot Performance on Low-Resource Indic
  Languages with Example Selection from Related Example Banks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumya Suvra Ghosal, Soumyabrata Pal, Koyel Mukherjee, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently demonstrated impressive few-shot
learning capabilities through in-context learning (ICL). However, ICL
performance is highly dependent on the choice of few-shot demonstrations,
making the selection of the most optimal examples a persistent research
challenge. This issue is further amplified in low-resource Indic languages,
where the scarcity of ground-truth data complicates the selection process. In
this work, we propose PromptRefine, a novel Alternating Minimization approach
for example selection that improves ICL performance on low-resource Indic
languages. PromptRefine leverages auxiliary example banks from related
high-resource Indic languages and employs multi-task learning techniques to
align language-specific retrievers, enabling effective cross-language
retrieval. Additionally, we incorporate diversity in the selected examples to
enhance generalization and reduce bias. Through comprehensive evaluations on
four text generation tasks -- Cross-Lingual Question Answering, Multilingual
Question Answering, Machine Translation, and Cross-Lingual Summarization using
state-of-the-art LLMs such as LLAMA-3.1-8B, LLAMA-2-7B, Qwen-2-7B, and
Qwen-2.5-7B, we demonstrate that PromptRefine significantly outperforms
existing frameworks for retrieving examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the effective transfer of knowledge from English to Hindi Wikipedia <span class="chip">COLING</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paramita Das, Amartya Roy, Ritabrata Chakraborty, Animesh Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Wikipedia is the largest multilingual encyclopedia, it remains
inherently incomplete. There is a significant disparity in the quality of
content between high-resource languages (HRLs, e.g., English) and low-resource
languages (LRLs, e.g., Hindi), with many LRL articles lacking adequate
information. To bridge these content gaps, we propose a lightweight framework
to enhance knowledge equity between English and Hindi. In case the English
Wikipedia page is not up-to-date, our framework extracts relevant information
from external resources readily available (such as English books) and adapts it
to align with Wikipedia's distinctive style, including its \textit{neutral
point of view} (NPOV) policy, using in-context learning capabilities of large
language models. The adapted content is then machine-translated into Hindi for
integration into the corresponding Wikipedia articles. On the other hand, if
the English version is comprehensive and up-to-date, the framework directly
transfers knowledge from English to Hindi. Our framework effectively generates
new content for Hindi Wikipedia sections, enhancing Hindi Wikipedia articles
respectively by 65% and 62% according to automatic and human judgment-based
evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at COLING Industry Track 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KG-Retriever: Efficient Knowledge Indexing for Retrieval-Augmented Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijie Chen, Ting Bai, Jinbo Su, Jian Luan, Wei Liu, Chuan Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models with retrieval-augmented generation encounter a pivotal
challenge in intricate retrieval tasks, e.g., multi-hop question answering,
which requires the model to navigate across multiple documents and generate
comprehensive responses based on fragmented information. To tackle this
challenge, we introduce a novel Knowledge Graph-based RAG framework with a
hierarchical knowledge retriever, termed KG-Retriever. The retrieval indexing
in KG-Retriever is constructed on a hierarchical index graph that consists of a
knowledge graph layer and a collaborative document layer. The associative
nature of graph structures is fully utilized to strengthen intra-document and
inter-document connectivity, thereby fundamentally alleviating the information
fragmentation problem and meanwhile improving the retrieval efficiency in
cross-document retrieval of LLMs. With the coarse-grained collaborative
information from neighboring documents and concise information from the
knowledge graph, KG-Retriever achieves marked improvements on five public QA
datasets, showing the effectiveness and efficiency of our proposed RAG
framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ULMRec: User-centric Large Language Model for Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minglai Shao, Hua Huang, Qiyao Peng, Hongtao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have demonstrated promising
performance in sequential recommendation tasks, leveraging their superior
language understanding capabilities. However, existing LLM-based recommendation
approaches predominantly focus on modeling item-level co-occurrence patterns
while failing to adequately capture user-level personalized preferences. This
is problematic since even users who display similar behavioral patterns (e.g.,
clicking or purchasing similar items) may have fundamentally different
underlying interests. To alleviate this problem, in this paper, we propose
ULMRec, a framework that effectively integrates user personalized preferences
into LLMs for sequential recommendation. Considering there has the semantic gap
between item IDs and LLMs, we replace item IDs with their corresponding titles
in user historical behaviors, enabling the model to capture the item semantics.
For integrating the user personalized preference, we design two key components:
(1) user indexing: a personalized user indexing mechanism that leverages vector
quantization on user reviews and user IDs to generate meaningful and unique
user representations, and (2) alignment tuning: an alignment-based tuning stage
that employs comprehensive preference alignment tasks to enhance the model's
capability in capturing personalized information. Through this design, ULMRec
achieves deep integration of language semantics with user personalized
preferences, facilitating effective adaptation to recommendation. Extensive
experiments on two public datasets demonstrate that ULMRec significantly
outperforms existing methods, validating the effectiveness of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Impact of User-Level Explanation Properties on Explanation Goals in
  Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14379v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14379v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André Levi Zanon, Marcelo Garcia Manzato, Leonardo Rocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explanations are crucial for improving users' transparency, persuasiveness,
engagement, and trust in Recommender Systems (RSs) by connecting interacted
items to recommended items based on shared attributes. However, evaluating the
effectiveness of explanation algorithms regarding those goals offline remains
challenging due to their subjectiveness. This paper investigates the impact of
user-level explanation properties, such as diversity and popularity of
attributes, on the user perception of explanation goals. In an offline setting,
we used metrics adapted from ranking to evaluate the characteristics of
explanations generated by three state-of-the-art post-hoc explanation
algorithms, based on the items and properties used to form the explanation
sentence, across six recommendation systems. We compared the offline metrics
results with those of an online user study. The findings highlight a trade-off
between the goals of transparency and trust, which are related to popular
properties, and the goals of engagement and persuasiveness, which are
associated with the diversification of properties displayed to users.
Furthermore, the study contributes to developing more robust evaluation methods
for explanation algorithms in RSs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Big data searching using words 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15346v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15346v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Santanu Acharjee, Ripunjoy Choudhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Big data analytics is one of the most promising areas of new research and
development in computer science, enterprises, e-commerce, and defense. For many
organizations, big data is regarded as one of their most important strategic
assets. This explosive growth has made it necessary to develop effective
techniques for examining and analyzing big data from a mathematical
perspective. Among various methods of analyzing big data, topological data
analysis (TDA) is now considered one of the useful tools. However, there is no
fundamental concept related to topological structure in big data. In this
paper, we introduce some fundamental ideas related to the neighborhood
structure of words in data searching, which can be extended to form important
topological structures of big data in the future. Additionally, we introduce
big data primal in big data searching and discuss the application of
neighborhood structures in detecting anomalies in data searching using the
Jaccard similarity coefficient.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combining Genre Classification and Harmonic-Percussive Features with
  Diffusion Models for Music-Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Pina, Yongmin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a novel method for generating music visualisers using
diffusion models, combining audio input with user-selected artwork. The process
involves two main stages: image generation and video creation. First, music
captioning and genre classification are performed, followed by the retrieval of
artistic style descriptions. A diffusion model then generates images based on
the user's input image and the derived artistic style descriptions. The video
generation stage utilises the same diffusion model to interpolate frames,
controlled by audio energy vectors derived from key musical features of
harmonics and percussives. The method demonstrates promising results across
various genres, and a new metric, Audio-Visual Synchrony (AVS), is introduced
to quantitatively evaluate the synchronisation between visual and audio
elements. Comparative analysis shows significantly higher AVS values for videos
generated using the proposed method with audio energy vectors, compared to
linear interpolation. This approach has potential applications in diverse
fields, including independent music video creation, film production, live music
events, and enhancing audio-visual experiences in public spaces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Li, Jiusong Luo, Wanjun Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech emotion recognition (SER) remains a challenging yet crucial task due
to the inherent complexity and diversity of human emotions. To address this
problem, researchers attempt to fuse information from other modalities via
multimodal learning. However, existing multimodal fusion techniques often
overlook the intricacies of cross-modal interactions, resulting in suboptimal
feature representations. In this paper, we propose WavFusion, a multimodal
speech emotion recognition framework that addresses critical research problems
in effective multimodal fusion, heterogeneity among modalities, and
discriminative representation learning. By leveraging a gated cross-modal
attention mechanism and multimodal homogeneous feature discrepancy learning,
WavFusion demonstrates improved performance over existing state-of-the-art
methods on benchmark datasets. Our work highlights the importance of capturing
nuanced cross-modal interactions and learning discriminative representations
for accurate multimodal SER. Experimental results on two benchmark datasets
(IEMOCAP and MELD) demonstrate that WavFusion succeeds over the
state-of-the-art strategies on emotion recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 31st International Conference on MultiMedia Modeling
  (MMM2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Securing Social Media Against Deepfakes using Identity, Behavioral, and
  Geometric Signatures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Umar Farooq, Awais Khan, Ijaz Ul Haq, Khalid Mahmood Malik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trust in social media is a growing concern due to its ability to influence
significant societal changes. However, this space is increasingly compromised
by various types of deepfake multimedia, which undermine the authenticity of
shared content. Although substantial efforts have been made to address the
challenge of deepfake content, existing detection techniques face a major
limitation in generalization: they tend to perform well only on specific types
of deepfakes they were trained on.This dependency on recognizing specific
deepfake artifacts makes current methods vulnerable when applied to unseen or
varied deepfakes, thereby compromising their performance in real-world
applications such as social media platforms. To address the generalizability of
deepfake detection, there is a need for a holistic approach that can capture a
broader range of facial attributes and manipulations beyond isolated artifacts.
To address this, we propose a novel deepfake detection framework featuring an
effective feature descriptor that integrates Deep identity, Behavioral, and
Geometric (DBaG) signatures, along with a classifier named DBaGNet.
Specifically, the DBaGNet classifier utilizes the extracted DBaG signatures,
leveraging a triplet loss objective to enhance generalized representation
learning for improved classification. Specifically, the DBaGNet classifier
utilizes the extracted DBaG signatures and applies a triplet loss objective to
enhance generalized representation learning for improved classification. To
test the effectiveness and generalizability of our proposed approach, we
conduct extensive experiments using six benchmark deepfake datasets: WLDR,
CelebDF, DFDC, FaceForensics++, DFD, and NVFAIR. Specifically, to ensure the
effectiveness of our approach, we perform cross-dataset evaluations, and the
results demonstrate significant performance gains over several state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Fusion Balancing Through Game-Theoretic Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07335v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07335v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantinos Kontras, Thomas Strypsteen, Christos Chatzichristos, Paul Pu Liang, Matthew Blaschko, Maarten De Vos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal learning can complete the picture of information extraction by
uncovering key dependencies between data sources. However, current systems fail
to fully leverage multiple modalities for optimal performance. This has been
attributed to modality competition, where modalities strive for training
resources, leaving some underoptimized. We show that current balancing methods
struggle to train multimodal models that surpass even simple baselines, such as
ensembles. This raises the question: how can we ensure that all modalities in
multimodal training are sufficiently trained, and that learning from new
modalities consistently improves performance? This paper proposes the
Multimodal Competition Regularizer (MCR), a new loss component inspired by
mutual information (MI) decomposition designed to prevent the adverse effects
of competition in multimodal training. Our key contributions are: 1)
Introducing game-theoretic principles in multimodal learning, where each
modality acts as a player competing to maximize its influence on the final
outcome, enabling automatic balancing of the MI terms. 2) Refining lower and
upper bounds for each MI term to enhance the extraction of task-relevant unique
and shared information across modalities. 3) Suggesting latent space
permutations for conditional MI estimation, significantly improving
computational efficiency. MCR outperforms all previously suggested training
strategies and is the first to consistently improve multimodal learning beyond
the ensemble baseline, clearly demonstrating that combining modalities leads to
significant performance gains on both synthetic and large real-world datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 6 figures, 4 tables, 1 algorithm</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-06T00:00:00Z">2024-12-06</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">9</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Graph-Based Approach for Conversational AI-Driven Personal Memory
  Capture and Retrieval in a Real-world Application 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Savini Kashmira, Jayanaka L. Dantanarayana, Joshua Brodsky, Ashish Mahendra, Yiping Kang, Krisztian Flautner, Lingjia Tang, Jason Mars
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  TOBU is a novel mobile application that captures and retrieves `personal
memories' (pictures/videos together with stories and context around those
moments) in a user-engaging AI-guided conversational approach. Our initial
prototype showed that existing retrieval techniques such as retrieval-augmented
generation (RAG) systems fall short due to their limitations in understanding
memory relationships, causing low recall, hallucination, and unsatisfactory
user experience. We design TOBUGraph, a novel graph-based retrieval approach.
During capturing, TOBUGraph leverages large language models (LLMs) to
automatically create a dynamic knowledge graph of memories, establishing
context and relationships of those memories. During retrieval, TOBUGraph
combines LLMs with the memory graph to achieve comprehensive recall through
graph traversal. Our evaluation using real user data demonstrates that
TOBUGraph outperforms multiple RAG implementations in both precision and
recall, significantly improving user experience through improved retrieval
accuracy and reduced hallucination.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ConQRet: Benchmarking Fine-Grained Evaluation of Retrieval Augmented
  Argumentation with LLM Judges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaustubh D. Dhole, Kai Shu, Eugene Agichtein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational argumentation, which involves generating answers or summaries
for controversial topics like abortion bans and vaccination, has become
increasingly important in today's polarized environment. Sophisticated LLM
capabilities offer the potential to provide nuanced, evidence-based answers to
such questions through Retrieval-Augmented Argumentation (RAArg), leveraging
real-world evidence for high-quality, grounded arguments. However, evaluating
RAArg remains challenging, as human evaluation is costly and difficult for
complex, lengthy answers on complicated topics. At the same time, re-using
existing argumentation datasets is no longer sufficient, as they lack long,
complex arguments and realistic evidence from potentially misleading sources,
limiting holistic evaluation of retrieval effectiveness and argument quality.
To address these gaps, we investigate automated evaluation methods using
multiple fine-grained LLM judges, providing better and more interpretable
assessments than traditional single-score metrics and even previously reported
human crowdsourcing. To validate the proposed techniques, we introduce ConQRet,
a new benchmark featuring long and complex human-authored arguments on debated
topics, grounded in real-world websites, allowing an exhaustive evaluation
across retrieval effectiveness, argument quality, and groundedness. We validate
our LLM Judges on a prior dataset and the new ConQRet benchmark. Our proposed
LLM Judges and the ConQRet benchmark can enable rapid progress in computational
argumentation and can be naturally extended to other complex
retrieval-augmented generation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ eXpath: Explaining Knowledge Graph Link Prediction with Ontological
  Closed Path Rules <span class="chip">VLDB</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Sun, Lei Shi, Yongxin Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Link prediction (LP) is crucial for Knowledge Graphs (KG) completion but
commonly suffers from interpretability issues. While several methods have been
proposed to explain embedding-based LP models, they are generally limited to
local explanations on KG and are deficient in providing human interpretable
semantics. Based on real-world observations of the characteristics of KGs from
multiple domains, we propose to explain LP models in KG with path-based
explanations. An integrated framework, namely eXpath, is introduced which
incorporates the concept of relation path with ontological closed path rules to
enhance both the efficiency and effectiveness of LP interpretation. Notably,
the eXpath explanations can be fused with other single-link explanation
approaches to achieve a better overall solution. Extensive experiments across
benchmark datasets and LP models demonstrate that introducing eXpath can boost
the quality of resulting explanations by about 20% on two key metrics and
reduce the required explanation time by 61.4%, in comparison to the best
existing method. Case studies further highlight eXpath's ability to provide
more semantically meaningful explanations through path-based evidence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures. Submitted to PVLDB volumn 18 on 20241201</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PyTerrier-GenRank: The PyTerrier Plugin for Reranking with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaustubh D. Dhole
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using LLMs as rerankers requires experimenting with various hyperparameters,
such as prompt formats, model choice, and reformulation strategies. We
introduce PyTerrier-GenRank, a PyTerrier plugin to facilitate seamless
reranking experiments with LLMs, supporting popular ranking strategies like
pointwise and listwise prompting. We validate our plugin through HuggingFace
and OpenAI hosted endpoints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diff4Steer: Steerable Diffusion Prior for Generative Music Retrieval
  with Semantic Guidance <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuchan Bao, Judith Yue Li, Zhong Yi Wan, Kun Su, Timo Denk, Joonseok Lee, Dima Kuzmin, Fei Sha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern music retrieval systems often rely on fixed representations of user
preferences, limiting their ability to capture users' diverse and uncertain
retrieval needs. To address this limitation, we introduce Diff4Steer, a novel
generative retrieval framework that employs lightweight diffusion models to
synthesize diverse seed embeddings from user queries that represent potential
directions for music exploration. Unlike deterministic methods that map user
query to a single point in embedding space, Diff4Steer provides a statistical
prior on the target modality (audio) for retrieval, effectively capturing the
uncertainty and multi-faceted nature of user preferences. Furthermore,
Diff4Steer can be steered by image or text inputs, enabling more flexible and
controllable music discovery combined with nearest neighbor search. Our
framework outperforms deterministic regression methods and LLM-based generative
retrieval baseline in terms of retrieval and ranking metrics, demonstrating its
effectiveness in capturing user preferences, leading to more diverse and
relevant recommendations. Listening examples are available at
tinyurl.com/diff4steer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Creative AI Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unifying Generative and Dense Retrieval for Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18814v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18814v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liu Yang, Fabian Paischer, Kaveh Hassani, Jiacheng Li, Shuai Shao, Zhang Gabriel Li, Yun He, Xue Feng, Nima Noorshams, Sem Park, Bo Long, Robert D Nowak, Xiaoli Gao, Hamid Eghbalzadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential dense retrieval models utilize advanced sequence learning
techniques to compute item and user representations, which are then used to
rank relevant items for a user through inner product computation between the
user and all item representations. However, this approach requires storing a
unique representation for each item, resulting in significant memory
requirements as the number of items grow. In contrast, the recently proposed
generative retrieval paradigm offers a promising alternative by directly
predicting item indices using a generative model trained on semantic IDs that
encapsulate items' semantic information. Despite its potential for large-scale
applications, a comprehensive comparison between generative retrieval and
sequential dense retrieval under fair conditions is still lacking, leaving open
questions regarding performance, and computation trade-offs. To address this,
we compare these two approaches under controlled conditions on academic
benchmarks and propose LIGER (LeveragIng dense retrieval for GEnerative
Retrieval), a hybrid model that combines the strengths of these two widely used
methods. LIGER integrates sequential dense retrieval into generative retrieval,
mitigating performance differences and enhancing cold-start item recommendation
in the datasets evaluated. This hybrid approach provides insights into the
trade-offs between these approaches and demonstrates improvements in efficiency
and effectiveness for recommendation systems in small-scale benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Boosting LLMs-driven Relevance Modeling with Progressive
  Retrieved Behavior-augmented <span class="highlight-title">Prompt</span>ing <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09439v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09439v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyuan Chen, Haiyan Wu, Kaixin Wu, Wei Chen, Mingjie Zhong, Jia Xu, Zhongyi Liu, Wei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relevance modeling is a critical component for enhancing user experience in
search engines, with the primary objective of identifying items that align with
users' queries. Traditional models only rely on the semantic congruence between
queries and items to ascertain relevance. However, this approach represents
merely one aspect of the relevance judgement, and is insufficient in isolation.
Even powerful Large Language Models (LLMs) still cannot accurately judge the
relevance of a query and an item from a semantic perspective. To augment
LLMs-driven relevance modeling, this study proposes leveraging user
interactions recorded in search logs to yield insights into users' implicit
search intentions. The challenge lies in the effective prompting of LLMs to
capture dynamic search intentions, which poses several obstacles in real-world
relevance scenarios, i.e., the absence of domain-specific knowledge, the
inadequacy of an isolated prompt, and the prohibitive costs associated with
deploying LLMs. In response, we propose ProRBP, a novel Progressive Retrieved
Behavior-augmented Prompting framework for integrating search scenario-oriented
knowledge with LLMs effectively. Specifically, we perform the user-driven
behavior neighbors retrieval from the daily search logs to obtain
domain-specific knowledge in time, retrieving candidates that users consider to
meet their expectations. Then, we guide LLMs for relevance modeling by
employing advanced prompting techniques that progressively improve the outputs
of the LLMs, followed by a progressive aggregation with comprehensive
consideration of diverse aspects. For online serving, we have developed an
industrial application framework tailored for the deployment of LLMs in
relevance modeling. Experiments on real-world industry data and online A/B
testing demonstrate our proposal achieves promising performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted By COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TPRF: A <span class="highlight-title">Transformer</span>-based Pseudo-Relevance Feedback Model for Efficient
  and Effective Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.13509v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.13509v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Li, Chuting Yu, Ahmed Mourad, Bevan Koopman, Guido Zuccon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers Pseudo-Relevance Feedback (PRF) methods for dense
retrievers in a resource constrained environment such as that of cheap cloud
instances or embedded systems (e.g., smartphones and smartwatches), where
memory and CPU are limited and GPUs are not present. For this, we propose a
transformer-based PRF method (TPRF), which has a much smaller memory footprint
and faster inference time compared to other deep language models that employ
PRF mechanisms, with a marginal effectiveness loss. TPRF learns how to
effectively combine the relevance feedback signals from dense passage
representations. Specifically, TPRF provides a mechanism for modelling
relationships and weights between the query and the relevance feedback signals.
The method is agnostic to the specific dense representation used and thus can
be generally applied to any dense retriever.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17740v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17740v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuheng Fang, Kangfei Zhao, Yu Rong, Zhixun Li, Jeffrey Xu Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cold-start rating prediction is a fundamental problem in recommender systems
that has been extensively studied. Many methods have been proposed that exploit
explicit relations among existing data, such as collaborative filtering, social
recommendations and heterogeneous information network, to alleviate the data
insufficiency issue for cold-start users and items. However, the explicit
relations constructed based on data between different roles may be unreliable
and irrelevant, which limits the performance ceiling of the specific
recommendation task. Motivated by this, in this paper, we propose a flexible
framework dubbed heterogeneous interaction rating network (HIRE). HIRE dose not
solely rely on the pre-defined interaction pattern or the manually constructed
heterogeneous information network. Instead, we devise a Heterogeneous
Interaction Module (HIM) to jointly model the heterogeneous interactions and
directly infer the important interactions via the observed data. In the
experiments, we evaluate our model under three cold-start settings on three
real-world datasets. The experimental results show that HIRE outperforms other
baselines by a large margin. Furthermore, we visualize the inferred
interactions of HIRE to confirm the contribution of our model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ pyAMPACT: A Score-Audio Alignment Toolkit for Performance Data
  Estimation and Multi-modal Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johanna Devaney, Daniel McKemie, Alex Morgan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  pyAMPACT (Python-based Automatic Music Performance Analysis and Comparison
Toolkit) links symbolic and audio music representations to facilitate
score-informed estimation of performance data in audio as well as general
linking of symbolic and audio music representations with a variety of
annotations. pyAMPACT can read a range of symbolic formats and can output
note-linked audio descriptors/performance data into MEI-formatted files. The
audio analysis uses score alignment to calculate time-frequency regions of
importance for each note in the symbolic representation from which to estimate
a range of parameters. These include tuning-, dynamics-, and timbre-related
performance descriptors, with timing-related information available from the
score alignment. Beyond performance data estimation, pyAMPACT also facilitates
multi-modal investigations through its infrastructure for linking symbolic
representations and annotations to audio.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Society for Music Information Retrieval, Late Breaking
  Demo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SMIC: Semantic Multi-Item Compression based on CLIP dictionary 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Bachard, Thomas Maugey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic compression, a compression scheme where the distortion metric,
typically MSE, is replaced with semantic fidelity metrics, tends to become more
and more popular. Most recent semantic compression schemes rely on the
foundation model CLIP. In this work, we extend such a scheme to image
collection compression, where inter-item redundancy is taken into account
during the coding phase. For that purpose, we first show that CLIP's latent
space allows for easy semantic additions and subtractions. From this property,
we define a dictionary-based multi-item codec that outperforms state-of-the-art
generative codec in terms of compression rate, around $10^{-5}$ BPP per image,
while not sacrificing semantic fidelity. We also show that the learned
dictionary is of a semantic nature and works as a semantic projector for the
semantic content of images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 14 figures, 3 tables, journal paper, preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diff4Steer: Steerable Diffusion Prior for Generative Music Retrieval
  with Semantic Guidance <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuchan Bao, Judith Yue Li, Zhong Yi Wan, Kun Su, Timo Denk, Joonseok Lee, Dima Kuzmin, Fei Sha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern music retrieval systems often rely on fixed representations of user
preferences, limiting their ability to capture users' diverse and uncertain
retrieval needs. To address this limitation, we introduce Diff4Steer, a novel
generative retrieval framework that employs lightweight diffusion models to
synthesize diverse seed embeddings from user queries that represent potential
directions for music exploration. Unlike deterministic methods that map user
query to a single point in embedding space, Diff4Steer provides a statistical
prior on the target modality (audio) for retrieval, effectively capturing the
uncertainty and multi-faceted nature of user preferences. Furthermore,
Diff4Steer can be steered by image or text inputs, enabling more flexible and
controllable music discovery combined with nearest neighbor search. Our
framework outperforms deterministic regression methods and LLM-based generative
retrieval baseline in terms of retrieval and ranking metrics, demonstrating its
effectiveness in capturing user preferences, leading to more diverse and
relevant recommendations. Listening examples are available at
tinyurl.com/diff4steer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Creative AI Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware
  Omni-Modal Perception of Long Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19772v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19772v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiantian Geng, Jinrui Zhang, Qingni Wang, Teng Wang, Jinming Duan, Feng Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite impressive advancements in video understanding, most efforts remain
limited to coarse-grained or visual-only video tasks. However, real-world
videos encompass omni-modal information (vision, audio, and speech) with a
series of events forming a cohesive storyline. The lack of multi-modal video
data with fine-grained event annotations and the high cost of manual labeling
are major obstacles to comprehensive omni-modality video perception. To address
this gap, we propose an automatic pipeline consisting of high-quality
multi-modal video filtering, semantically coherent omni-modal event boundary
detection, and cross-modal correlation-aware event captioning. In this way, we
present LongVALE, the first-ever Vision-Audio-Language Event understanding
benchmark comprising 105K omni-modal events with precise temporal boundaries
and detailed relation-aware captions within 8.4K high-quality long videos.
Further, we build a baseline that leverages LongVALE to enable video large
language models (LLMs) for omni-modality fine-grained temporal video
understanding for the first time. Extensive experiments demonstrate the
effectiveness and great potential of LongVALE in advancing comprehensive
multi-modal video understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TopoCode: Topologically Informed Error Detection and Correction in
  Communication Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12825v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12825v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongzhi Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional error detection and correction codes focus on bit-level fidelity,
which is insufficient for emerging technologies like eXtended Reality (XR) and
holographic communications requiring high-data-rate, low-latency systems.
Bit-level metrics cannot comprehensively evaluate Quality-of-Service (QoS) in
these scenarios. This letter proposes TopoCode which leverages Topological Data
Analysis (TDA) and persistent homology to encode topological information for
message-level error detection and correction. It introduces minimal redundancy
while enabling effective data reconstruction, especially in low Signal-to-Noise
Ratio (SNR) conditions. TopoCode offers a promising approach to meet the
demands of next-generation communication systems prioritizing semantic accuracy
and message-level integrity.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-05T00:00:00Z">2024-12-05</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HEAL: Hierarchical Embedding Alignment Loss for Improved Retrieval and
  Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04661v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04661v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manish Bhattarai, Ryan Barron, Maksim Eren, Minh Vu, Vesselin Grantcharov, Ismael Boureima, Valentin Stanev, Cynthia Matuszek, Vladimir Valtchinov, Kim Rasmussen, Boian Alexandrov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
integrating external document retrieval to provide domain-specific or
up-to-date knowledge. The effectiveness of RAG depends on the relevance of
retrieved documents, which is influenced by the semantic alignment of
embeddings with the domain's specialized content. Although full fine-tuning can
align language models to specific domains, it is computationally intensive and
demands substantial data. This paper introduces Hierarchical Embedding
Alignment Loss (HEAL), a novel method that leverages hierarchical fuzzy
clustering with matrix factorization within contrastive learning to efficiently
align LLM embeddings with domain-specific content. HEAL computes
level/depth-wise contrastive losses and incorporates hierarchical penalties to
align embeddings with the underlying relationships in label hierarchies. This
approach enhances retrieval relevance and document classification, effectively
reducing hallucinations in LLM outputs. In our experiments, we benchmark and
evaluate HEAL across diverse domains, including Healthcare, Material Science,
Cyber-security, and Applied Maths.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Retrieval at Walmart <span class="chip">KDD 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Magnani, Feng Liu, Suthee Chaidaroon, Sachin Yadav, Praveen Reddy Suram, Ajit Puthenputhussery, Sijie Chen, Min Xie, Anirudh Kashi, Tony Lee, Ciya Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In product search, the retrieval of candidate products before re-ranking is
more critical and challenging than other search like web search, especially for
tail queries, which have a complex and specific search intent. In this paper,
we present a hybrid system for e-commerce search deployed at Walmart that
combines traditional inverted index and embedding-based neural retrieval to
better answer user tail queries. Our system significantly improved the
relevance of the search engine, measured by both offline and online
evaluations. The improvements were achieved through a combination of different
approaches. We present a new technique to train the neural model at scale. and
describe how the system was deployed in production with little impact on
response time. We highlight multiple learnings and practical tricks that were
used in the deployment of this system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 page, 2 figures, 10 tables, KDD 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ User-item fairness tradeoffs in recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sophie Greenwood, Sudalakshmee Chiniah, Nikhil Garg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the basic recommendation paradigm, the most (predicted) relevant item is
recommended to each user. This may result in some items receiving lower
exposure than they "should"; to counter this, several algorithmic approaches
have been developed to ensure item fairness. These approaches necessarily
degrade recommendations for some users to improve outcomes for items, leading
to user fairness concerns. In turn, a recent line of work has focused on
developing algorithms for multi-sided fairness, to jointly optimize user
fairness, item fairness, and overall recommendation quality. This induces the
question: what is the tradeoff between these objectives, and what are the
characteristics of (multi-objective) optimal solutions? Theoretically, we
develop a model of recommendations with user and item fairness objectives and
characterize the solutions of fairness-constrained optimization. We identify
two phenomena: (a) when user preferences are diverse, there is "free" item and
user fairness; and (b) users whose preferences are misestimated can be
especially disadvantaged by item fairness constraints. Empirically, we
prototype a recommendation system for preprints on arXiv and implement our
framework, measuring the phenomena in practice and showing how these phenomena
inform the design of markets with recommendation systems-intermediated
matching.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Thirty-Eighth Annual Conference on Neural Information
  Processing Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-Sequential Alignment and Uniformity: Toward Enhanced
  Recommendation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuwei Cao, Liangwei Yang, Zhiwei Liu, Yuqing Liu, Chen Wang, Yueqing Liang, Hao Peng, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph-based and sequential methods are two popular recommendation paradigms,
each excelling in its domain but lacking the ability to leverage signals from
the other. To address this, we propose a novel method that integrates both
approaches for enhanced performance. Our framework uses Graph Neural Network
(GNN)-based and sequential recommenders as separate submodules while sharing a
unified embedding space optimized jointly. To enable positive knowledge
transfer, we design a loss function that enforces alignment and uniformity both
within and across submodules. Experiments on three real-world datasets
demonstrate that the proposed method significantly outperforms using either
approach alone and achieves state-of-the-art results. Our implementations are
publicly available at https://github.com/YuweiCao-UIC/GSAU.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PoTable: Programming Standardly on Table-based Reasoning Like a Human
  Analyst 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyang Mao, Qi Liu, Zhi Li, Mingyue Cheng, Zheng Zhang, Rui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Table-based reasoning has garnered substantial research interest,
particularly in its integration with Large Language Model (LLM) which has
revolutionized the general reasoning paradigm. Numerous LLM-based studies
introduce symbolic tools (e.g., databases, Python) as assistants to extend
human-like abilities in structured table understanding and complex arithmetic
computations. However, these studies can be improved better in simulating human
cognitive behavior when using symbolic tools, as they still suffer from
limitations of non-standard logical splits and constrained operation pools. In
this study, we propose PoTable as a novel table-based reasoning method that
simulates a human tabular analyst, which integrates a Python interpreter as the
real-time executor accompanied by an LLM-based operation planner and code
generator. Specifically, PoTable follows a human-like logical stage split and
extends the operation pool into an open-world space without any constraints.
Through planning and executing in each distinct stage, PoTable standardly
completes the entire reasoning process and produces superior reasoning results
along with highly accurate, steply commented and completely executable
programs. Accordingly, the effectiveness and explainability of PoTable are
fully demonstrated. Extensive experiments over three evaluation datasets from
two public benchmarks on two backbones show the outstanding performance of our
approach. In particular, GPT-based PoTable achieves over 4% higher absolute
accuracy than runner-ups on all evaluation datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Pre-train</span>, Align, and Disentangle: Empowering Sequential Recommendation
  with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Wang, Junwei Pan, Xiangyu Zhao, Pengyue Jia, Wanyu Wang, Yuan Wang, Yue Liu, Dapeng Liu, Jie Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation (SR) aims to model the sequential dependencies in
users' historical interactions to better capture their evolving interests.
However, existing SR approaches primarily rely on collaborative data, which
leads to limitations such as the cold-start problem and sub-optimal
performance. Meanwhile, despite the success of large language models (LLMs),
their application in industrial recommender systems is hindered by high
inference latency, inability to capture all distribution statistics, and
catastrophic forgetting. To this end, we propose a novel Pre-train, Align, and
Disentangle (PAD) paradigm to empower recommendation models with LLMs.
Specifically, we first pre-train both the SR and LLM models to get
collaborative and textual embeddings. Next, a characteristic
recommendation-anchored alignment loss is proposed using multi-kernel maximum
mean discrepancy with Gaussian kernels. Finally, a triple-experts architecture,
consisting aligned and modality-specific experts with disentangled embeddings,
is fine-tuned in a frequency-aware manner. Experiments conducted on three
public datasets demonstrate the effectiveness of PAD, showing significant
improvements and compatibility with various SR backbone models, especially on
cold items. The implementation code and datasets will be publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Disentangle Causal Model: Enhancing Causal Inference in Networked
  Observational Data <span class="chip">WSDM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binbin Hu, Zhicheng An, Zhengwei Wu, Ke Tu, Ziqi Liu, Zhiqiang Zhang, Jun Zhou, Yufei Feng, Jiawei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating individual treatment effects (ITE) from observational data is a
critical task across various domains. However, many existing works on ITE
estimation overlook the influence of hidden confounders, which remain
unobserved at the individual unit level. To address this limitation,
researchers have utilized graph neural networks to aggregate neighbors'
features to capture the hidden confounders and mitigate confounding bias by
minimizing the discrepancy of confounder representations between the treated
and control groups. Despite the success of these approaches, practical
scenarios often treat all features as confounders and involve substantial
differences in feature distributions between the treated and control groups.
Confusing the adjustment and confounder and enforcing strict balance on the
confounder representations could potentially undermine the effectiveness of
outcome prediction. To mitigate this issue, we propose a novel framework called
the \textit{Graph Disentangle Causal model} (GDC) to conduct ITE estimation in
the network setting. GDC utilizes a causal disentangle module to separate unit
features into adjustment and confounder representations. Then we design a graph
aggregation module consisting of three distinct graph aggregators to obtain
adjustment, confounder, and counterfactual confounder representations. Finally,
a causal constraint module is employed to enforce the disentangled
representations as true causal factors. The effectiveness of our proposed
method is demonstrated by conducting comprehensive experiments on two networked
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WSDM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Hash for Recommendation: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03875v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03875v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangyuan Luo, Honglei Zhang, Tong Li, Jun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the explosive growth of users and items, Recommender Systems (RS) are
facing unprecedented challenges on both retrieval efficiency and storage cost.
Fortunately, Learning to Hash (L2H) techniques have been shown as a promising
solution to address the two dilemmas, whose core idea is encoding
high-dimensional data into compact hash codes. To this end, L2H for RS (HashRec
for short) has recently received widespread attention to support large-scale
recommendations. In this survey, we present a comprehensive review of current
HashRec algorithms. Specifically, we first introduce the commonly used
two-tower models in the recall stage and identify two search strategies
frequently employed in L2H. Then, we categorize prior works into two-tier
taxonomy based on: (i) the type of loss function and (ii) the optimization
strategy. We also introduce some commonly used evaluation metrics to measure
the performance of HashRec algorithms. Finally, we shed light on the
limitations of the current research and outline the future research directions.
Furthermore, the summary of HashRec methods reviewed in this survey can be
found at
\href{https://github.com/Luo-Fangyuan/HashRec}{https://github.com/Luo-Fangyuan/HashRec}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Agent-OM: Leveraging LLM Agents for Ontology Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00326v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00326v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangcheng Qiang, Weiqing Wang, Kerry Taylor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ontology matching (OM) enables semantic interoperability between different
ontologies and resolves their conceptual heterogeneity by aligning related
entities. OM systems currently have two prevailing design paradigms:
conventional knowledge-based expert systems and newer machine learning-based
predictive systems. While large language models (LLMs) and LLM agents have
revolutionised data engineering and have been applied creatively in many
domains, their potential for OM remains underexplored. This study introduces a
novel agent-powered LLM-based design paradigm for OM systems. With
consideration of several specific challenges in leveraging LLM agents for OM,
we propose a generic framework, namely Agent-OM (Agent for Ontology Matching),
consisting of two Siamese agents for retrieval and matching, with a set of
simple OM tools. Our framework is implemented in a proof-of-concept system.
Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks
over state-of-the-art OM systems show that our system can achieve results very
close to the long-standing best performance on simple OM tasks and can
significantly improve the performance on complex and few-shot OM tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 13 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TiM4Rec: An Efficient Sequential Recommendation Model Based on
  Time-Aware Structured State Space Duality Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16182v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16182v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Fan, Mengyi Zhu, Yanrong Hu, Hailin Feng, Zhijie He, Hongjiu Liu, Qingyang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Sequential Recommendation modeling paradigm is shifting from Transformer
to Mamba architecture, which comprises two generations: Mamba1, based on the
State Space Model (SSM), and Mamba2, based on State Space Duality (SSD).
Although SSD offers superior computational efficiency compared to SSM, it
suffers performance degradation in sequential recommendation tasks, especially
in low-dimensional scenarios that are critical for these tasks. Considering
that time-aware enhancement methods are commonly employed to mitigate
performance loss, our analysis reveals that the performance decline of SSD can
similarly be fundamentally compensated by leveraging mechanisms in time-aware
methods. Thus, we propose integrating time-awareness into the SSD framework to
address these performance issues. However, integrating current time-aware
methods, modeled after TiSASRec, into SSD faces the following challenges: 1)
the complexity of integrating these transformer-based mechanisms with the SSD
architecture, and 2) the computational inefficiency caused by the need for
dimensionality expansion of time-difference modeling. To overcome these
challenges, we introduce a novel Time-aware Structured Masked Matrix that
efficiently incorporates time-aware capabilities into SSD. Building on this, we
propose Time-Aware Mamba for Recommendation (TiM4Rec), which mitigates
performance degradation in low-dimensional SSD contexts while preserving
computational efficiency. This marks the inaugural application of a time-aware
enhancement method specifically tailored for the Mamba architecture within the
domain of sequential recommendation. Extensive experiments conducted on three
real-world datasets demonstrate the superiority of our approach. The code for
our model is accessible at https://github.com/AlwaysFHao/TiM4Rec.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lexicalization Is All You Need: Examining the Impact of Lexical
  Knowledge in a Compositional QALD System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03906v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03906v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Maria Schmidt, Mohammad Fazleh Elahi, Philipp Cimiano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we examine the impact of lexicalization on Question Answering
over Linked Data (QALD). It is well known that one of the key challenges in
interpreting natural language questions with respect to SPARQL lies in bridging
the lexical gap, that is mapping the words in the query to the correct
vocabulary elements. We argue in this paper that lexicalization, that is
explicit knowledge about the potential interpretations of a word with respect
to the given vocabulary, significantly eases the task and increases the
performance of QA systems. Towards this goal, we present a compositional QA
system that can leverage explicit lexical knowledge in a compositional manner
to infer the meaning of a question in terms of a SPARQL query. We show that
such a system, given lexical knowledge, has a performance well beyond current
QA systems, achieving up to a $35.8\%$ increase in the micro $F_1$ score
compared to the best QA system on QALD-9. This shows the importance and
potential of including explicit lexical knowledge. In contrast, we show that
LLMs have limited abilities to exploit lexical knowledge, with only marginal
improvements compared to a version without lexical knowledge. This shows that
LLMs have no ability to compositionally interpret a question on the basis of
the meaning of its parts, a key feature of compositional approaches. Taken
together, our work shows new avenues for QALD research, emphasizing the
importance of lexicalization and compositionality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24th International Conference on Knowledge Engineering and Knowledge
  Management (EKAW 2024), November 26-28, 2024, Amsterdam, The Netherlands</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Point-of-Interest Recommendations Leveraging Heterogeneous
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07426v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07426v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehui Wang, Wolfram Höpken, Dietmar Jannach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tourism is an important application domain for recommender systems. In this
domain, recommender systems are for example tasked with providing personalized
recommendations for transportation, accommodation, points-of-interest (POIs),
etc. Among these tasks, in particular the problem of recommending POIs that are
of likely interest to individual tourists has gained growing attention in
recent years. Providing POI recommendations to tourists can however be
especially challenging due to the variability of the user's context. With the
rapid development of the Web and today's multitude of online services, vast
amounts of data from various sources have become available, and these
heterogeneous data represent a huge potential to better address the challenges
of POI recommendation problems. In this work, we provide a survey of published
research on the problem of POI recommendation between 2021 and 2023. The
literature was surveyed to identify the information types, techniques and
evaluation methods employed. Based on the analysis, it was observed that the
current research tends to focus on a relatively narrow range of information
types and there is a significant potential in improving POI recommendation by
leveraging heterogeneous data. As the first information-centric survey on POI
recommendation research, this study serves as a reference for researchers
aiming to develop increasingly accurate, personalized and context-aware POI
recommender systems.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature Coding in the Era of Large Models: <span class="highlight-title">Dataset</span>, Test Conditions, and
  Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changsheng Gao, Yifan Ma, Qiaoxi Chen, Yenan Xu, Dong Liu, Weisi Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large models have achieved remarkable performance across various tasks, yet
they incur significant computational costs and privacy concerns during both
training and inference. Distributed deployment has emerged as a potential
solution, but it necessitates the exchange of intermediate information between
model segments, with feature representations serving as crucial information
carriers. To optimize information exchange, feature coding methods are applied
to reduce transmission and storage overhead. Despite its importance, feature
coding for large models remains an under-explored area. In this paper, we draw
attention to large model feature coding and make three contributions to this
field. First, we introduce a comprehensive dataset encompassing diverse
features generated by three representative types of large models. Second, we
establish unified test conditions, enabling standardized evaluation pipelines
and fair comparisons across future feature coding studies. Third, we introduce
two baseline methods derived from widely used image coding techniques and
benchmark their performance on the proposed dataset. These contributions aim to
advance the field of feature coding, facilitating more efficient large model
deployment. All source code and the dataset will be made available on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identity-Preserving Text-to-Video Generation by Frequency Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17440v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17440v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyuan Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, Li Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identity-preserving text-to-video (IPT2V) generation aims to create
high-fidelity videos with consistent human identity. It is an important task in
video generation but remains an open problem for generative models. This paper
pushes the technical frontier of IPT2V in two directions that have not been
resolved in literature: (1) A tuning-free pipeline without tedious case-by-case
finetuning, and (2) A frequency-aware heuristic identity-preserving DiT-based
control scheme. We propose ConsisID, a tuning-free DiT-based controllable IPT2V
model to keep human identity consistent in the generated video. Inspired by
prior findings in frequency analysis of diffusion transformers, it employs
identity-control signals in the frequency domain, where facial features can be
decomposed into low-frequency global features and high-frequency intrinsic
features. First, from a low-frequency perspective, we introduce a global facial
extractor, which encodes reference images and facial key points into a latent
space, generating features enriched with low-frequency information. These
features are then integrated into shallow layers of the network to alleviate
training challenges associated with DiT. Second, from a high-frequency
perspective, we design a local facial extractor to capture high-frequency
details and inject them into transformer blocks, enhancing the model's ability
to preserve fine-grained features. We propose a hierarchical training strategy
to leverage frequency information for identity preservation, transforming a
vanilla pre-trained video generation model into an IPT2V model. Extensive
experiments demonstrate that our frequency-aware heuristic scheme provides an
optimal control solution for DiT-based models. Thanks to this scheme, our
ConsisID generates high-quality, identity-preserving videos, making strides
towards more effective IPT2V.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures, Code: https://github.com/PKU-YuanGroup/ConsisID</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Memories are One-to-Many Mapping Alleviators in Talking Face Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05005v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05005v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anni Tang, Tianyu He, Xu Tan, Jun Ling, Li Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Talking face generation aims at generating photo-realistic video portraits of
a target person driven by input audio. Due to its nature of one-to-many mapping
from the input audio to the output video (e.g., one speech content may have
multiple feasible visual appearances), learning a deterministic mapping like
previous works brings ambiguity during training, and thus causes inferior
visual results. Although this one-to-many mapping could be alleviated in part
by a two-stage framework (i.e., an audio-to-expression model followed by a
neural-rendering model), it is still insufficient since the prediction is
produced without enough information (e.g., emotions, wrinkles, etc.). In this
paper, we propose MemFace to complement the missing information with an
implicit memory and an explicit memory that follow the sense of the two stages
respectively. More specifically, the implicit memory is employed in the
audio-to-expression model to capture high-level semantics in the
audio-expression shared space, while the explicit memory is employed in the
neural-rendering model to help synthesize pixel-level details. Our experimental
results show that our proposed MemFace surpasses all the state-of-the-art
results across multiple scenarios consistently and significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions on Pattern Analysis and Machine Intelligence
  (2024). Project page: see https://memoryface.github.io</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-12-13T05:28:44.392595794Z">
            2024-12-13 05:28:44 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
